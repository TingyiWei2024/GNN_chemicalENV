{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader as PyGDataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GCNConv, GINEConv, global_mean_pool, BatchNorm\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset that:\n",
    "      - Reads external factors from CSV\n",
    "      - Loads the corresponding pickle for the molecule's graph\n",
    "      - Converts it into a PyG Data object\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 raw_dataframe: pd.DataFrame,\n",
    "                 nx_graph_dict: dict,\n",
    "                 *,\n",
    "                 component_col: str,\n",
    "                 global_state_cols: list[str],\n",
    "                 label_col: str,\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        \"\"\"\n",
    "        self.raw_dataframe = raw_dataframe\n",
    "        self.nx_graph_dict = nx_graph_dict\n",
    "        self.component_col = [component_col] if type(component_col) is str else component_col\n",
    "        self.global_state_cols = global_state_cols\n",
    "        self.label_col = [label_col] if type(label_col) is str else label_col\n",
    "        self.transform = transform\n",
    "        \n",
    "        required_cols = set(self.global_state_cols + self.label_col + self.component_col)\n",
    "        for col in required_cols:\n",
    "            if col not in self.raw_dataframe.columns:\n",
    "                raise ValueError(f\"Missing column in DataFrame: '{col}'\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.raw_dataframe.iloc[idx]\n",
    "        \n",
    "        # 1. Load the molecule graph\n",
    "        component_name = row[self.component_col[0]]  # e.g. \"C23\"\n",
    "        pyg_data = self.nx_graph_dict[component_name]\n",
    "\n",
    "        # 3. Prepare the external factors\n",
    "        #    Convert selected columns into a float tensor\n",
    "        externals = torch.tensor(row[self.global_state_cols].values.astype(float), dtype=torch.float)\n",
    "        externals = externals.unsqueeze(0)\n",
    "\n",
    "        # 4. Prepare the label (regression target)\n",
    "        label = torch.tensor([row[self.label_col][0]], dtype=torch.float)\n",
    "\n",
    "        # 5. Attach externals & label to the Data object for use in the model\n",
    "        #    (We can store them in Data object attributes if you like)\n",
    "        pyg_data.externals = externals  # 1D vector of external factors\n",
    "        pyg_data.y = label  # shape [1]\n",
    "\n",
    "        if self.transform:\n",
    "            pyg_data = self.transform(pyg_data)\n",
    "\n",
    "        return pyg_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def networkx_to_pyg(nx_graph):\n",
    "    \"\"\"\n",
    "    Convert a networkx graph to a torch_geometric.data.Data object.\n",
    "    This is a basic template; adjust for your actual node/edge features.\n",
    "    \"\"\"\n",
    "    # Sort nodes to ensure consistent ordering\n",
    "    # e.g. node 0, node 1, ...\n",
    "    # In some networkx graphs, node labels might be strings. We’ll map them to integers.\n",
    "    node_mapping = {node: i for i, node in enumerate(nx_graph.nodes())}\n",
    "\n",
    "    # Build lists for PyG\n",
    "    x_list = []\n",
    "    edge_index_list = []\n",
    "    edge_attr_list = []\n",
    "\n",
    "    for node in nx_graph.nodes(data=True):\n",
    "        original_id = node[0]\n",
    "        attrs = node[1]\n",
    "        # Example: 'symbol' might be in attrs, etc.\n",
    "        # For demonstration, let's store only \"symbol\" as a simple categorical embedding\n",
    "        # You might do something more sophisticated (e.g., one-hot) for real usage\n",
    "        symbol = attrs.get(\"symbol\", \"C\")\n",
    "        # Convert symbol to a simple ID (C=0, H=1, etc.) or some vector\n",
    "        # We'll do a naive approach here:\n",
    "        symbol_id = 0 if symbol == \"C\" else 1 if symbol == \"H\" else 2\n",
    "        \n",
    "        x_list.append([symbol_id])\n",
    "\n",
    "    for u, v, edge_attrs in nx_graph.edges(data=True):\n",
    "        u_idx = node_mapping[u]\n",
    "        v_idx = node_mapping[v]\n",
    "        edge_index_list.append((u_idx, v_idx))\n",
    "        # Possibly store bond features: \"bond_index\", \"bde_pred\", etc.\n",
    "        bde_pred = edge_attrs.get(\"bde_pred\", 0.0)\n",
    "        if bde_pred is None:\n",
    "            bde_pred = 0.0\n",
    "        bdfe_pred = edge_attrs.get(\"bdfe_pred\", 0.0)\n",
    "        if bdfe_pred is None:\n",
    "            bdfe_pred = 0.0\n",
    "        edge_attr_list.append([bde_pred, bdfe_pred])\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    x = torch.tensor(x_list, dtype=torch.float)  # shape [num_nodes, num_node_features]\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()  # shape [2, num_edges]\n",
    "    edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)  # shape [num_edges, edge_feat_dim]\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINE_Regression(nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dim: int = 128,\n",
    "                 num_layers: int = 3,\n",
    "                 dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        A more 'realistic' GNN for regression, using GINEConv layers + edge attributes.\n",
    "        \n",
    "        Args:\n",
    "            node_in_dim (int): Dim of node features (e.g. 1 or 3).\n",
    "            edge_in_dim (int): Dim of edge features (e.g. 2 for [bde_pred, bdfe_pred]).\n",
    "            external_in_dim (int): Dim of external factor features (e.g. 6).\n",
    "            hidden_dim (int): Hidden embedding size for GNN layers.\n",
    "            num_layers (int): Number of GNN layers.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # A learnable linear transform for edge features (required by GINEConv's \"nn\" argument):\n",
    "        # Typically GINEConv uses a small MLP to incorporate edge_attr into the message.\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # A learnable linear transform for node features:\n",
    "        self.node_encoder = nn.Linear(node_in_dim, hidden_dim)\n",
    "        \n",
    "        # Create multiple GINEConv layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            # GINEConv requires an MLP for node update:\n",
    "            # We'll use a simple 2-layer MLP\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dim))  # batch norm for stability\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # An MLP to process external factors\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression MLP after pooling + external embedding\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: PyG Data object, expected fields:\n",
    "                - x: Node features [num_nodes, node_in_dim]\n",
    "                - edge_index: [2, num_edges]\n",
    "                - edge_attr: [num_edges, edge_in_dim]\n",
    "                - batch: [num_nodes] mapping each node to a graph ID\n",
    "                - externals: [batch_size, external_in_dim]\n",
    "        Returns:\n",
    "            A tensor of shape [batch_size], the predicted regression value.\n",
    "        \"\"\"\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # 1) Encode node features and edge features\n",
    "        x = self.node_encoder(x)                 # [num_nodes, hidden_dim]\n",
    "        edge_emb = self.edge_encoder(edge_attr)  # [num_edges, hidden_dim]\n",
    "        \n",
    "        # 2) Pass through multiple GINEConv layers\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # 3) Global pooling to get graph embedding\n",
    "        graph_emb = global_mean_pool(x, batch)  # [batch_size, hidden_dim]\n",
    "\n",
    "        # 4) Process external factors\n",
    "        ext_emb = self.externals_mlp(data.externals)  # [batch_size, hidden_dim]\n",
    "\n",
    "        # 5) Combine + final regression\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)  # [batch_size, hidden_dim * 2]\n",
    "        out = self.final_regressor(combined).squeeze(-1)    # [batch_size]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_data)               # [batch_size]\n",
    "        y = batch_data.y.to(device).view(-1)    # [batch_size]\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "        count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            preds = model(batch_data)\n",
    "            y = batch_data.y.to(device).view(-1)\n",
    "            loss = criterion(preds, y)\n",
    "            total_loss += loss.item() * batch_data.num_graphs\n",
    "            count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset loader and compute R² and RMSE.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained GNN model.\n",
    "        loader (DataLoader): The PyG DataLoader for the evaluation dataset.\n",
    "        device (torch.device): The device to run on.\n",
    "    \n",
    "    Returns:\n",
    "        r2 (float): Coefficient of determination.\n",
    "        rmse (float): Root Mean Squared Error.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            preds = model(batch)\n",
    "            y_true.append(batch.y.cpu())\n",
    "            y_pred.append(preds.cpu())\n",
    "\n",
    "    # If your labels are stored as tensors with an extra dimension, use .squeeze() if needed.\n",
    "    y_true = torch.cat(y_true).numpy().squeeze()\n",
    "    y_pred = torch.cat(y_pred).numpy().squeeze()\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return r2, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_file = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\dataset02.xlsx\"\n",
    "\n",
    "data = pd.read_excel(env_file, engine='openpyxl').dropna(subset=['degradation_rate'])\n",
    "data['seawater'] = data['seawater'].map({'art': 1, 'sea': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1023 entries, 0 to 1039\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   data number       1023 non-null   float64\n",
      " 1   temperature       1023 non-null   float64\n",
      " 2   seawater          1023 non-null   int64  \n",
      " 3   concentration     1023 non-null   int64  \n",
      " 4   time              1023 non-null   int64  \n",
      " 5   component         1023 non-null   object \n",
      " 6   BDE               1023 non-null   float64\n",
      " 7   BDFE              1023 non-null   float64\n",
      " 8   energy            1023 non-null   float64\n",
      " 9   degradation_rate  1023 non-null   float64\n",
      "dtypes: float64(6), int64(3), object(1)\n",
      "memory usage: 87.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\molecules\"\n",
    "graph_pickles = [f for f in os.listdir(folder_path) if f.endswith(\".pkl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\molecules\n",
      "Files in directory: ['gpickle_graph_0.pkl', 'gpickle_graph_1.pkl', 'gpickle_graph_10.pkl', 'gpickle_graph_11.pkl', 'gpickle_graph_12.pkl', 'gpickle_graph_13.pkl', 'gpickle_graph_14.pkl', 'gpickle_graph_15.pkl', 'gpickle_graph_16.pkl', 'gpickle_graph_17.pkl', 'gpickle_graph_18.pkl', 'gpickle_graph_19.pkl', 'gpickle_graph_2.pkl', 'gpickle_graph_3.pkl', 'gpickle_graph_4.pkl', 'gpickle_graph_5.pkl', 'gpickle_graph_6.pkl', 'gpickle_graph_7.pkl', 'gpickle_graph_8.pkl', 'gpickle_graph_9.pkl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\molecules\"\n",
    "\n",
    "if os.path.exists(base_dir):\n",
    "    print(\"Directory exists:\", base_dir)\n",
    "    print(\"Files in directory:\", os.listdir(base_dir))\n",
    "else:\n",
    "    print(f\"Error: Directory {base_dir} does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = data.component.unique()\n",
    "graphs_dict = {}\n",
    "\n",
    "for compound, graph_pickle in zip(compounds, graph_pickles):\n",
    "    with open(os.path.join(base_dir, graph_pickle), 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "        graphs_dict[compound] = networkx_to_pyg(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = MolDataset(\n",
    "    raw_dataframe=data,\n",
    "    nx_graph_dict=graphs_dict,\n",
    "    component_col=\"component\",\n",
    "    global_state_cols=[\"temperature\", \"concentration\", \"time\", \"seawater\"],\n",
    "    label_col=\"degradation_rate\",\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# Simple random split for demonstration\n",
    "train_size = int(0.8 * len(dataset))  # 80% train\n",
    "val_size   = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = PyGDataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = PyGDataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 2) Instantiate model + optimizer\n",
    "# -----------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GINE_Regression(\n",
    "    node_in_dim=1,\n",
    "    edge_in_dim=2,\n",
    "    external_in_dim=4,\n",
    "    hidden_dim=16,\n",
    "    num_layers=5,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] train_loss: 0.1527, val_loss: 0.0777\n",
      "[Epoch 20] train_loss: 0.1020, val_loss: 0.0644\n",
      "[Epoch 30] train_loss: 0.0765, val_loss: 0.0652\n",
      "[Epoch 40] train_loss: 0.0765, val_loss: 0.0595\n",
      "[Epoch 50] train_loss: 0.0719, val_loss: 0.0607\n",
      "[Epoch 60] train_loss: 0.0602, val_loss: 0.0608\n",
      "[Epoch 70] train_loss: 0.0808, val_loss: 0.0572\n",
      "[Epoch 80] train_loss: 0.0644, val_loss: 0.0691\n",
      "[Epoch 90] train_loss: 0.0675, val_loss: 0.0560\n",
      "[Epoch 100] train_loss: 0.0748, val_loss: 0.0604\n",
      "[Epoch 110] train_loss: 0.0680, val_loss: 0.0581\n",
      "[Epoch 120] train_loss: 0.0687, val_loss: 0.0534\n",
      "[Epoch 130] train_loss: 0.0633, val_loss: 0.0565\n",
      "[Epoch 140] train_loss: 0.0605, val_loss: 0.0618\n",
      "[Epoch 150] train_loss: 0.0615, val_loss: 0.0507\n",
      "[Epoch 160] train_loss: 0.0590, val_loss: 0.0493\n",
      "[Epoch 170] train_loss: 0.0603, val_loss: 0.0511\n",
      "[Epoch 180] train_loss: 0.0677, val_loss: 0.0526\n",
      "[Epoch 190] train_loss: 0.0569, val_loss: 0.0484\n",
      "[Epoch 200] train_loss: 0.0506, val_loss: 0.0485\n",
      "[Epoch 210] train_loss: 0.0599, val_loss: 0.0488\n",
      "[Epoch 220] train_loss: 0.0542, val_loss: 0.0446\n",
      "[Epoch 230] train_loss: 0.0542, val_loss: 0.0476\n",
      "[Epoch 240] train_loss: 0.0521, val_loss: 0.0520\n",
      "[Epoch 250] train_loss: 0.0493, val_loss: 0.0445\n",
      "[Epoch 260] train_loss: 0.0549, val_loss: 0.0490\n",
      "[Epoch 270] train_loss: 0.0481, val_loss: 0.0430\n",
      "[Epoch 280] train_loss: 0.0480, val_loss: 0.0413\n",
      "[Epoch 290] train_loss: 0.0509, val_loss: 0.0402\n",
      "[Epoch 300] train_loss: 0.0465, val_loss: 0.0412\n",
      "[Epoch 310] train_loss: 0.0463, val_loss: 0.0436\n",
      "[Epoch 320] train_loss: 0.0449, val_loss: 0.0351\n",
      "[Epoch 330] train_loss: 0.0419, val_loss: 0.0369\n",
      "[Epoch 340] train_loss: 0.0370, val_loss: 0.0344\n",
      "[Epoch 350] train_loss: 0.0414, val_loss: 0.0350\n",
      "[Epoch 360] train_loss: 0.0372, val_loss: 0.0313\n",
      "[Epoch 370] train_loss: 0.0499, val_loss: 0.0416\n",
      "[Epoch 380] train_loss: 0.0421, val_loss: 0.0391\n",
      "[Epoch 390] train_loss: 0.0590, val_loss: 0.0395\n",
      "[Epoch 400] train_loss: 0.0506, val_loss: 0.0499\n",
      "[Epoch 410] train_loss: 0.0453, val_loss: 0.0354\n",
      "[Epoch 420] train_loss: 0.0357, val_loss: 0.0394\n",
      "[Epoch 430] train_loss: 0.0524, val_loss: 0.0448\n",
      "[Epoch 440] train_loss: 0.0393, val_loss: 0.0309\n",
      "[Epoch 450] train_loss: 0.0398, val_loss: 0.0307\n",
      "[Epoch 460] train_loss: 0.0356, val_loss: 0.0317\n",
      "[Epoch 470] train_loss: 0.0438, val_loss: 0.0438\n",
      "[Epoch 480] train_loss: 0.0407, val_loss: 0.0381\n",
      "[Epoch 490] train_loss: 0.0349, val_loss: 0.0549\n",
      "[Epoch 500] train_loss: 0.0346, val_loss: 0.0431\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 3) Training Loop\n",
    "# -----------------------------------\n",
    "num_epochs = 500\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[Epoch {epoch}] train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}\")\n",
    "\n",
    "# Optionally, test or save the model\n",
    "# torch.save(model.state_dict(), \"trained_gine_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.3789\n",
      "RMSE: 0.2076\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage after training:\n",
    "r2, rmse = evaluate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\n",
      "Files in directory: ['gpickle_graph_0.pkl', 'gpickle_graph_1.pkl', 'gpickle_graph_10.pkl', 'gpickle_graph_11.pkl', 'gpickle_graph_12.pkl', 'gpickle_graph_13.pkl', 'gpickle_graph_14.pkl', 'gpickle_graph_15.pkl', 'gpickle_graph_16.pkl', 'gpickle_graph_17.pkl', 'gpickle_graph_18.pkl', 'gpickle_graph_19.pkl', 'gpickle_graph_2.pkl', 'gpickle_graph_3.pkl', 'gpickle_graph_4.pkl', 'gpickle_graph_5.pkl', 'gpickle_graph_6.pkl', 'gpickle_graph_7.pkl', 'gpickle_graph_8.pkl', 'gpickle_graph_9.pkl']\n",
      "\n",
      "--- Fold 1 ---\n",
      "[Fold 1 Epoch 10] Train Loss: 0.3651 | Val Loss: 0.0789\n",
      "[Fold 1 Epoch 20] Train Loss: 0.1172 | Val Loss: 0.0645\n",
      "[Fold 1 Epoch 30] Train Loss: 0.1094 | Val Loss: 0.0559\n",
      "[Fold 1 Epoch 40] Train Loss: 0.0870 | Val Loss: 0.0554\n",
      "[Fold 1 Epoch 50] Train Loss: 0.0811 | Val Loss: 0.0502\n",
      "[Fold 1 Epoch 60] Train Loss: 0.0809 | Val Loss: 0.0512\n",
      "[Fold 1 Epoch 70] Train Loss: 0.0749 | Val Loss: 0.0537\n",
      "[Fold 1 Epoch 80] Train Loss: 0.0699 | Val Loss: 0.0502\n",
      "[Fold 1 Epoch 90] Train Loss: 0.0702 | Val Loss: 0.0486\n",
      "[Fold 1 Epoch 100] Train Loss: 0.0580 | Val Loss: 0.0497\n",
      "[Fold 1 Epoch 110] Train Loss: 0.0614 | Val Loss: 0.0495\n",
      "[Fold 1 Epoch 120] Train Loss: 0.0586 | Val Loss: 0.0526\n",
      "[Fold 1 Epoch 130] Train Loss: 0.0676 | Val Loss: 0.0474\n",
      "[Fold 1 Epoch 140] Train Loss: 0.0665 | Val Loss: 0.0458\n",
      "[Fold 1 Epoch 150] Train Loss: 0.0708 | Val Loss: 0.0538\n",
      "[Fold 1 Epoch 160] Train Loss: 0.0654 | Val Loss: 0.0463\n",
      "[Fold 1 Epoch 170] Train Loss: 0.0649 | Val Loss: 0.0446\n",
      "[Fold 1 Epoch 180] Train Loss: 0.0617 | Val Loss: 0.0420\n",
      "[Fold 1 Epoch 190] Train Loss: 0.0637 | Val Loss: 0.0414\n",
      "[Fold 1 Epoch 200] Train Loss: 0.0506 | Val Loss: 0.0404\n",
      "Evaluating fold 1 ...\n",
      "R²: 0.2815\n",
      "RMSE: 0.2009\n",
      "\n",
      "--- Fold 2 ---\n",
      "[Fold 2 Epoch 10] Train Loss: 1.9264 | Val Loss: 0.2187\n",
      "[Fold 2 Epoch 20] Train Loss: 0.4418 | Val Loss: 0.1652\n",
      "[Fold 2 Epoch 30] Train Loss: 0.2734 | Val Loss: 0.1326\n",
      "[Fold 2 Epoch 40] Train Loss: 0.1680 | Val Loss: 0.1234\n",
      "[Fold 2 Epoch 50] Train Loss: 0.1340 | Val Loss: 0.1178\n",
      "[Fold 2 Epoch 60] Train Loss: 0.1266 | Val Loss: 0.0996\n",
      "[Fold 2 Epoch 70] Train Loss: 0.1051 | Val Loss: 0.1058\n",
      "[Fold 2 Epoch 80] Train Loss: 0.1036 | Val Loss: 0.0931\n",
      "[Fold 2 Epoch 90] Train Loss: 0.0892 | Val Loss: 0.1075\n",
      "[Fold 2 Epoch 100] Train Loss: 0.0743 | Val Loss: 0.1114\n",
      "[Fold 2 Epoch 110] Train Loss: 0.0819 | Val Loss: 0.1059\n",
      "[Fold 2 Epoch 120] Train Loss: 0.0770 | Val Loss: 0.0936\n",
      "[Fold 2 Epoch 130] Train Loss: 0.0738 | Val Loss: 0.0949\n",
      "[Fold 2 Epoch 140] Train Loss: 0.0748 | Val Loss: 0.1019\n",
      "[Fold 2 Epoch 150] Train Loss: 0.0695 | Val Loss: 0.1262\n",
      "[Fold 2 Epoch 160] Train Loss: 0.0647 | Val Loss: 0.1085\n",
      "[Fold 2 Epoch 170] Train Loss: 0.0661 | Val Loss: 0.0900\n",
      "[Fold 2 Epoch 180] Train Loss: 0.0704 | Val Loss: 0.0925\n",
      "[Fold 2 Epoch 190] Train Loss: 0.0627 | Val Loss: 0.0905\n",
      "[Fold 2 Epoch 200] Train Loss: 0.0697 | Val Loss: 0.1001\n",
      "Evaluating fold 2 ...\n",
      "R²: 0.0414\n",
      "RMSE: 0.3164\n",
      "\n",
      "--- Fold 3 ---\n",
      "[Fold 3 Epoch 10] Train Loss: 0.2300 | Val Loss: 0.1023\n",
      "[Fold 3 Epoch 20] Train Loss: 0.1271 | Val Loss: 0.0691\n",
      "[Fold 3 Epoch 30] Train Loss: 0.0931 | Val Loss: 0.0675\n",
      "[Fold 3 Epoch 40] Train Loss: 0.0778 | Val Loss: 0.0958\n",
      "[Fold 3 Epoch 50] Train Loss: 0.0861 | Val Loss: 0.0601\n",
      "[Fold 3 Epoch 60] Train Loss: 0.0720 | Val Loss: 0.0581\n",
      "[Fold 3 Epoch 70] Train Loss: 0.0785 | Val Loss: 0.0570\n",
      "[Fold 3 Epoch 80] Train Loss: 0.0650 | Val Loss: 0.0619\n",
      "[Fold 3 Epoch 90] Train Loss: 0.0741 | Val Loss: 0.0622\n",
      "[Fold 3 Epoch 100] Train Loss: 0.0662 | Val Loss: 0.0544\n",
      "[Fold 3 Epoch 110] Train Loss: 0.0654 | Val Loss: 0.0603\n",
      "[Fold 3 Epoch 120] Train Loss: 0.0666 | Val Loss: 0.0640\n",
      "[Fold 3 Epoch 130] Train Loss: 0.0658 | Val Loss: 0.0600\n",
      "[Fold 3 Epoch 140] Train Loss: 0.0579 | Val Loss: 0.0653\n",
      "[Fold 3 Epoch 150] Train Loss: 0.0607 | Val Loss: 0.0497\n",
      "[Fold 3 Epoch 160] Train Loss: 0.0618 | Val Loss: 0.0517\n",
      "[Fold 3 Epoch 170] Train Loss: 0.0600 | Val Loss: 0.0483\n",
      "[Fold 3 Epoch 180] Train Loss: 0.0592 | Val Loss: 0.0506\n",
      "[Fold 3 Epoch 190] Train Loss: 0.0695 | Val Loss: 0.0528\n",
      "[Fold 3 Epoch 200] Train Loss: 0.0627 | Val Loss: 0.0474\n",
      "Evaluating fold 3 ...\n",
      "R²: 0.3211\n",
      "RMSE: 0.2177\n",
      "\n",
      "--- Fold 4 ---\n",
      "[Fold 4 Epoch 10] Train Loss: 0.1859 | Val Loss: 0.1171\n",
      "[Fold 4 Epoch 20] Train Loss: 0.1484 | Val Loss: 0.1000\n",
      "[Fold 4 Epoch 30] Train Loss: 0.0958 | Val Loss: 0.1088\n",
      "[Fold 4 Epoch 40] Train Loss: 0.0905 | Val Loss: 0.1060\n",
      "[Fold 4 Epoch 50] Train Loss: 0.0806 | Val Loss: 0.0968\n",
      "[Fold 4 Epoch 60] Train Loss: 0.0618 | Val Loss: 0.0854\n",
      "[Fold 4 Epoch 70] Train Loss: 0.0655 | Val Loss: 0.0999\n",
      "[Fold 4 Epoch 80] Train Loss: 0.0533 | Val Loss: 0.0878\n",
      "[Fold 4 Epoch 90] Train Loss: 0.0673 | Val Loss: 0.0853\n",
      "[Fold 4 Epoch 100] Train Loss: 0.0595 | Val Loss: 0.0825\n",
      "[Fold 4 Epoch 110] Train Loss: 0.0611 | Val Loss: 0.0833\n",
      "[Fold 4 Epoch 120] Train Loss: 0.0583 | Val Loss: 0.0852\n",
      "[Fold 4 Epoch 130] Train Loss: 0.0585 | Val Loss: 0.0776\n",
      "[Fold 4 Epoch 140] Train Loss: 0.0535 | Val Loss: 0.0770\n",
      "[Fold 4 Epoch 150] Train Loss: 0.0579 | Val Loss: 0.0835\n",
      "[Fold 4 Epoch 160] Train Loss: 0.0637 | Val Loss: 0.0781\n",
      "[Fold 4 Epoch 170] Train Loss: 0.0603 | Val Loss: 0.0717\n",
      "[Fold 4 Epoch 180] Train Loss: 0.0548 | Val Loss: 0.0759\n",
      "[Fold 4 Epoch 190] Train Loss: 0.0614 | Val Loss: 0.0729\n",
      "[Fold 4 Epoch 200] Train Loss: 0.0525 | Val Loss: 0.0729\n",
      "Evaluating fold 4 ...\n",
      "R²: 0.1876\n",
      "RMSE: 0.2701\n",
      "\n",
      "--- Fold 5 ---\n",
      "[Fold 5 Epoch 10] Train Loss: 0.2624 | Val Loss: 0.0748\n",
      "[Fold 5 Epoch 20] Train Loss: 0.1486 | Val Loss: 0.0748\n",
      "[Fold 5 Epoch 30] Train Loss: 0.1012 | Val Loss: 0.0627\n",
      "[Fold 5 Epoch 40] Train Loss: 0.1152 | Val Loss: 0.0638\n",
      "[Fold 5 Epoch 50] Train Loss: 0.1065 | Val Loss: 0.0610\n",
      "[Fold 5 Epoch 60] Train Loss: 0.0898 | Val Loss: 0.0902\n",
      "[Fold 5 Epoch 70] Train Loss: 0.0856 | Val Loss: 0.0610\n",
      "[Fold 5 Epoch 80] Train Loss: 0.0822 | Val Loss: 0.0593\n",
      "[Fold 5 Epoch 90] Train Loss: 0.0754 | Val Loss: 0.0596\n",
      "[Fold 5 Epoch 100] Train Loss: 0.0689 | Val Loss: 0.0593\n",
      "[Fold 5 Epoch 110] Train Loss: 0.0711 | Val Loss: 0.0594\n",
      "[Fold 5 Epoch 120] Train Loss: 0.0780 | Val Loss: 0.0587\n",
      "[Fold 5 Epoch 130] Train Loss: 0.0741 | Val Loss: 0.0570\n",
      "[Fold 5 Epoch 140] Train Loss: 0.0676 | Val Loss: 0.0576\n",
      "[Fold 5 Epoch 150] Train Loss: 0.0609 | Val Loss: 0.0596\n",
      "[Fold 5 Epoch 160] Train Loss: 0.0712 | Val Loss: 0.0629\n",
      "[Fold 5 Epoch 170] Train Loss: 0.0599 | Val Loss: 0.0594\n",
      "[Fold 5 Epoch 180] Train Loss: 0.0649 | Val Loss: 0.0553\n",
      "[Fold 5 Epoch 190] Train Loss: 0.0671 | Val Loss: 0.0544\n",
      "[Fold 5 Epoch 200] Train Loss: 0.0742 | Val Loss: 0.0544\n",
      "Evaluating fold 5 ...\n",
      "R²: 0.2146\n",
      "RMSE: 0.2333\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "Fold 1: R² = 0.2815, RMSE = 0.2009\n",
      "Fold 2: R² = 0.0414, RMSE = 0.3164\n",
      "Fold 3: R² = 0.3211, RMSE = 0.2177\n",
      "Fold 4: R² = 0.1876, RMSE = 0.2701\n",
      "Fold 5: R² = 0.2146, RMSE = 0.2333\n",
      "\n",
      "Average R²: 0.2092 ± 0.0963\n",
      "Average RMSE: 0.2477 ± 0.0413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-11 13:44:54,438] A new study created in RDB with name: GNN-mixed model base128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88356c72415549ff8fc54bdfacae8084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.0204\n",
      "RMSE: 0.2346\n",
      "R²: 0.0025\n",
      "RMSE: 0.3227\n",
      "R²: -0.1620\n",
      "RMSE: 0.2848\n",
      "R²: -0.1515\n",
      "RMSE: 0.3215\n",
      "R²: 0.0719\n",
      "RMSE: 0.2536\n",
      "[I 2025-03-11 13:51:46,503] Trial 0 finished with value: 0.28346090601663115 and parameters: {'learning_rate': 0.003, 'dropout': 0.5, 'num_layers': 5}. Best is trial 0 with value: 0.28346090601663115.\n",
      "R²: 0.0720\n",
      "RMSE: 0.2284\n",
      "R²: -0.1503\n",
      "RMSE: 0.3466\n",
      "R²: 0.0486\n",
      "RMSE: 0.2577\n",
      "R²: 0.0415\n",
      "RMSE: 0.2933\n",
      "R²: 0.1484\n",
      "RMSE: 0.2429\n",
      "[I 2025-03-11 13:57:15,721] Trial 1 finished with value: 0.27378467165679127 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 6}. Best is trial 1 with value: 0.27378467165679127.\n",
      "R²: -0.4411\n",
      "RMSE: 0.2846\n",
      "R²: -0.0638\n",
      "RMSE: 0.3333\n",
      "R²: -3.4555\n",
      "RMSE: 0.5577\n",
      "R²: -0.5191\n",
      "RMSE: 0.3693\n",
      "R²: -3.0766\n",
      "RMSE: 0.5315\n",
      "[I 2025-03-11 14:02:19,654] Trial 2 finished with value: 0.4152780603625663 and parameters: {'learning_rate': 0.0003, 'dropout': 0.5, 'num_layers': 2}. Best is trial 1 with value: 0.27378467165679127.\n",
      "R²: -1.1889\n",
      "RMSE: 0.3507\n",
      "R²: -0.4554\n",
      "RMSE: 0.3899\n",
      "R²: 0.0992\n",
      "RMSE: 0.2508\n",
      "R²: -1.0552\n",
      "RMSE: 0.4295\n",
      "R²: -0.1956\n",
      "RMSE: 0.2879\n",
      "[I 2025-03-11 14:05:17,128] Trial 3 finished with value: 0.3417413809750668 and parameters: {'learning_rate': 0.0001, 'dropout': 0.1, 'num_layers': 2}. Best is trial 1 with value: 0.27378467165679127.\n",
      "R²: -0.4185\n",
      "RMSE: 0.2823\n",
      "R²: -0.5133\n",
      "RMSE: 0.3975\n",
      "R²: -0.1177\n",
      "RMSE: 0.2793\n",
      "R²: -0.3256\n",
      "RMSE: 0.3450\n",
      "R²: 0.0702\n",
      "RMSE: 0.2538\n",
      "[I 2025-03-11 14:09:10,314] Trial 4 finished with value: 0.31159568356215583 and parameters: {'learning_rate': 0.003, 'dropout': 0.5, 'num_layers': 3}. Best is trial 1 with value: 0.27378467165679127.\n",
      "[I 2025-03-11 14:09:15,217] Trial 5 pruned. \n",
      "[I 2025-03-11 14:09:16,570] Trial 6 pruned. \n",
      "[I 2025-03-11 14:09:18,438] Trial 7 pruned. \n",
      "[I 2025-03-11 14:09:19,741] Trial 8 pruned. \n",
      "R²: -0.0756\n",
      "RMSE: 0.2458\n",
      "R²: -0.0741\n",
      "RMSE: 0.3349\n",
      "R²: -0.0247\n",
      "RMSE: 0.2675\n",
      "R²: -0.0886\n",
      "RMSE: 0.3126\n",
      "R²: 0.0610\n",
      "RMSE: 0.2551\n",
      "[I 2025-03-11 14:12:33,540] Trial 9 finished with value: 0.2831835453286605 and parameters: {'learning_rate': 0.0003, 'dropout': 0.1, 'num_layers': 2}. Best is trial 1 with value: 0.27378467165679127.\n",
      "R²: -0.0406\n",
      "RMSE: 0.2418\n",
      "R²: -0.4204\n",
      "RMSE: 0.3851\n",
      "R²: 0.1829\n",
      "RMSE: 0.2388\n",
      "R²: 0.1245\n",
      "RMSE: 0.2804\n",
      "R²: -0.1878\n",
      "RMSE: 0.2869\n",
      "[I 2025-03-11 14:20:08,784] Trial 10 finished with value: 0.28661088730894213 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 6}. Best is trial 1 with value: 0.27378467165679127.\n",
      "[I 2025-03-11 14:20:10,784] Trial 11 pruned. \n",
      "[I 2025-03-11 14:20:12,413] Trial 12 pruned. \n",
      "[I 2025-03-11 14:20:16,643] Trial 13 pruned. \n",
      "[I 2025-03-11 14:20:18,526] Trial 14 pruned. \n",
      "[I 2025-03-11 14:20:19,990] Trial 15 pruned. \n",
      "[I 2025-03-11 14:20:24,754] Trial 16 pruned. \n",
      "R²: 0.0210\n",
      "RMSE: 0.2345\n",
      "R²: 0.1071\n",
      "RMSE: 0.3054\n",
      "R²: 0.1279\n",
      "RMSE: 0.2467\n",
      "R²: 0.0539\n",
      "RMSE: 0.2914\n",
      "R²: 0.0969\n",
      "RMSE: 0.2502\n",
      "[I 2025-03-11 14:28:48,227] Trial 17 finished with value: 0.2656473948110836 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 6}. Best is trial 17 with value: 0.2656473948110836.\n",
      "R²: 0.0191\n",
      "RMSE: 0.2348\n",
      "R²: 0.1268\n",
      "RMSE: 0.3020\n",
      "R²: -0.2956\n",
      "RMSE: 0.3008\n",
      "R²: -0.2358\n",
      "RMSE: 0.3331\n",
      "R²: 0.1752\n",
      "RMSE: 0.2391\n",
      "[I 2025-03-11 14:34:09,753] Trial 18 finished with value: 0.28193053946708835 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 6}. Best is trial 17 with value: 0.2656473948110836.\n",
      "R²: -0.0554\n",
      "RMSE: 0.2435\n",
      "R²: -0.1989\n",
      "RMSE: 0.3538\n",
      "R²: 0.1172\n",
      "RMSE: 0.2483\n",
      "R²: 0.1324\n",
      "RMSE: 0.2791\n",
      "R²: -0.1309\n",
      "RMSE: 0.2799\n",
      "[I 2025-03-11 14:41:22,864] Trial 19 finished with value: 0.28093024339034345 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 6}. Best is trial 17 with value: 0.2656473948110836.\n",
      "[I 2025-03-11 14:41:28,565] Trial 20 pruned. \n",
      "[I 2025-03-11 14:41:30,704] Trial 21 pruned. \n",
      "[I 2025-03-11 14:41:32,814] Trial 22 pruned. \n",
      "R²: 0.0586\n",
      "RMSE: 0.2300\n",
      "R²: -0.2098\n",
      "RMSE: 0.3554\n",
      "R²: 0.2493\n",
      "RMSE: 0.2289\n",
      "R²: -0.1439\n",
      "RMSE: 0.3205\n",
      "R²: 0.1453\n",
      "RMSE: 0.2434\n",
      "[I 2025-03-11 14:48:35,063] Trial 23 finished with value: 0.2756365093922639 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 5}. Best is trial 17 with value: 0.2656473948110836.\n",
      "[I 2025-03-11 14:48:36,944] Trial 24 pruned. \n",
      "[I 2025-03-11 14:48:42,377] Trial 25 pruned. \n",
      "[I 2025-03-11 14:48:44,450] Trial 26 pruned. \n",
      "[I 2025-03-11 14:49:09,624] Trial 27 pruned. \n",
      "[I 2025-03-11 14:49:44,353] Trial 28 pruned. \n",
      "[I 2025-03-11 14:49:46,349] Trial 29 pruned. \n",
      "\n",
      "================= Optuna Study Results =================\n",
      "Best Trial Value (RMSE): 0.2656473948110836\n",
      "Best Hyperparameters:\n",
      "  learning_rate: 0.001\n",
      "  dropout: 0.1\n",
      "  num_layers: 6\n",
      "User Attrs (R², etc.): {'avg_r2': 0.0813754677772522}\n",
      "Could not generate optimization history plot: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\n",
      "Could not generate hyperparameter importance plot: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\n",
      "Could not generate intermediate values plot: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\n",
      "\n",
      "================= End of Optuna Tuning =================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data, DataLoader as PyGDataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool, BatchNorm\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "#                             DATASET & PREP\n",
    "# =============================================================================\n",
    "\n",
    "class MolDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset that:\n",
    "      - Reads external factors from CSV\n",
    "      - Loads the corresponding pickle for the molecule's graph\n",
    "      - Converts it into a PyG Data object\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 raw_dataframe: pd.DataFrame,\n",
    "                 nx_graph_dict: dict,\n",
    "                 *,\n",
    "                 component_col: str,\n",
    "                 global_state_cols: list[str],\n",
    "                 label_col: str,\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_dataframe: The input dataframe containing molecule info.\n",
    "            nx_graph_dict: Dictionary mapping component names to networkx graphs.\n",
    "            component_col: Column name for the component.\n",
    "            global_state_cols: List of columns representing external factors.\n",
    "            label_col: Column name for the regression target.\n",
    "            transform: Any transform to apply to each PyG Data object.\n",
    "        \"\"\"\n",
    "        self.raw_dataframe = raw_dataframe\n",
    "        self.nx_graph_dict = nx_graph_dict\n",
    "        self.component_col = [component_col] if type(component_col) is str else component_col\n",
    "        self.global_state_cols = global_state_cols\n",
    "        self.label_col = [label_col] if type(label_col) is str else label_col\n",
    "        self.transform = transform\n",
    "        \n",
    "        required_cols = set(self.global_state_cols + self.label_col + self.component_col)\n",
    "        for col in required_cols:\n",
    "            if col not in self.raw_dataframe.columns:\n",
    "                raise ValueError(f\"Missing column in DataFrame: '{col}'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.raw_dataframe.iloc[idx]\n",
    "        \n",
    "        # 1. Load the molecule graph\n",
    "        component_name = row[self.component_col[0]]  # e.g. \"C23\"\n",
    "        pyg_data = self.nx_graph_dict[component_name]\n",
    "\n",
    "        # 2. Prepare the external factors\n",
    "        externals = torch.tensor(row[self.global_state_cols].values.astype(float), dtype=torch.float)\n",
    "        externals = externals.unsqueeze(0)\n",
    "\n",
    "        # 3. Prepare the label (regression target)\n",
    "        label = torch.tensor([row[self.label_col][0]], dtype=torch.float)\n",
    "\n",
    "        # 4. Attach externals & label to the Data object\n",
    "        pyg_data.externals = externals  # shape [1, external_in_dim]\n",
    "        pyg_data.y = label  # shape [1]\n",
    "\n",
    "        if self.transform:\n",
    "            pyg_data = self.transform(pyg_data)\n",
    "\n",
    "        return pyg_data\n",
    "\n",
    "\n",
    "def networkx_to_pyg(nx_graph):\n",
    "    \"\"\"\n",
    "    Convert a networkx graph to a torch_geometric.data.Data object.\n",
    "    This is a basic template; adjust for your actual node/edge features.\n",
    "    \"\"\"\n",
    "    # Sort nodes to ensure consistent ordering\n",
    "    node_mapping = {node: i for i, node in enumerate(nx_graph.nodes())}\n",
    "\n",
    "    x_list = []\n",
    "    edge_index_list = []\n",
    "    edge_attr_list = []\n",
    "\n",
    "    # Node features\n",
    "    for node in nx_graph.nodes(data=True):\n",
    "        original_id = node[0]\n",
    "        attrs = node[1]\n",
    "        symbol = attrs.get(\"symbol\", \"C\")\n",
    "        symbol_id = 0 if symbol == \"C\" else 1 if symbol == \"H\" else 2\n",
    "        x_list.append([symbol_id])\n",
    "\n",
    "    # Edge features\n",
    "    for u, v, edge_attrs in nx_graph.edges(data=True):\n",
    "        u_idx = node_mapping[u]\n",
    "        v_idx = node_mapping[v]\n",
    "        edge_index_list.append((u_idx, v_idx))\n",
    "        bde_pred = edge_attrs.get(\"bde_pred\", 0.0) or 0.0\n",
    "        bdfe_pred = edge_attrs.get(\"bdfe_pred\", 0.0) or 0.0\n",
    "        edge_attr_list.append([bde_pred, bdfe_pred])\n",
    "\n",
    "    x = torch.tensor(x_list, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                     BASE GNN MODEL (WITH DIM MATCH)\n",
    "# =============================================================================\n",
    "\n",
    "class GINE_Regression(nn.Module):\n",
    "    \"\"\"\n",
    "    A GNN for regression using GINEConv layers + edge attributes,\n",
    "    where all layers have the same hidden_dim (no dimension mismatch).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dim: int = 128,\n",
    "                 num_layers: int = 3,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encode edges from edge_in_dim to hidden_dim\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Encode nodes from node_in_dim to hidden_dim\n",
    "        self.node_encoder = nn.Linear(node_in_dim, hidden_dim)\n",
    "        \n",
    "        # Multiple GINEConv layers & corresponding BatchNorm\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Process external factors\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Encode\n",
    "        x = self.node_encoder(x)\n",
    "        edge_emb = self.edge_encoder(edge_attr)\n",
    "        \n",
    "        # Pass through GINEConv layers\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Global pooling\n",
    "        graph_emb = global_mean_pool(x, batch)\n",
    "\n",
    "        # Process external factors\n",
    "        ext_emb = self.externals_mlp(data.externals)\n",
    "\n",
    "        # Combine & regress\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)\n",
    "        out = self.final_regressor(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                   TRAIN/VALID/EVALUATION UTILS\n",
    "# =============================================================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_data)\n",
    "        y = batch_data.y.to(device).view(-1)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "        count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            preds = model(batch_data)\n",
    "            y = batch_data.y.to(device).view(-1)\n",
    "            loss = criterion(preds, y)\n",
    "            total_loss += loss.item() * batch_data.num_graphs\n",
    "            count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset loader and compute R² and RMSE.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            preds = model(batch)\n",
    "            y_true.append(batch.y.cpu())\n",
    "            y_pred.append(preds.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true).numpy().squeeze()\n",
    "    y_pred = torch.cat(y_pred).numpy().squeeze()\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return r2, rmse\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                             DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "env_file = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\dataset02.xlsx\"\n",
    "data = pd.read_excel(env_file, engine='openpyxl').dropna(subset=['degradation_rate'])\n",
    "data['seawater'] = data['seawater'].map({'art': 1, 'sea': 0})\n",
    "\n",
    "folder_path = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\"\n",
    "graph_pickles = [f for f in os.listdir(folder_path) if f.endswith(\".pkl\")]\n",
    "\n",
    "base_dir = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\"\n",
    "if os.path.exists(base_dir):\n",
    "    print(\"Directory exists:\", base_dir)\n",
    "    print(\"Files in directory:\", os.listdir(base_dir))\n",
    "else:\n",
    "    print(f\"Error: Directory {base_dir} does not exist!\")\n",
    "\n",
    "compounds = data.component.unique()\n",
    "graphs_dict = {}\n",
    "for compound, graph_pickle in zip(compounds, graph_pickles):\n",
    "    with open(os.path.join(base_dir, graph_pickle), 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "        graphs_dict[compound] = networkx_to_pyg(graph)\n",
    "\n",
    "dataset = MolDataset(\n",
    "    raw_dataframe=data,\n",
    "    nx_graph_dict=graphs_dict,\n",
    "    component_col=\"component\",\n",
    "    global_state_cols=[\"temperature\", \"concentration\", \"time\", \"seawater\"],\n",
    "    label_col=\"degradation_rate\",\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "#                      CROSS-VALIDATION (FIXED MODEL)\n",
    "# =============================================================================\n",
    "from torch_geometric.data import DataLoader as PyGDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 5  # number of folds\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"\\n--- Fold {fold + 1} ---\")\n",
    "\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "    val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = GINE_Regression(\n",
    "        node_in_dim=1,\n",
    "        edge_in_dim=2,\n",
    "        external_in_dim=4,\n",
    "        hidden_dim=16,  # Example hidden_dim\n",
    "        num_layers=5,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    num_epochs = 200\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[Fold {fold + 1} Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    print(f\"Evaluating fold {fold + 1} ...\")\n",
    "    r2, rmse = evaluate_model(model, val_loader, device)\n",
    "    fold_results.append({\"fold\": fold + 1, \"r2\": r2, \"rmse\": rmse})\n",
    "\n",
    "r2_scores = [res[\"r2\"] for res in fold_results]\n",
    "rmse_scores = [res[\"rmse\"] for res in fold_results]\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "for res in fold_results:\n",
    "    print(f\"Fold {res['fold']}: R² = {res['r2']:.4f}, RMSE = {res['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#             IMPROVED MODEL WITH TRAPEZOID DIMENSIONS & PROJECTIONS\n",
    "# =============================================================================\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "\n",
    "def build_trapezoid_dims(num_layers):\n",
    "    \"\"\"\n",
    "    Build a list of hidden dimensions in a trapezoid manner:\n",
    "    up to the first 4 layers: [128, 64, 32, 16]\n",
    "    if num_layers > 4, extend with 16 for extra layers.\n",
    "    \"\"\"\n",
    "    base_dims = [128, 64, 32, 16]\n",
    "    if num_layers > 4:\n",
    "        extra_layers = num_layers - 4\n",
    "        return base_dims + [16] * extra_layers\n",
    "    else:\n",
    "        return base_dims[:num_layers]\n",
    "\n",
    "\n",
    "class GINE_RegressionTrapezoid(nn.Module):\n",
    "    \"\"\"\n",
    "    A GINEConv-based regression model that uses a list of hidden dimensions\n",
    "    to build layers with decreasing size (trapezoid architecture),\n",
    "    ensuring dimension consistency with projection layers between convs.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dims: list,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # For the first layer, encode edges to hidden_dims[0], and encode nodes as well\n",
    "        self.initial_edge_encoder = nn.Linear(edge_in_dim, hidden_dims[0])\n",
    "        self.initial_node_encoder = nn.Linear(node_in_dim, hidden_dims[0])\n",
    "\n",
    "        # We'll build each GINEConv to transform dimension: hidden_dims[i] -> hidden_dims[i].\n",
    "        # After each conv i, if i < len(hidden_dims)-1, we project to hidden_dims[i+1].\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.projections = nn.ModuleList()  # for node features\n",
    "        self.edge_projections = nn.ModuleList()  # for edge features\n",
    "\n",
    "        for i in range(len(hidden_dims)):\n",
    "            # GINEConv's internal MLP\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i])\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dims[i]))\n",
    "\n",
    "            # If there's a next layer, we need a projection from hidden_dims[i] -> hidden_dims[i+1]\n",
    "            if i < len(hidden_dims) - 1:\n",
    "                self.projections.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "                self.edge_projections.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            else:\n",
    "                # No projection needed for the last layer\n",
    "                self.projections.append(None)\n",
    "                self.edge_projections.append(None)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "\n",
    "        # We'll process external factors to the final dimension\n",
    "        final_dim = hidden_dims[-1]\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, final_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(final_dim, final_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression: combine final node features + final external features\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(final_dim + final_dim, final_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(final_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # 1) Encode node/edge to hidden_dims[0]\n",
    "        x = self.initial_node_encoder(x)\n",
    "        edge_emb = self.initial_edge_encoder(edge_attr)\n",
    "\n",
    "        # 2) Pass through each GINEConv\n",
    "        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):\n",
    "            # GINEConv forward\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout_layer(x)\n",
    "\n",
    "            # If there's a next layer, project node features and edge_emb to hidden_dims[i+1]\n",
    "            if i < len(self.projections) - 1 and self.projections[i] is not None:\n",
    "                x = self.projections[i](x)\n",
    "                edge_emb = self.edge_projections[i](edge_emb)\n",
    "\n",
    "        # 3) Global mean pooling\n",
    "        graph_emb = global_mean_pool(x, batch)\n",
    "\n",
    "        # 4) Process external factors into final dimension\n",
    "        ext_emb = self.externals_mlp(data.externals)\n",
    "\n",
    "        # 5) Concatenate and final regression\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)\n",
    "        out = self.final_regressor(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                          OPTUNA OBJECTIVE FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna.\n",
    "    We do k-fold cross validation using a new GNN model that\n",
    "    sets hidden_dims = [256]*num_layers (all layers = 256).\n",
    "    Return the average RMSE (lower = better).\n",
    "    We still compute R² for reference and store it in user_attrs.\n",
    "    Includes both Optuna pruning and custom early stopping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameter search space\n",
    "    lr = trial.suggest_categorical(\"learning_rate\", [1e-3, 3e-3, 1e-4, 3e-4])\n",
    "    dropout = trial.suggest_categorical(\"dropout\", [0.1, 0.5])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "\n",
    "    # ==========================\n",
    "    # CHANGE: all hidden_dims=128\n",
    "    # ==========================\n",
    "    hidden_dims = [128] * num_layers\n",
    "\n",
    "    # Early stopping parameters\n",
    "    max_epochs = 100\n",
    "    patience = 10\n",
    "    min_delta = 1e-5\n",
    "\n",
    "    # Prepare 5-fold CV\n",
    "    kf_local = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf_local.split(dataset)):\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "        val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "        # Build the GNN model with uniform 256-dim layers\n",
    "        model = GINE_RegressionTrapezoid(\n",
    "            node_in_dim=1,\n",
    "            edge_in_dim=2,\n",
    "            external_in_dim=4,\n",
    "            hidden_dims=hidden_dims,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Early-stopping tracking\n",
    "        best_val_loss = float(\"inf\")\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            # Train + Validate\n",
    "            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "            val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "            # Report intermediate metric to Optuna (for pruning visualization)\n",
    "            trial.report(val_loss, step=epoch)\n",
    "\n",
    "            # Check if Optuna wants to prune (e.g., median pruner)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "            # Custom Early Stopping\n",
    "            if val_loss < (best_val_loss - min_delta):\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    # stop training early, but complete the trial\n",
    "                    break\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        r2_fold, rmse_fold = evaluate_model(model, val_loader, device)\n",
    "        rmse_scores.append(rmse_fold)\n",
    "        r2_scores.append(r2_fold)\n",
    "\n",
    "    # Averages over folds\n",
    "    avg_rmse = float(np.mean(rmse_scores))\n",
    "    avg_r2 = float(np.mean(r2_scores))\n",
    "\n",
    "    # Log the R² so it appears in the dashboard's user_attrs\n",
    "    trial.set_user_attr(\"avg_r2\", avg_r2)\n",
    "\n",
    "    # Return RMSE (the main metric we want to minimize)\n",
    "    return avg_rmse\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                           OPTUNA STUDY & DASHBOARD\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(\n",
    "        storage=\"sqlite:///gnn_mix_op07.sqlite3\",\n",
    "        study_name=\"GNN-mixed modelbase128\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "    print(\"\\n================= Optuna Study Results =================\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Best Trial Value (RMSE): {best_trial.value}\")\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for key, val in best_trial.params.items():\n",
    "        print(f\"  {key}: {val}\")\n",
    "    print(f\"User Attrs (R², etc.): {best_trial.user_attrs}\")\n",
    "\n",
    "    try:\n",
    "        fig1 = vis.plot_optimization_history(study)\n",
    "        fig1.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate optimization history plot: {e}\")\n",
    "\n",
    "    try:\n",
    "        fig2 = vis.plot_param_importances(study)\n",
    "        fig2.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate hyperparameter importance plot: {e}\")\n",
    "\n",
    "    try:\n",
    "        fig3 = vis.plot_intermediate_values(study)\n",
    "        fig3.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate intermediate values plot: {e}\")\n",
    "\n",
    "    print(\"\\n================= End of Optuna Tuning =================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1 ---\n",
      "[Fold 1 Epoch 10] Train Loss: 0.1210 | Val Loss: 0.0644\n",
      "[Fold 1 Epoch 20] Train Loss: 0.0977 | Val Loss: 0.0594\n",
      "[Fold 1 Epoch 30] Train Loss: 0.0972 | Val Loss: 0.0560\n",
      "[Fold 1 Epoch 40] Train Loss: 0.0762 | Val Loss: 0.0503\n",
      "[Fold 1 Epoch 50] Train Loss: 0.0821 | Val Loss: 0.0633\n",
      "[Fold 1 Epoch 60] Train Loss: 0.0713 | Val Loss: 0.0479\n",
      "[Fold 1 Epoch 70] Train Loss: 0.0706 | Val Loss: 0.0467\n",
      "[Fold 1 Epoch 80] Train Loss: 0.0635 | Val Loss: 0.0454\n",
      "[Fold 1 Epoch 90] Train Loss: 0.0611 | Val Loss: 0.0453\n",
      "[Fold 1 Epoch 100] Train Loss: 0.0603 | Val Loss: 0.0442\n",
      "[Fold 1 Epoch 110] Train Loss: 0.0595 | Val Loss: 0.0426\n",
      "[Fold 1 Epoch 120] Train Loss: 0.0574 | Val Loss: 0.0488\n",
      "[Fold 1 Epoch 130] Train Loss: 0.0613 | Val Loss: 0.0419\n",
      "[Fold 1 Epoch 140] Train Loss: 0.0658 | Val Loss: 0.0567\n",
      "[Fold 1 Epoch 150] Train Loss: 0.0585 | Val Loss: 0.0405\n",
      "[Fold 1 Epoch 160] Train Loss: 0.0558 | Val Loss: 0.0384\n",
      "[Fold 1 Epoch 170] Train Loss: 0.0468 | Val Loss: 0.0374\n",
      "[Fold 1 Epoch 180] Train Loss: 0.0512 | Val Loss: 0.0356\n",
      "[Fold 1 Epoch 190] Train Loss: 0.0474 | Val Loss: 0.0354\n",
      "[Fold 1 Epoch 200] Train Loss: 0.0444 | Val Loss: 0.0359\n",
      "[Fold 1 Epoch 210] Train Loss: 0.0394 | Val Loss: 0.0336\n",
      "[Fold 1 Epoch 220] Train Loss: 0.0434 | Val Loss: 0.0326\n",
      "[Fold 1 Epoch 230] Train Loss: 0.0401 | Val Loss: 0.0314\n",
      "[Fold 1 Epoch 240] Train Loss: 0.0383 | Val Loss: 0.0488\n",
      "[Fold 1 Epoch 250] Train Loss: 0.0397 | Val Loss: 0.0350\n",
      "[Fold 1 Epoch 260] Train Loss: 0.0430 | Val Loss: 0.0327\n",
      "[Fold 1 Epoch 270] Train Loss: 0.0459 | Val Loss: 0.0335\n",
      "[Fold 1 Epoch 280] Train Loss: 0.0491 | Val Loss: 0.0332\n",
      "[Fold 1 Epoch 290] Train Loss: 0.0391 | Val Loss: 0.0318\n",
      "[Fold 1 Epoch 300] Train Loss: 0.0494 | Val Loss: 0.0369\n",
      "[Fold 1 Epoch 310] Train Loss: 0.0403 | Val Loss: 0.0331\n",
      "[Fold 1 Epoch 320] Train Loss: 0.0364 | Val Loss: 0.0322\n",
      "[Fold 1 Epoch 330] Train Loss: 0.0393 | Val Loss: 0.0375\n",
      "[Fold 1 Epoch 340] Train Loss: 0.0407 | Val Loss: 0.0330\n",
      "[Fold 1 Epoch 350] Train Loss: 0.0349 | Val Loss: 0.0328\n",
      "[Fold 1 Epoch 360] Train Loss: 0.0377 | Val Loss: 0.0331\n",
      "[Fold 1 Epoch 370] Train Loss: 0.0391 | Val Loss: 0.0359\n",
      "[Fold 1 Epoch 380] Train Loss: 0.0383 | Val Loss: 0.0335\n",
      "[Fold 1 Epoch 390] Train Loss: 0.0405 | Val Loss: 0.0356\n",
      "[Fold 1 Epoch 400] Train Loss: 0.0415 | Val Loss: 0.0372\n",
      "[Fold 1 Epoch 410] Train Loss: 0.0354 | Val Loss: 0.0356\n",
      "[Fold 1 Epoch 420] Train Loss: 0.0399 | Val Loss: 0.0331\n",
      "[Fold 1 Epoch 430] Train Loss: 0.0331 | Val Loss: 0.0365\n",
      "[Fold 1 Epoch 440] Train Loss: 0.0362 | Val Loss: 0.0337\n",
      "[Fold 1 Epoch 450] Train Loss: 0.0394 | Val Loss: 0.0401\n",
      "[Fold 1 Epoch 460] Train Loss: 0.0390 | Val Loss: 0.0325\n",
      "[Fold 1 Epoch 470] Train Loss: 0.0358 | Val Loss: 0.0340\n",
      "[Fold 1 Epoch 480] Train Loss: 0.0417 | Val Loss: 0.0393\n",
      "[Fold 1 Epoch 490] Train Loss: 0.0353 | Val Loss: 0.0337\n",
      "[Fold 1 Epoch 500] Train Loss: 0.0379 | Val Loss: 0.0332\n",
      "Evaluating fold 1 ...\n",
      "R²: 0.4096\n",
      "RMSE: 0.1821\n",
      "\n",
      "--- Fold 2 ---\n",
      "[Fold 2 Epoch 10] Train Loss: 0.1002 | Val Loss: 0.1067\n",
      "[Fold 2 Epoch 20] Train Loss: 0.0770 | Val Loss: 0.1244\n",
      "[Fold 2 Epoch 30] Train Loss: 0.0866 | Val Loss: 0.1121\n",
      "[Fold 2 Epoch 40] Train Loss: 0.0736 | Val Loss: 0.1034\n",
      "[Fold 2 Epoch 50] Train Loss: 0.0703 | Val Loss: 0.1125\n",
      "[Fold 2 Epoch 60] Train Loss: 0.0685 | Val Loss: 0.1055\n",
      "[Fold 2 Epoch 70] Train Loss: 0.0655 | Val Loss: 0.0950\n",
      "[Fold 2 Epoch 80] Train Loss: 0.0740 | Val Loss: 0.1013\n",
      "[Fold 2 Epoch 90] Train Loss: 0.0673 | Val Loss: 0.1009\n",
      "[Fold 2 Epoch 100] Train Loss: 0.0666 | Val Loss: 0.0974\n",
      "[Fold 2 Epoch 110] Train Loss: 0.0635 | Val Loss: 0.0966\n",
      "[Fold 2 Epoch 120] Train Loss: 0.0598 | Val Loss: 0.0973\n",
      "[Fold 2 Epoch 130] Train Loss: 0.0609 | Val Loss: 0.1010\n",
      "[Fold 2 Epoch 140] Train Loss: 0.0643 | Val Loss: 0.1122\n",
      "[Fold 2 Epoch 150] Train Loss: 0.0560 | Val Loss: 0.0978\n",
      "[Fold 2 Epoch 160] Train Loss: 0.0555 | Val Loss: 0.0931\n",
      "[Fold 2 Epoch 170] Train Loss: 0.0563 | Val Loss: 0.0863\n",
      "[Fold 2 Epoch 180] Train Loss: 0.0527 | Val Loss: 0.0878\n",
      "[Fold 2 Epoch 190] Train Loss: 0.0539 | Val Loss: 0.0913\n",
      "[Fold 2 Epoch 200] Train Loss: 0.0494 | Val Loss: 0.0848\n",
      "[Fold 2 Epoch 210] Train Loss: 0.0509 | Val Loss: 0.0840\n",
      "[Fold 2 Epoch 220] Train Loss: 0.0516 | Val Loss: 0.0746\n",
      "[Fold 2 Epoch 230] Train Loss: 0.0485 | Val Loss: 0.0723\n",
      "[Fold 2 Epoch 240] Train Loss: 0.0476 | Val Loss: 0.0778\n",
      "[Fold 2 Epoch 250] Train Loss: 0.0505 | Val Loss: 0.0706\n",
      "[Fold 2 Epoch 260] Train Loss: 0.0480 | Val Loss: 0.0796\n",
      "[Fold 2 Epoch 270] Train Loss: 0.0482 | Val Loss: 0.0779\n",
      "[Fold 2 Epoch 280] Train Loss: 0.0518 | Val Loss: 0.0822\n",
      "[Fold 2 Epoch 290] Train Loss: 0.0518 | Val Loss: 0.0647\n",
      "[Fold 2 Epoch 300] Train Loss: 0.0460 | Val Loss: 0.0610\n",
      "[Fold 2 Epoch 310] Train Loss: 0.0472 | Val Loss: 0.0626\n",
      "[Fold 2 Epoch 320] Train Loss: 0.0425 | Val Loss: 0.0634\n",
      "[Fold 2 Epoch 330] Train Loss: 0.0423 | Val Loss: 0.0620\n",
      "[Fold 2 Epoch 340] Train Loss: 0.0548 | Val Loss: 0.0768\n",
      "[Fold 2 Epoch 350] Train Loss: 0.0472 | Val Loss: 0.0605\n",
      "[Fold 2 Epoch 360] Train Loss: 0.0477 | Val Loss: 0.0938\n",
      "[Fold 2 Epoch 370] Train Loss: 0.0428 | Val Loss: 0.0465\n",
      "[Fold 2 Epoch 380] Train Loss: 0.0377 | Val Loss: 0.0587\n",
      "[Fold 2 Epoch 390] Train Loss: 0.0392 | Val Loss: 0.0487\n",
      "[Fold 2 Epoch 400] Train Loss: 0.0434 | Val Loss: 0.0542\n",
      "[Fold 2 Epoch 410] Train Loss: 0.0399 | Val Loss: 0.0574\n",
      "[Fold 2 Epoch 420] Train Loss: 0.0386 | Val Loss: 0.0506\n",
      "[Fold 2 Epoch 430] Train Loss: 0.0414 | Val Loss: 0.0580\n",
      "[Fold 2 Epoch 440] Train Loss: 0.0431 | Val Loss: 0.0675\n",
      "[Fold 2 Epoch 450] Train Loss: 0.0399 | Val Loss: 0.0634\n",
      "[Fold 2 Epoch 460] Train Loss: 0.0405 | Val Loss: 0.0719\n",
      "[Fold 2 Epoch 470] Train Loss: 0.0411 | Val Loss: 0.0584\n",
      "[Fold 2 Epoch 480] Train Loss: 0.0405 | Val Loss: 0.0614\n",
      "[Fold 2 Epoch 490] Train Loss: 0.0419 | Val Loss: 0.0615\n",
      "[Fold 2 Epoch 500] Train Loss: 0.0390 | Val Loss: 0.0603\n",
      "Evaluating fold 2 ...\n",
      "R²: 0.4225\n",
      "RMSE: 0.2456\n",
      "\n",
      "--- Fold 3 ---\n",
      "[Fold 3 Epoch 10] Train Loss: 0.1249 | Val Loss: 0.0701\n",
      "[Fold 3 Epoch 20] Train Loss: 0.0956 | Val Loss: 0.0708\n",
      "[Fold 3 Epoch 30] Train Loss: 0.0850 | Val Loss: 0.0544\n",
      "[Fold 3 Epoch 40] Train Loss: 0.0645 | Val Loss: 0.0568\n",
      "[Fold 3 Epoch 50] Train Loss: 0.0613 | Val Loss: 0.0536\n",
      "[Fold 3 Epoch 60] Train Loss: 0.0654 | Val Loss: 0.0507\n",
      "[Fold 3 Epoch 70] Train Loss: 0.0661 | Val Loss: 0.0506\n",
      "[Fold 3 Epoch 80] Train Loss: 0.0620 | Val Loss: 0.0551\n",
      "[Fold 3 Epoch 90] Train Loss: 0.0582 | Val Loss: 0.0506\n",
      "[Fold 3 Epoch 100] Train Loss: 0.0618 | Val Loss: 0.0524\n",
      "[Fold 3 Epoch 110] Train Loss: 0.0715 | Val Loss: 0.0512\n",
      "[Fold 3 Epoch 120] Train Loss: 0.0580 | Val Loss: 0.0548\n",
      "[Fold 3 Epoch 130] Train Loss: 0.0629 | Val Loss: 0.0501\n",
      "[Fold 3 Epoch 140] Train Loss: 0.0539 | Val Loss: 0.0523\n",
      "[Fold 3 Epoch 150] Train Loss: 0.0517 | Val Loss: 0.0514\n",
      "[Fold 3 Epoch 160] Train Loss: 0.0552 | Val Loss: 0.0495\n",
      "[Fold 3 Epoch 170] Train Loss: 0.0552 | Val Loss: 0.0563\n",
      "[Fold 3 Epoch 180] Train Loss: 0.0528 | Val Loss: 0.0494\n",
      "[Fold 3 Epoch 190] Train Loss: 0.0599 | Val Loss: 0.0460\n",
      "[Fold 3 Epoch 200] Train Loss: 0.0485 | Val Loss: 0.0488\n",
      "[Fold 3 Epoch 210] Train Loss: 0.0451 | Val Loss: 0.0523\n",
      "[Fold 3 Epoch 220] Train Loss: 0.0486 | Val Loss: 0.0487\n",
      "[Fold 3 Epoch 230] Train Loss: 0.0496 | Val Loss: 0.0428\n",
      "[Fold 3 Epoch 240] Train Loss: 0.0425 | Val Loss: 0.0414\n",
      "[Fold 3 Epoch 250] Train Loss: 0.0403 | Val Loss: 0.0449\n",
      "[Fold 3 Epoch 260] Train Loss: 0.0495 | Val Loss: 0.0453\n",
      "[Fold 3 Epoch 270] Train Loss: 0.0413 | Val Loss: 0.0429\n",
      "[Fold 3 Epoch 280] Train Loss: 0.0435 | Val Loss: 0.0379\n",
      "[Fold 3 Epoch 290] Train Loss: 0.0427 | Val Loss: 0.0393\n",
      "[Fold 3 Epoch 300] Train Loss: 0.0440 | Val Loss: 0.0377\n",
      "[Fold 3 Epoch 310] Train Loss: 0.0384 | Val Loss: 0.0385\n",
      "[Fold 3 Epoch 320] Train Loss: 0.0427 | Val Loss: 0.0403\n",
      "[Fold 3 Epoch 330] Train Loss: 0.0387 | Val Loss: 0.0448\n",
      "[Fold 3 Epoch 340] Train Loss: 0.0430 | Val Loss: 0.0439\n",
      "[Fold 3 Epoch 350] Train Loss: 0.0402 | Val Loss: 0.0357\n",
      "[Fold 3 Epoch 360] Train Loss: 0.0368 | Val Loss: 0.0353\n",
      "[Fold 3 Epoch 370] Train Loss: 0.0411 | Val Loss: 0.0431\n",
      "[Fold 3 Epoch 380] Train Loss: 0.0402 | Val Loss: 0.0364\n",
      "[Fold 3 Epoch 390] Train Loss: 0.0381 | Val Loss: 0.0357\n",
      "[Fold 3 Epoch 400] Train Loss: 0.0405 | Val Loss: 0.0375\n",
      "[Fold 3 Epoch 410] Train Loss: 0.0413 | Val Loss: 0.0340\n",
      "[Fold 3 Epoch 420] Train Loss: 0.0420 | Val Loss: 0.0343\n",
      "[Fold 3 Epoch 430] Train Loss: 0.0405 | Val Loss: 0.0343\n",
      "[Fold 3 Epoch 440] Train Loss: 0.0397 | Val Loss: 0.0340\n",
      "[Fold 3 Epoch 450] Train Loss: 0.0402 | Val Loss: 0.0360\n",
      "[Fold 3 Epoch 460] Train Loss: 0.0382 | Val Loss: 0.0396\n",
      "[Fold 3 Epoch 470] Train Loss: 0.0411 | Val Loss: 0.0350\n",
      "[Fold 3 Epoch 480] Train Loss: 0.0426 | Val Loss: 0.0347\n",
      "[Fold 3 Epoch 490] Train Loss: 0.0367 | Val Loss: 0.0351\n",
      "[Fold 3 Epoch 500] Train Loss: 0.0398 | Val Loss: 0.0314\n",
      "Evaluating fold 3 ...\n",
      "R²: 0.5500\n",
      "RMSE: 0.1772\n",
      "\n",
      "--- Fold 4 ---\n",
      "[Fold 4 Epoch 10] Train Loss: 0.1662 | Val Loss: 0.0956\n",
      "[Fold 4 Epoch 20] Train Loss: 0.1288 | Val Loss: 0.1279\n",
      "[Fold 4 Epoch 30] Train Loss: 0.0853 | Val Loss: 0.0974\n",
      "[Fold 4 Epoch 40] Train Loss: 0.0789 | Val Loss: 0.0863\n",
      "[Fold 4 Epoch 50] Train Loss: 0.0773 | Val Loss: 0.0990\n",
      "[Fold 4 Epoch 60] Train Loss: 0.0892 | Val Loss: 0.0881\n",
      "[Fold 4 Epoch 70] Train Loss: 0.0686 | Val Loss: 0.0940\n",
      "[Fold 4 Epoch 80] Train Loss: 0.0671 | Val Loss: 0.0921\n",
      "[Fold 4 Epoch 90] Train Loss: 0.0596 | Val Loss: 0.0935\n",
      "[Fold 4 Epoch 100] Train Loss: 0.0616 | Val Loss: 0.0897\n",
      "[Fold 4 Epoch 110] Train Loss: 0.0542 | Val Loss: 0.0850\n",
      "[Fold 4 Epoch 120] Train Loss: 0.0581 | Val Loss: 0.0852\n",
      "[Fold 4 Epoch 130] Train Loss: 0.0592 | Val Loss: 0.0866\n",
      "[Fold 4 Epoch 140] Train Loss: 0.0605 | Val Loss: 0.0879\n",
      "[Fold 4 Epoch 150] Train Loss: 0.0609 | Val Loss: 0.0823\n",
      "[Fold 4 Epoch 160] Train Loss: 0.0766 | Val Loss: 0.0854\n",
      "[Fold 4 Epoch 170] Train Loss: 0.0611 | Val Loss: 0.0833\n",
      "[Fold 4 Epoch 180] Train Loss: 0.0574 | Val Loss: 0.0807\n",
      "[Fold 4 Epoch 190] Train Loss: 0.0601 | Val Loss: 0.0803\n",
      "[Fold 4 Epoch 200] Train Loss: 0.0517 | Val Loss: 0.0768\n",
      "[Fold 4 Epoch 210] Train Loss: 0.0575 | Val Loss: 0.0729\n",
      "[Fold 4 Epoch 220] Train Loss: 0.0551 | Val Loss: 0.0723\n",
      "[Fold 4 Epoch 230] Train Loss: 0.0619 | Val Loss: 0.0737\n",
      "[Fold 4 Epoch 240] Train Loss: 0.0561 | Val Loss: 0.0690\n",
      "[Fold 4 Epoch 250] Train Loss: 0.0572 | Val Loss: 0.0686\n",
      "[Fold 4 Epoch 260] Train Loss: 0.0484 | Val Loss: 0.0686\n",
      "[Fold 4 Epoch 270] Train Loss: 0.0550 | Val Loss: 0.0718\n",
      "[Fold 4 Epoch 280] Train Loss: 0.0535 | Val Loss: 0.0788\n",
      "[Fold 4 Epoch 290] Train Loss: 0.0469 | Val Loss: 0.0696\n",
      "[Fold 4 Epoch 300] Train Loss: 0.0506 | Val Loss: 0.0700\n",
      "[Fold 4 Epoch 310] Train Loss: 0.0474 | Val Loss: 0.0665\n",
      "[Fold 4 Epoch 320] Train Loss: 0.0504 | Val Loss: 0.0686\n",
      "[Fold 4 Epoch 330] Train Loss: 0.0474 | Val Loss: 0.0685\n",
      "[Fold 4 Epoch 340] Train Loss: 0.0420 | Val Loss: 0.0556\n",
      "[Fold 4 Epoch 350] Train Loss: 0.0508 | Val Loss: 0.0637\n",
      "[Fold 4 Epoch 360] Train Loss: 0.0383 | Val Loss: 0.0653\n",
      "[Fold 4 Epoch 370] Train Loss: 0.0454 | Val Loss: 0.0551\n",
      "[Fold 4 Epoch 380] Train Loss: 0.0440 | Val Loss: 0.0576\n",
      "[Fold 4 Epoch 390] Train Loss: 0.0419 | Val Loss: 0.0491\n",
      "[Fold 4 Epoch 400] Train Loss: 0.0424 | Val Loss: 0.0499\n",
      "[Fold 4 Epoch 410] Train Loss: 0.0403 | Val Loss: 0.0552\n",
      "[Fold 4 Epoch 420] Train Loss: 0.0440 | Val Loss: 0.0538\n",
      "[Fold 4 Epoch 430] Train Loss: 0.0442 | Val Loss: 0.0508\n",
      "[Fold 4 Epoch 440] Train Loss: 0.0354 | Val Loss: 0.0539\n",
      "[Fold 4 Epoch 450] Train Loss: 0.0408 | Val Loss: 0.0457\n",
      "[Fold 4 Epoch 460] Train Loss: 0.0444 | Val Loss: 0.0500\n",
      "[Fold 4 Epoch 470] Train Loss: 0.0396 | Val Loss: 0.0494\n",
      "[Fold 4 Epoch 480] Train Loss: 0.0464 | Val Loss: 0.0494\n",
      "[Fold 4 Epoch 490] Train Loss: 0.0351 | Val Loss: 0.0448\n",
      "[Fold 4 Epoch 500] Train Loss: 0.0347 | Val Loss: 0.0472\n",
      "Evaluating fold 4 ...\n",
      "R²: 0.4746\n",
      "RMSE: 0.2172\n",
      "\n",
      "--- Fold 5 ---\n",
      "[Fold 5 Epoch 10] Train Loss: 0.2208 | Val Loss: 0.0688\n",
      "[Fold 5 Epoch 20] Train Loss: 0.1073 | Val Loss: 0.0548\n",
      "[Fold 5 Epoch 30] Train Loss: 0.1024 | Val Loss: 0.0553\n",
      "[Fold 5 Epoch 40] Train Loss: 0.0846 | Val Loss: 0.0596\n",
      "[Fold 5 Epoch 50] Train Loss: 0.0784 | Val Loss: 0.0583\n",
      "[Fold 5 Epoch 60] Train Loss: 0.0628 | Val Loss: 0.0872\n",
      "[Fold 5 Epoch 70] Train Loss: 0.0664 | Val Loss: 0.0566\n",
      "[Fold 5 Epoch 80] Train Loss: 0.0647 | Val Loss: 0.0594\n",
      "[Fold 5 Epoch 90] Train Loss: 0.0664 | Val Loss: 0.0573\n",
      "[Fold 5 Epoch 100] Train Loss: 0.0610 | Val Loss: 0.0547\n",
      "[Fold 5 Epoch 110] Train Loss: 0.0674 | Val Loss: 0.0570\n",
      "[Fold 5 Epoch 120] Train Loss: 0.0668 | Val Loss: 0.0541\n",
      "[Fold 5 Epoch 130] Train Loss: 0.0618 | Val Loss: 0.0540\n",
      "[Fold 5 Epoch 140] Train Loss: 0.0601 | Val Loss: 0.0535\n",
      "[Fold 5 Epoch 150] Train Loss: 0.0619 | Val Loss: 0.0549\n",
      "[Fold 5 Epoch 160] Train Loss: 0.0626 | Val Loss: 0.0563\n",
      "[Fold 5 Epoch 170] Train Loss: 0.0622 | Val Loss: 0.0554\n",
      "[Fold 5 Epoch 180] Train Loss: 0.0613 | Val Loss: 0.0542\n",
      "[Fold 5 Epoch 190] Train Loss: 0.0624 | Val Loss: 0.0529\n",
      "[Fold 5 Epoch 200] Train Loss: 0.0588 | Val Loss: 0.0523\n",
      "[Fold 5 Epoch 210] Train Loss: 0.0550 | Val Loss: 0.0527\n",
      "[Fold 5 Epoch 220] Train Loss: 0.0662 | Val Loss: 0.0518\n",
      "[Fold 5 Epoch 230] Train Loss: 0.0609 | Val Loss: 0.0505\n",
      "[Fold 5 Epoch 240] Train Loss: 0.0564 | Val Loss: 0.0507\n",
      "[Fold 5 Epoch 250] Train Loss: 0.0541 | Val Loss: 0.0484\n",
      "[Fold 5 Epoch 260] Train Loss: 0.0614 | Val Loss: 0.0423\n",
      "[Fold 5 Epoch 270] Train Loss: 0.0528 | Val Loss: 0.0388\n",
      "[Fold 5 Epoch 280] Train Loss: 0.0484 | Val Loss: 0.0369\n",
      "[Fold 5 Epoch 290] Train Loss: 0.0473 | Val Loss: 0.0471\n",
      "[Fold 5 Epoch 300] Train Loss: 0.0524 | Val Loss: 0.0341\n",
      "[Fold 5 Epoch 310] Train Loss: 0.0391 | Val Loss: 0.0348\n",
      "[Fold 5 Epoch 320] Train Loss: 0.0482 | Val Loss: 0.0361\n",
      "[Fold 5 Epoch 330] Train Loss: 0.0448 | Val Loss: 0.0362\n",
      "[Fold 5 Epoch 340] Train Loss: 0.0474 | Val Loss: 0.0350\n",
      "[Fold 5 Epoch 350] Train Loss: 0.0559 | Val Loss: 0.0320\n",
      "[Fold 5 Epoch 360] Train Loss: 0.0428 | Val Loss: 0.0342\n",
      "[Fold 5 Epoch 370] Train Loss: 0.0523 | Val Loss: 0.0376\n",
      "[Fold 5 Epoch 380] Train Loss: 0.0493 | Val Loss: 0.0307\n",
      "[Fold 5 Epoch 390] Train Loss: 0.0461 | Val Loss: 0.0366\n",
      "[Fold 5 Epoch 400] Train Loss: 0.0491 | Val Loss: 0.0328\n",
      "[Fold 5 Epoch 410] Train Loss: 0.0426 | Val Loss: 0.0337\n",
      "[Fold 5 Epoch 420] Train Loss: 0.0490 | Val Loss: 0.0325\n",
      "[Fold 5 Epoch 430] Train Loss: 0.0480 | Val Loss: 0.0325\n",
      "[Fold 5 Epoch 440] Train Loss: 0.0449 | Val Loss: 0.0347\n",
      "[Fold 5 Epoch 450] Train Loss: 0.0480 | Val Loss: 0.0317\n",
      "[Fold 5 Epoch 460] Train Loss: 0.0551 | Val Loss: 0.0334\n",
      "[Fold 5 Epoch 470] Train Loss: 0.0458 | Val Loss: 0.0361\n",
      "[Fold 5 Epoch 480] Train Loss: 0.0446 | Val Loss: 0.0313\n",
      "[Fold 5 Epoch 490] Train Loss: 0.0425 | Val Loss: 0.0332\n",
      "[Fold 5 Epoch 500] Train Loss: 0.0527 | Val Loss: 0.0336\n",
      "Evaluating fold 5 ...\n",
      "R²: 0.5154\n",
      "RMSE: 0.1833\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "Fold 1: R² = 0.4096, RMSE = 0.1821\n",
      "Fold 2: R² = 0.4225, RMSE = 0.2456\n",
      "Fold 3: R² = 0.5500, RMSE = 0.1772\n",
      "Fold 4: R² = 0.4746, RMSE = 0.2172\n",
      "Fold 5: R² = 0.5154, RMSE = 0.1833\n",
      "\n",
      "Average R²: 0.4744 ± 0.0535\n",
      "Average RMSE: 0.2011 ± 0.0264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold  #  k 折交叉验证\n",
    "\n",
    "# ---------------------\n",
    "# k-Fold Cross Validation\n",
    "# ---------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 5  # number of folds\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42) # shuffle = True 参数 随机打乱\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)): # 循环 kf.split的折叠 enumerate函数添加计数\n",
    "                                                                # fold 计数器 迭代次数\n",
    "    print(f\"\\n--- Fold {fold + 1} ---\")\n",
    "    \n",
    "    # Create train and validation subsets\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx) # 子集：训练集\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "    \n",
    "    train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True) # PyG 数据加载\n",
    "    val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Initialize a new instance of the model, optimizer, and loss function for each fold\n",
    "    model = GINE_Regression(\n",
    "        node_in_dim=1,\n",
    "        edge_in_dim=2,\n",
    "        external_in_dim=4,\n",
    "        hidden_dim=16,\n",
    "        num_layers=5,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    num_epochs = 500\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[Fold {fold + 1} Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    print(f\"Evaluating fold {fold + 1} ...\")\n",
    "    r2, rmse = evaluate_model(model, val_loader, device)\n",
    "    fold_results.append({\"fold\": fold + 1, \"r2\": r2, \"rmse\": rmse})\n",
    "\n",
    "# ---------------------\n",
    "# Summary of Cross-Validation Results\n",
    "# ---------------------\n",
    "r2_scores = [res[\"r2\"] for res in fold_results]\n",
    "rmse_scores = [res[\"rmse\"] for res in fold_results]\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "for res in fold_results:\n",
    "    print(f\"Fold {res['fold']}: R² = {res['r2']:.4f}, RMSE = {res['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfabet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
