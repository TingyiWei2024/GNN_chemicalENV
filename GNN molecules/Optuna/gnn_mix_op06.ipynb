{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader as PyGDataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GCNConv, GINEConv, global_mean_pool, BatchNorm\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset that:\n",
    "      - Reads external factors from CSV\n",
    "      - Loads the corresponding pickle for the molecule's graph\n",
    "      - Converts it into a PyG Data object\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 raw_dataframe: pd.DataFrame,\n",
    "                 nx_graph_dict: dict,\n",
    "                 *,\n",
    "                 component_col: str,\n",
    "                 global_state_cols: list[str],\n",
    "                 label_col: str,\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        \"\"\"\n",
    "        self.raw_dataframe = raw_dataframe\n",
    "        self.nx_graph_dict = nx_graph_dict\n",
    "        self.component_col = [component_col] if type(component_col) is str else component_col\n",
    "        self.global_state_cols = global_state_cols\n",
    "        self.label_col = [label_col] if type(label_col) is str else label_col\n",
    "        self.transform = transform\n",
    "        \n",
    "        required_cols = set(self.global_state_cols + self.label_col + self.component_col)\n",
    "        for col in required_cols:\n",
    "            if col not in self.raw_dataframe.columns:\n",
    "                raise ValueError(f\"Missing column in DataFrame: '{col}'\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.raw_dataframe.iloc[idx]\n",
    "        \n",
    "        # 1. Load the molecule graph\n",
    "        component_name = row[self.component_col[0]]  # e.g. \"C23\"\n",
    "        pyg_data = self.nx_graph_dict[component_name]\n",
    "\n",
    "        # 3. Prepare the external factors\n",
    "        #    Convert selected columns into a float tensor\n",
    "        externals = torch.tensor(row[self.global_state_cols].values.astype(float), dtype=torch.float)\n",
    "        externals = externals.unsqueeze(0)\n",
    "\n",
    "        # 4. Prepare the label (regression target)\n",
    "        label = torch.tensor([row[self.label_col][0]], dtype=torch.float)\n",
    "\n",
    "        # 5. Attach externals & label to the Data object for use in the model\n",
    "        #    (We can store them in Data object attributes if you like)\n",
    "        pyg_data.externals = externals  # 1D vector of external factors\n",
    "        pyg_data.y = label  # shape [1]\n",
    "\n",
    "        if self.transform:\n",
    "            pyg_data = self.transform(pyg_data)\n",
    "\n",
    "        return pyg_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def networkx_to_pyg(nx_graph):\n",
    "    \"\"\"\n",
    "    Convert a networkx graph to a torch_geometric.data.Data object.\n",
    "    This is a basic template; adjust for your actual node/edge features.\n",
    "    \"\"\"\n",
    "    # Sort nodes to ensure consistent ordering\n",
    "    # e.g. node 0, node 1, ...\n",
    "    # In some networkx graphs, node labels might be strings. We’ll map them to integers.\n",
    "    node_mapping = {node: i for i, node in enumerate(nx_graph.nodes())}\n",
    "\n",
    "    # Build lists for PyG\n",
    "    x_list = []\n",
    "    edge_index_list = []\n",
    "    edge_attr_list = []\n",
    "\n",
    "    for node in nx_graph.nodes(data=True):\n",
    "        original_id = node[0]\n",
    "        attrs = node[1]\n",
    "        # Example: 'symbol' might be in attrs, etc.\n",
    "        # For demonstration, let's store only \"symbol\" as a simple categorical embedding\n",
    "        # You might do something more sophisticated (e.g., one-hot) for real usage\n",
    "        symbol = attrs.get(\"symbol\", \"C\")\n",
    "        # Convert symbol to a simple ID (C=0, H=1, etc.) or some vector\n",
    "        # We'll do a naive approach here:\n",
    "        symbol_id = 0 if symbol == \"C\" else 1 if symbol == \"H\" else 2\n",
    "        \n",
    "        x_list.append([symbol_id])\n",
    "\n",
    "    for u, v, edge_attrs in nx_graph.edges(data=True):\n",
    "        u_idx = node_mapping[u]\n",
    "        v_idx = node_mapping[v]\n",
    "        edge_index_list.append((u_idx, v_idx))\n",
    "        # Possibly store bond features: \"bond_index\", \"bde_pred\", etc.\n",
    "        bde_pred = edge_attrs.get(\"bde_pred\", 0.0)\n",
    "        if bde_pred is None:\n",
    "            bde_pred = 0.0\n",
    "        bdfe_pred = edge_attrs.get(\"bdfe_pred\", 0.0)\n",
    "        if bdfe_pred is None:\n",
    "            bdfe_pred = 0.0\n",
    "        edge_attr_list.append([bde_pred, bdfe_pred])\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    x = torch.tensor(x_list, dtype=torch.float)  # shape [num_nodes, num_node_features]\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()  # shape [2, num_edges]\n",
    "    edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)  # shape [num_edges, edge_feat_dim]\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINE_Regression(nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dim: int = 128,\n",
    "                 num_layers: int = 3,\n",
    "                 dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        A more 'realistic' GNN for regression, using GINEConv layers + edge attributes.\n",
    "        \n",
    "        Args:\n",
    "            node_in_dim (int): Dim of node features (e.g. 1 or 3).\n",
    "            edge_in_dim (int): Dim of edge features (e.g. 2 for [bde_pred, bdfe_pred]).\n",
    "            external_in_dim (int): Dim of external factor features (e.g. 6).\n",
    "            hidden_dim (int): Hidden embedding size for GNN layers.\n",
    "            num_layers (int): Number of GNN layers.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # A learnable linear transform for edge features (required by GINEConv's \"nn\" argument):\n",
    "        # Typically GINEConv uses a small MLP to incorporate edge_attr into the message.\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # A learnable linear transform for node features:\n",
    "        self.node_encoder = nn.Linear(node_in_dim, hidden_dim)\n",
    "        \n",
    "        # Create multiple GINEConv layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            # GINEConv requires an MLP for node update:\n",
    "            # We'll use a simple 2-layer MLP\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dim))  # batch norm for stability\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # An MLP to process external factors\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression MLP after pooling + external embedding\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: PyG Data object, expected fields:\n",
    "                - x: Node features [num_nodes, node_in_dim]\n",
    "                - edge_index: [2, num_edges]\n",
    "                - edge_attr: [num_edges, edge_in_dim]\n",
    "                - batch: [num_nodes] mapping each node to a graph ID\n",
    "                - externals: [batch_size, external_in_dim]\n",
    "        Returns:\n",
    "            A tensor of shape [batch_size], the predicted regression value.\n",
    "        \"\"\"\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # 1) Encode node features and edge features\n",
    "        x = self.node_encoder(x)                 # [num_nodes, hidden_dim]\n",
    "        edge_emb = self.edge_encoder(edge_attr)  # [num_edges, hidden_dim]\n",
    "        \n",
    "        # 2) Pass through multiple GINEConv layers\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # 3) Global pooling to get graph embedding\n",
    "        graph_emb = global_mean_pool(x, batch)  # [batch_size, hidden_dim]\n",
    "\n",
    "        # 4) Process external factors\n",
    "        ext_emb = self.externals_mlp(data.externals)  # [batch_size, hidden_dim]\n",
    "\n",
    "        # 5) Combine + final regression\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)  # [batch_size, hidden_dim * 2]\n",
    "        out = self.final_regressor(combined).squeeze(-1)    # [batch_size]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_data)               # [batch_size]\n",
    "        y = batch_data.y.to(device).view(-1)    # [batch_size]\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "        count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            preds = model(batch_data)\n",
    "            y = batch_data.y.to(device).view(-1)\n",
    "            loss = criterion(preds, y)\n",
    "            total_loss += loss.item() * batch_data.num_graphs\n",
    "            count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset loader and compute R² and RMSE.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained GNN model.\n",
    "        loader (DataLoader): The PyG DataLoader for the evaluation dataset.\n",
    "        device (torch.device): The device to run on.\n",
    "    \n",
    "    Returns:\n",
    "        r2 (float): Coefficient of determination.\n",
    "        rmse (float): Root Mean Squared Error.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            preds = model(batch)\n",
    "            y_true.append(batch.y.cpu())\n",
    "            y_pred.append(preds.cpu())\n",
    "\n",
    "    # If your labels are stored as tensors with an extra dimension, use .squeeze() if needed.\n",
    "    y_true = torch.cat(y_true).numpy().squeeze()\n",
    "    y_pred = torch.cat(y_pred).numpy().squeeze()\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return r2, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_file = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\dataset02.xlsx\"\n",
    "\n",
    "data = pd.read_excel(env_file, engine='openpyxl').dropna(subset=['degradation_rate'])\n",
    "data['seawater'] = data['seawater'].map({'art': 1, 'sea': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1023 entries, 0 to 1039\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   data number       1023 non-null   float64\n",
      " 1   temperature       1023 non-null   float64\n",
      " 2   seawater          1023 non-null   int64  \n",
      " 3   concentration     1023 non-null   int64  \n",
      " 4   time              1023 non-null   int64  \n",
      " 5   component         1023 non-null   object \n",
      " 6   BDE               1023 non-null   float64\n",
      " 7   BDFE              1023 non-null   float64\n",
      " 8   energy            1023 non-null   float64\n",
      " 9   degradation_rate  1023 non-null   float64\n",
      "dtypes: float64(6), int64(3), object(1)\n",
      "memory usage: 87.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\molecules\"\n",
    "graph_pickles = [f for f in os.listdir(folder_path) if f.endswith(\".pkl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\molecules\n",
      "Files in directory: ['gpickle_graph_0.pkl', 'gpickle_graph_1.pkl', 'gpickle_graph_10.pkl', 'gpickle_graph_11.pkl', 'gpickle_graph_12.pkl', 'gpickle_graph_13.pkl', 'gpickle_graph_14.pkl', 'gpickle_graph_15.pkl', 'gpickle_graph_16.pkl', 'gpickle_graph_17.pkl', 'gpickle_graph_18.pkl', 'gpickle_graph_19.pkl', 'gpickle_graph_2.pkl', 'gpickle_graph_3.pkl', 'gpickle_graph_4.pkl', 'gpickle_graph_5.pkl', 'gpickle_graph_6.pkl', 'gpickle_graph_7.pkl', 'gpickle_graph_8.pkl', 'gpickle_graph_9.pkl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\molecules\"\n",
    "\n",
    "if os.path.exists(base_dir):\n",
    "    print(\"Directory exists:\", base_dir)\n",
    "    print(\"Files in directory:\", os.listdir(base_dir))\n",
    "else:\n",
    "    print(f\"Error: Directory {base_dir} does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = data.component.unique()\n",
    "graphs_dict = {}\n",
    "\n",
    "for compound, graph_pickle in zip(compounds, graph_pickles):\n",
    "    with open(os.path.join(base_dir, graph_pickle), 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "        graphs_dict[compound] = networkx_to_pyg(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = MolDataset(\n",
    "    raw_dataframe=data,\n",
    "    nx_graph_dict=graphs_dict,\n",
    "    component_col=\"component\",\n",
    "    global_state_cols=[\"temperature\", \"concentration\", \"time\", \"seawater\"],\n",
    "    label_col=\"degradation_rate\",\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# Simple random split for demonstration\n",
    "train_size = int(0.8 * len(dataset))  # 80% train\n",
    "val_size   = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = PyGDataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = PyGDataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 2) Instantiate model + optimizer\n",
    "# -----------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GINE_Regression(\n",
    "    node_in_dim=1,\n",
    "    edge_in_dim=2,\n",
    "    external_in_dim=4,\n",
    "    hidden_dim=16,\n",
    "    num_layers=5,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] train_loss: 0.1527, val_loss: 0.0777\n",
      "[Epoch 20] train_loss: 0.1020, val_loss: 0.0644\n",
      "[Epoch 30] train_loss: 0.0765, val_loss: 0.0652\n",
      "[Epoch 40] train_loss: 0.0765, val_loss: 0.0595\n",
      "[Epoch 50] train_loss: 0.0719, val_loss: 0.0607\n",
      "[Epoch 60] train_loss: 0.0602, val_loss: 0.0608\n",
      "[Epoch 70] train_loss: 0.0808, val_loss: 0.0572\n",
      "[Epoch 80] train_loss: 0.0644, val_loss: 0.0691\n",
      "[Epoch 90] train_loss: 0.0675, val_loss: 0.0560\n",
      "[Epoch 100] train_loss: 0.0748, val_loss: 0.0604\n",
      "[Epoch 110] train_loss: 0.0680, val_loss: 0.0581\n",
      "[Epoch 120] train_loss: 0.0687, val_loss: 0.0534\n",
      "[Epoch 130] train_loss: 0.0633, val_loss: 0.0565\n",
      "[Epoch 140] train_loss: 0.0605, val_loss: 0.0618\n",
      "[Epoch 150] train_loss: 0.0615, val_loss: 0.0507\n",
      "[Epoch 160] train_loss: 0.0590, val_loss: 0.0493\n",
      "[Epoch 170] train_loss: 0.0603, val_loss: 0.0511\n",
      "[Epoch 180] train_loss: 0.0677, val_loss: 0.0526\n",
      "[Epoch 190] train_loss: 0.0569, val_loss: 0.0484\n",
      "[Epoch 200] train_loss: 0.0506, val_loss: 0.0485\n",
      "[Epoch 210] train_loss: 0.0599, val_loss: 0.0488\n",
      "[Epoch 220] train_loss: 0.0542, val_loss: 0.0446\n",
      "[Epoch 230] train_loss: 0.0542, val_loss: 0.0476\n",
      "[Epoch 240] train_loss: 0.0521, val_loss: 0.0520\n",
      "[Epoch 250] train_loss: 0.0493, val_loss: 0.0445\n",
      "[Epoch 260] train_loss: 0.0549, val_loss: 0.0490\n",
      "[Epoch 270] train_loss: 0.0481, val_loss: 0.0430\n",
      "[Epoch 280] train_loss: 0.0480, val_loss: 0.0413\n",
      "[Epoch 290] train_loss: 0.0509, val_loss: 0.0402\n",
      "[Epoch 300] train_loss: 0.0465, val_loss: 0.0412\n",
      "[Epoch 310] train_loss: 0.0463, val_loss: 0.0436\n",
      "[Epoch 320] train_loss: 0.0449, val_loss: 0.0351\n",
      "[Epoch 330] train_loss: 0.0419, val_loss: 0.0369\n",
      "[Epoch 340] train_loss: 0.0370, val_loss: 0.0344\n",
      "[Epoch 350] train_loss: 0.0414, val_loss: 0.0350\n",
      "[Epoch 360] train_loss: 0.0372, val_loss: 0.0313\n",
      "[Epoch 370] train_loss: 0.0499, val_loss: 0.0416\n",
      "[Epoch 380] train_loss: 0.0421, val_loss: 0.0391\n",
      "[Epoch 390] train_loss: 0.0590, val_loss: 0.0395\n",
      "[Epoch 400] train_loss: 0.0506, val_loss: 0.0499\n",
      "[Epoch 410] train_loss: 0.0453, val_loss: 0.0354\n",
      "[Epoch 420] train_loss: 0.0357, val_loss: 0.0394\n",
      "[Epoch 430] train_loss: 0.0524, val_loss: 0.0448\n",
      "[Epoch 440] train_loss: 0.0393, val_loss: 0.0309\n",
      "[Epoch 450] train_loss: 0.0398, val_loss: 0.0307\n",
      "[Epoch 460] train_loss: 0.0356, val_loss: 0.0317\n",
      "[Epoch 470] train_loss: 0.0438, val_loss: 0.0438\n",
      "[Epoch 480] train_loss: 0.0407, val_loss: 0.0381\n",
      "[Epoch 490] train_loss: 0.0349, val_loss: 0.0549\n",
      "[Epoch 500] train_loss: 0.0346, val_loss: 0.0431\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 3) Training Loop\n",
    "# -----------------------------------\n",
    "num_epochs = 500\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[Epoch {epoch}] train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}\")\n",
    "\n",
    "# Optionally, test or save the model\n",
    "# torch.save(model.state_dict(), \"trained_gine_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.3789\n",
      "RMSE: 0.2076\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage after training:\n",
    "r2, rmse = evaluate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\molecules\n",
      "Files in directory: ['gpickle_graph_0.pkl', 'gpickle_graph_1.pkl', 'gpickle_graph_10.pkl', 'gpickle_graph_11.pkl', 'gpickle_graph_12.pkl', 'gpickle_graph_13.pkl', 'gpickle_graph_14.pkl', 'gpickle_graph_15.pkl', 'gpickle_graph_16.pkl', 'gpickle_graph_17.pkl', 'gpickle_graph_18.pkl', 'gpickle_graph_19.pkl', 'gpickle_graph_2.pkl', 'gpickle_graph_3.pkl', 'gpickle_graph_4.pkl', 'gpickle_graph_5.pkl', 'gpickle_graph_6.pkl', 'gpickle_graph_7.pkl', 'gpickle_graph_8.pkl', 'gpickle_graph_9.pkl']\n",
      "\n",
      "--- Fold 1 ---\n",
      "[Fold 1 Epoch 10] Train Loss: 0.1751 | Val Loss: 0.0972\n",
      "Evaluating fold 1 ...\n",
      "R²: -0.7291\n",
      "RMSE: 0.3117\n",
      "\n",
      "--- Fold 2 ---\n",
      "[Fold 2 Epoch 10] Train Loss: 0.1456 | Val Loss: 0.1514\n",
      "Evaluating fold 2 ...\n",
      "R²: -0.4495\n",
      "RMSE: 0.3891\n",
      "\n",
      "--- Fold 3 ---\n",
      "[Fold 3 Epoch 10] Train Loss: 0.1790 | Val Loss: 0.0765\n",
      "Evaluating fold 3 ...\n",
      "R²: -0.0965\n",
      "RMSE: 0.2767\n",
      "\n",
      "--- Fold 4 ---\n",
      "[Fold 4 Epoch 10] Train Loss: 0.1980 | Val Loss: 0.1193\n",
      "Evaluating fold 4 ...\n",
      "R²: -0.3287\n",
      "RMSE: 0.3454\n",
      "\n",
      "--- Fold 5 ---\n",
      "[Fold 5 Epoch 10] Train Loss: 0.1291 | Val Loss: 0.0682\n",
      "Evaluating fold 5 ...\n",
      "R²: 0.0158\n",
      "RMSE: 0.2612\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "Fold 1: R² = -0.7291, RMSE = 0.3117\n",
      "Fold 2: R² = -0.4495, RMSE = 0.3891\n",
      "Fold 3: R² = -0.0965, RMSE = 0.2767\n",
      "Fold 4: R² = -0.3287, RMSE = 0.3454\n",
      "Fold 5: R² = 0.0158, RMSE = 0.2612\n",
      "\n",
      "Average R²: -0.3176 ± 0.2634\n",
      "Average RMSE: 0.3168 ± 0.0464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:06:22,878] Using an existing study with name 'GNN-mixed model' instead of creating a new one.\n",
      "Best trial: 1. Best value: 8.25468:   3%|▎         | 1/30 [01:35<46:05, 95.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:07:58,246] Trial 1 finished with value: 8.254682528706155 and parameters: {'learning_rate': 0.0003, 'dropout': 0.5, 'num_layers': 6}. Best is trial 1 with value: 8.254682528706155.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 6.04862:   7%|▋         | 2/30 [02:53<39:38, 84.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:09:15,895] Trial 2 finished with value: 6.0486228104918265 and parameters: {'learning_rate': 0.0003, 'dropout': 0.5, 'num_layers': 3}. Best is trial 2 with value: 6.0486228104918265.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 3.77184:  10%|█         | 3/30 [04:26<40:02, 89.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:10:49,710] Trial 3 finished with value: 3.7718372421276127 and parameters: {'learning_rate': 0.001, 'dropout': 0.5, 'num_layers': 5}. Best is trial 3 with value: 3.7718372421276127.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -0.00334065:  13%|█▎        | 4/30 [06:04<40:06, 92.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:12:27,707] Trial 4 finished with value: -0.0033406528048234075 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 5}. Best is trial 4 with value: -0.0033406528048234075.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -0.00334065:  17%|█▋        | 5/30 [07:14<35:11, 84.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:13:37,790] Trial 5 finished with value: 0.6038138096718783 and parameters: {'learning_rate': 0.0003, 'dropout': 0.1, 'num_layers': 2}. Best is trial 4 with value: -0.0033406528048234075.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -0.00334065:  20%|██        | 6/30 [08:39<33:47, 84.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:15:02,297] Trial 6 finished with value: 0.30380781836931475 and parameters: {'learning_rate': 0.003, 'dropout': 0.5, 'num_layers': 5}. Best is trial 4 with value: -0.0033406528048234075.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -0.00334065:  23%|██▎       | 7/30 [10:10<33:09, 86.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:16:32,982] Trial 7 finished with value: 0.9671505289985702 and parameters: {'learning_rate': 0.0001, 'dropout': 0.1, 'num_layers': 4}. Best is trial 4 with value: -0.0033406528048234075.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -0.00334065:  27%|██▋       | 8/30 [11:23<30:13, 82.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:17:46,678] Trial 8 finished with value: 0.7906228473522988 and parameters: {'learning_rate': 0.003, 'dropout': 0.5, 'num_layers': 2}. Best is trial 4 with value: -0.0033406528048234075.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -0.00334065:  30%|███       | 9/30 [12:46<28:53, 82.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:19:09,540] Trial 9 finished with value: 0.05595016899844936 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 3}. Best is trial 4 with value: -0.0033406528048234075.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -0.00334065:  33%|███▎      | 10/30 [14:03<26:58, 80.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:20:26,833] Trial 10 finished with value: 0.10217657931349247 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 2}. Best is trial 4 with value: -0.0033406528048234075.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: -0.00334065:  37%|███▋      | 11/30 [15:42<27:17, 86.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:22:04,969] Trial 11 finished with value: 1.1269340422957144 and parameters: {'learning_rate': 0.0001, 'dropout': 0.1, 'num_layers': 6}. Best is trial 4 with value: -0.0033406528048234075.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: -0.00945485:  40%|████      | 12/30 [17:06<25:39, 85.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:23:28,958] Trial 12 finished with value: -0.009454851060532076 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 4}. Best is trial 12 with value: -0.009454851060532076.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: -0.00945485:  43%|████▎     | 13/30 [18:31<24:10, 85.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:24:53,881] Trial 13 finished with value: 0.00878230627360559 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 4}. Best is trial 12 with value: -0.009454851060532076.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: -0.00945485:  47%|████▋     | 14/30 [20:03<23:21, 87.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:26:26,764] Trial 14 finished with value: 0.19008760818935352 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 5}. Best is trial 12 with value: -0.009454851060532076.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: -0.00945485:  50%|█████     | 15/30 [21:31<21:52, 87.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:27:54,043] Trial 15 finished with value: 0.1875651667138046 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 4}. Best is trial 12 with value: -0.009454851060532076.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: -0.00945485:  53%|█████▎    | 16/30 [23:00<20:33, 88.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:29:23,443] Trial 16 finished with value: 0.057244678100036195 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 5}. Best is trial 12 with value: -0.009454851060532076.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: -0.00945485:  57%|█████▋    | 17/30 [24:19<18:29, 85.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:30:42,451] Trial 17 finished with value: -0.00777799274279709 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 3}. Best is trial 12 with value: -0.009454851060532076.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: -0.00945485:  60%|██████    | 18/30 [25:34<16:28, 82.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:31:57,783] Trial 18 finished with value: 0.15792282827266102 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 3}. Best is trial 12 with value: -0.009454851060532076.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: -0.00945485:  63%|██████▎   | 19/30 [25:50<11:25, 62.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:32:13,567] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: -0.00945485:  67%|██████▋   | 20/30 [27:12<11:22, 68.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:33:35,634] Trial 20 finished with value: 0.16893437152438945 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 4}. Best is trial 12 with value: -0.009454851060532076.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: -0.0104739:  70%|███████   | 21/30 [28:24<10:22, 69.22s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:34:47,063] Trial 21 finished with value: -0.010473899258953302 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 3}. Best is trial 21 with value: -0.010473899258953302.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: -0.0104739:  73%|███████▎  | 22/30 [29:40<09:30, 71.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:36:03,372] Trial 22 finished with value: -0.006417567281854675 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 3}. Best is trial 21 with value: -0.010473899258953302.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: -0.0104739:  77%|███████▋  | 23/30 [29:57<06:24, 55.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:36:20,225] Trial 23 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: -0.0104739:  80%|████████  | 24/30 [31:31<06:39, 66.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:37:53,916] Trial 24 finished with value: -0.01025150647571158 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 3}. Best is trial 21 with value: -0.010473899258953302.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: -0.0104739:  83%|████████▎ | 25/30 [32:42<05:40, 68.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:39:05,476] Trial 25 finished with value: 0.0023797463156685515 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 2}. Best is trial 21 with value: -0.010473899258953302.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: -0.0104739:  87%|████████▋ | 26/30 [34:15<05:02, 75.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:40:38,380] Trial 26 finished with value: 0.07758547551878041 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 4}. Best is trial 21 with value: -0.010473899258953302.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: -0.0104739:  90%|█████████ | 27/30 [34:33<02:54, 58.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:40:56,312] Trial 27 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: -0.0104739:  93%|█████████▎| 28/30 [35:59<02:12, 66.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:42:22,053] Trial 28 finished with value: 0.02271963241202597 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 3}. Best is trial 21 with value: -0.010473899258953302.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: -0.0104739:  97%|█████████▋| 29/30 [36:16<00:51, 51.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:42:39,469] Trial 29 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: -0.0104739: 100%|██████████| 30/30 [36:35<00:00, 73.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:42:58,365] Trial 30 pruned. \n",
      "\n",
      "================= Optuna Study Results =================\n",
      "Best Trial Value (Negative R²): -0.010473899258953302\n",
      "Best Hyperparameters:\n",
      "  learning_rate: 0.003\n",
      "  dropout: 0.1\n",
      "  num_layers: 3\n",
      "User Attrs (e.g., RMSE): {'avg_rmse': 0.2757047116756439}\n",
      "Could not generate optimization history plot: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\n",
      "Could not generate hyperparameter importance plot: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\n",
      "Could not generate intermediate values plot: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\n",
      "\n",
      "================= End of Optuna Tuning =================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data, DataLoader as PyGDataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool, BatchNorm\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "#                             DATASET & PREP\n",
    "# =============================================================================\n",
    "\n",
    "class MolDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset that:\n",
    "      - Reads external factors from CSV\n",
    "      - Loads the corresponding pickle for the molecule's graph\n",
    "      - Converts it into a PyG Data object\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 raw_dataframe: pd.DataFrame,\n",
    "                 nx_graph_dict: dict,\n",
    "                 *,\n",
    "                 component_col: str,\n",
    "                 global_state_cols: list[str],\n",
    "                 label_col: str,\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_dataframe: The input dataframe containing molecule info.\n",
    "            nx_graph_dict: Dictionary mapping component names to networkx graphs.\n",
    "            component_col: Column name for the component.\n",
    "            global_state_cols: List of columns representing external factors.\n",
    "            label_col: Column name for the regression target.\n",
    "            transform: Any transform to apply to each PyG Data object.\n",
    "        \"\"\"\n",
    "        self.raw_dataframe = raw_dataframe\n",
    "        self.nx_graph_dict = nx_graph_dict\n",
    "        self.component_col = [component_col] if type(component_col) is str else component_col\n",
    "        self.global_state_cols = global_state_cols\n",
    "        self.label_col = [label_col] if type(label_col) is str else label_col\n",
    "        self.transform = transform\n",
    "        \n",
    "        required_cols = set(self.global_state_cols + self.label_col + self.component_col)\n",
    "        for col in required_cols:\n",
    "            if col not in self.raw_dataframe.columns:\n",
    "                raise ValueError(f\"Missing column in DataFrame: '{col}'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.raw_dataframe.iloc[idx]\n",
    "        \n",
    "        # 1. Load the molecule graph\n",
    "        component_name = row[self.component_col[0]]  # e.g. \"C23\"\n",
    "        pyg_data = self.nx_graph_dict[component_name]\n",
    "\n",
    "        # 2. Prepare the external factors\n",
    "        externals = torch.tensor(row[self.global_state_cols].values.astype(float), dtype=torch.float)\n",
    "        externals = externals.unsqueeze(0)\n",
    "\n",
    "        # 3. Prepare the label (regression target)\n",
    "        label = torch.tensor([row[self.label_col][0]], dtype=torch.float)\n",
    "\n",
    "        # 4. Attach externals & label to the Data object\n",
    "        pyg_data.externals = externals  # shape [1, external_in_dim]\n",
    "        pyg_data.y = label  # shape [1]\n",
    "\n",
    "        if self.transform:\n",
    "            pyg_data = self.transform(pyg_data)\n",
    "\n",
    "        return pyg_data\n",
    "\n",
    "\n",
    "def networkx_to_pyg(nx_graph):\n",
    "    \"\"\"\n",
    "    Convert a networkx graph to a torch_geometric.data.Data object.\n",
    "    This is a basic template; adjust for your actual node/edge features.\n",
    "    \"\"\"\n",
    "    # Sort nodes to ensure consistent ordering\n",
    "    node_mapping = {node: i for i, node in enumerate(nx_graph.nodes())}\n",
    "\n",
    "    x_list = []\n",
    "    edge_index_list = []\n",
    "    edge_attr_list = []\n",
    "\n",
    "    # Node features\n",
    "    for node in nx_graph.nodes(data=True):\n",
    "        original_id = node[0]\n",
    "        attrs = node[1]\n",
    "        symbol = attrs.get(\"symbol\", \"C\")\n",
    "        symbol_id = 0 if symbol == \"C\" else 1 if symbol == \"H\" else 2\n",
    "        x_list.append([symbol_id])\n",
    "\n",
    "    # Edge features\n",
    "    for u, v, edge_attrs in nx_graph.edges(data=True):\n",
    "        u_idx = node_mapping[u]\n",
    "        v_idx = node_mapping[v]\n",
    "        edge_index_list.append((u_idx, v_idx))\n",
    "        bde_pred = edge_attrs.get(\"bde_pred\", 0.0) or 0.0\n",
    "        bdfe_pred = edge_attrs.get(\"bdfe_pred\", 0.0) or 0.0\n",
    "        edge_attr_list.append([bde_pred, bdfe_pred])\n",
    "\n",
    "    x = torch.tensor(x_list, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                     BASE GNN MODEL (WITH DIM MATCH)\n",
    "# =============================================================================\n",
    "\n",
    "class GINE_Regression(nn.Module):\n",
    "    \"\"\"\n",
    "    A GNN for regression using GINEConv layers + edge attributes,\n",
    "    where all layers have the same hidden_dim (no dimension mismatch).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dim: int = 128,\n",
    "                 num_layers: int = 3,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encode edges from edge_in_dim to hidden_dim\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Encode nodes from node_in_dim to hidden_dim\n",
    "        self.node_encoder = nn.Linear(node_in_dim, hidden_dim)\n",
    "        \n",
    "        # Multiple GINEConv layers & corresponding BatchNorm\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Process external factors\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Encode\n",
    "        x = self.node_encoder(x)\n",
    "        edge_emb = self.edge_encoder(edge_attr)\n",
    "        \n",
    "        # Pass through GINEConv layers\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Global pooling\n",
    "        graph_emb = global_mean_pool(x, batch)\n",
    "\n",
    "        # Process external factors\n",
    "        ext_emb = self.externals_mlp(data.externals)\n",
    "\n",
    "        # Combine & regress\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)\n",
    "        out = self.final_regressor(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                   TRAIN/VALID/EVALUATION UTILS\n",
    "# =============================================================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_data)\n",
    "        y = batch_data.y.to(device).view(-1)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "        count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            preds = model(batch_data)\n",
    "            y = batch_data.y.to(device).view(-1)\n",
    "            loss = criterion(preds, y)\n",
    "            total_loss += loss.item() * batch_data.num_graphs\n",
    "            count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset loader and compute R² and RMSE.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            preds = model(batch)\n",
    "            y_true.append(batch.y.cpu())\n",
    "            y_pred.append(preds.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true).numpy().squeeze()\n",
    "    y_pred = torch.cat(y_pred).numpy().squeeze()\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return r2, rmse\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                             DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "env_file = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\dataset02.xlsx\"\n",
    "data = pd.read_excel(env_file, engine='openpyxl').dropna(subset=['degradation_rate'])\n",
    "data['seawater'] = data['seawater'].map({'art': 1, 'sea': 0})\n",
    "\n",
    "folder_path = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\molecules\"\n",
    "graph_pickles = [f for f in os.listdir(folder_path) if f.endswith(\".pkl\")]\n",
    "\n",
    "base_dir = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\molecules\"\n",
    "if os.path.exists(base_dir):\n",
    "    print(\"Directory exists:\", base_dir)\n",
    "    print(\"Files in directory:\", os.listdir(base_dir))\n",
    "else:\n",
    "    print(f\"Error: Directory {base_dir} does not exist!\")\n",
    "\n",
    "compounds = data.component.unique()\n",
    "graphs_dict = {}\n",
    "for compound, graph_pickle in zip(compounds, graph_pickles):\n",
    "    with open(os.path.join(base_dir, graph_pickle), 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "        graphs_dict[compound] = networkx_to_pyg(graph)\n",
    "\n",
    "dataset = MolDataset(\n",
    "    raw_dataframe=data,\n",
    "    nx_graph_dict=graphs_dict,\n",
    "    component_col=\"component\",\n",
    "    global_state_cols=[\"temperature\", \"concentration\", \"time\", \"seawater\"],\n",
    "    label_col=\"degradation_rate\",\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "#                      CROSS-VALIDATION (FIXED MODEL)\n",
    "# =============================================================================\n",
    "from torch_geometric.data import DataLoader as PyGDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 5  # number of folds\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"\\n--- Fold {fold + 1} ---\")\n",
    "\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "    val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = GINE_Regression(\n",
    "        node_in_dim=1,\n",
    "        edge_in_dim=2,\n",
    "        external_in_dim=4,\n",
    "        hidden_dim=16,  # Example hidden_dim\n",
    "        num_layers=5,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    num_epochs = 10\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[Fold {fold + 1} Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    print(f\"Evaluating fold {fold + 1} ...\")\n",
    "    r2, rmse = evaluate_model(model, val_loader, device)\n",
    "    fold_results.append({\"fold\": fold + 1, \"r2\": r2, \"rmse\": rmse})\n",
    "\n",
    "r2_scores = [res[\"r2\"] for res in fold_results]\n",
    "rmse_scores = [res[\"rmse\"] for res in fold_results]\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "for res in fold_results:\n",
    "    print(f\"Fold {res['fold']}: R² = {res['r2']:.4f}, RMSE = {res['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#             IMPROVED MODEL WITH TRAPEZOID DIMENSIONS & PROJECTIONS\n",
    "# =============================================================================\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "\n",
    "def build_trapezoid_dims(num_layers):\n",
    "    \"\"\"\n",
    "    Build a list of hidden dimensions in a trapezoid manner:\n",
    "    up to the first 4 layers: [128, 64, 32, 16]\n",
    "    if num_layers > 4, extend with 16 for extra layers.\n",
    "    \"\"\"\n",
    "    base_dims = [128, 64, 32, 16]\n",
    "    if num_layers > 4:\n",
    "        extra_layers = num_layers - 4\n",
    "        return base_dims + [16] * extra_layers\n",
    "    else:\n",
    "        return base_dims[:num_layers]\n",
    "\n",
    "\n",
    "class GINE_RegressionTrapezoid(nn.Module):\n",
    "    \"\"\"\n",
    "    A GINEConv-based regression model that uses a list of hidden dimensions\n",
    "    to build layers with decreasing size (trapezoid architecture),\n",
    "    ensuring dimension consistency with projection layers between convs.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dims: list,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # For the first layer, encode edges to hidden_dims[0], and encode nodes as well\n",
    "        self.initial_edge_encoder = nn.Linear(edge_in_dim, hidden_dims[0])\n",
    "        self.initial_node_encoder = nn.Linear(node_in_dim, hidden_dims[0])\n",
    "\n",
    "        # We'll build each GINEConv to transform dimension: hidden_dims[i] -> hidden_dims[i].\n",
    "        # After each conv i, if i < len(hidden_dims)-1, we project to hidden_dims[i+1].\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.projections = nn.ModuleList()  # for node features\n",
    "        self.edge_projections = nn.ModuleList()  # for edge features\n",
    "\n",
    "        for i in range(len(hidden_dims)):\n",
    "            # GINEConv's internal MLP\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i])\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dims[i]))\n",
    "\n",
    "            # If there's a next layer, we need a projection from hidden_dims[i] -> hidden_dims[i+1]\n",
    "            if i < len(hidden_dims) - 1:\n",
    "                self.projections.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "                self.edge_projections.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            else:\n",
    "                # No projection needed for the last layer\n",
    "                self.projections.append(None)\n",
    "                self.edge_projections.append(None)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "\n",
    "        # We'll process external factors to the final dimension\n",
    "        final_dim = hidden_dims[-1]\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, final_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(final_dim, final_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression: combine final node features + final external features\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(final_dim + final_dim, final_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(final_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # 1) Encode node/edge to hidden_dims[0]\n",
    "        x = self.initial_node_encoder(x)\n",
    "        edge_emb = self.initial_edge_encoder(edge_attr)\n",
    "\n",
    "        # 2) Pass through each GINEConv\n",
    "        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):\n",
    "            # GINEConv forward\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout_layer(x)\n",
    "\n",
    "            # If there's a next layer, project node features and edge_emb to hidden_dims[i+1]\n",
    "            if i < len(self.projections) - 1 and self.projections[i] is not None:\n",
    "                x = self.projections[i](x)\n",
    "                edge_emb = self.edge_projections[i](edge_emb)\n",
    "\n",
    "        # 3) Global mean pooling\n",
    "        graph_emb = global_mean_pool(x, batch)\n",
    "\n",
    "        # 4) Process external factors into final dimension\n",
    "        ext_emb = self.externals_mlp(data.externals)\n",
    "\n",
    "        # 5) Concatenate and final regression\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)\n",
    "        out = self.final_regressor(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                          OPTUNA OBJECTIVE FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna.\n",
    "    Performs k-fold cross validation using GINE_RegressionTrapezoid,\n",
    "    returning negative average R². We log RMSE as a user attribute.\n",
    "    \"\"\"\n",
    "    # Hyperparameter search space\n",
    "    lr = trial.suggest_categorical(\"learning_rate\", [1e-3, 3e-3, 1e-4, 3e-4])\n",
    "    dropout = trial.suggest_categorical(\"dropout\", [0.1, 0.5])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "\n",
    "    hidden_dims = build_trapezoid_dims(num_layers)\n",
    "    num_epochs = 10  # Could also be a hyperparameter\n",
    "\n",
    "    # Local 5-fold for CV\n",
    "    kf_local = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    r2_scores = []\n",
    "    rmse_scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf_local.split(dataset)):\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "        val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "        model = GINE_RegressionTrapezoid(\n",
    "            node_in_dim=1,\n",
    "            edge_in_dim=2,\n",
    "            external_in_dim=4,\n",
    "            hidden_dims=hidden_dims,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "            val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "            # Report intermediate values (optional)\n",
    "            if epoch % 10 == 0:\n",
    "                trial.report(val_loss, step=epoch)\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "\n",
    "        # Evaluate R² and RMSE for this fold\n",
    "        model.eval()\n",
    "        y_true_fold, y_pred_fold = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch_data in val_loader:\n",
    "                batch_data = batch_data.to(device)\n",
    "                preds = model(batch_data)\n",
    "                y_true_fold.append(batch_data.y.cpu())\n",
    "                y_pred_fold.append(preds.cpu())\n",
    "\n",
    "        y_true_fold = torch.cat(y_true_fold).numpy().squeeze()\n",
    "        y_pred_fold = torch.cat(y_pred_fold).numpy().squeeze()\n",
    "\n",
    "        fold_r2 = r2_score(y_true_fold, y_pred_fold)\n",
    "        fold_rmse = np.sqrt(mean_squared_error(y_true_fold, y_pred_fold))\n",
    "\n",
    "        r2_scores.append(fold_r2)\n",
    "        rmse_scores.append(fold_rmse)\n",
    "\n",
    "    avg_r2 = np.mean(r2_scores)\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "    # Log RMSE as user attribute\n",
    "    trial.set_user_attr(\"avg_rmse\", float(avg_rmse))\n",
    "\n",
    "    # Return negative R² because Optuna minimizes the objective\n",
    "    return -avg_r2\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                           OPTUNA STUDY & DASHBOARD\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(\n",
    "        storage=\"sqlite:///gnn_mix_op05.sqlite3\",\n",
    "        study_name=\"GNN-mixed model\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "    print(\"\\n================= Optuna Study Results =================\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Best Trial Value (Negative R²): {best_trial.value}\")\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for key, val in best_trial.params.items():\n",
    "        print(f\"  {key}: {val}\")\n",
    "    print(f\"User Attrs (e.g., RMSE): {best_trial.user_attrs}\")\n",
    "\n",
    "    try:\n",
    "        fig1 = vis.plot_optimization_history(study)\n",
    "        fig1.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate optimization history plot: {e}\")\n",
    "\n",
    "    try:\n",
    "        fig2 = vis.plot_param_importances(study)\n",
    "        fig2.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate hyperparameter importance plot: {e}\")\n",
    "\n",
    "    try:\n",
    "        fig3 = vis.plot_intermediate_values(study)\n",
    "        fig3.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate intermediate values plot: {e}\")\n",
    "\n",
    "    print(\"\\n================= End of Optuna Tuning =================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1 ---\n",
      "[Fold 1 Epoch 10] Train Loss: 0.1210 | Val Loss: 0.0644\n",
      "[Fold 1 Epoch 20] Train Loss: 0.0977 | Val Loss: 0.0594\n",
      "[Fold 1 Epoch 30] Train Loss: 0.0972 | Val Loss: 0.0560\n",
      "[Fold 1 Epoch 40] Train Loss: 0.0762 | Val Loss: 0.0503\n",
      "[Fold 1 Epoch 50] Train Loss: 0.0821 | Val Loss: 0.0633\n",
      "[Fold 1 Epoch 60] Train Loss: 0.0713 | Val Loss: 0.0479\n",
      "[Fold 1 Epoch 70] Train Loss: 0.0706 | Val Loss: 0.0467\n",
      "[Fold 1 Epoch 80] Train Loss: 0.0635 | Val Loss: 0.0454\n",
      "[Fold 1 Epoch 90] Train Loss: 0.0611 | Val Loss: 0.0453\n",
      "[Fold 1 Epoch 100] Train Loss: 0.0603 | Val Loss: 0.0442\n",
      "[Fold 1 Epoch 110] Train Loss: 0.0595 | Val Loss: 0.0426\n",
      "[Fold 1 Epoch 120] Train Loss: 0.0574 | Val Loss: 0.0488\n",
      "[Fold 1 Epoch 130] Train Loss: 0.0613 | Val Loss: 0.0419\n",
      "[Fold 1 Epoch 140] Train Loss: 0.0658 | Val Loss: 0.0567\n",
      "[Fold 1 Epoch 150] Train Loss: 0.0585 | Val Loss: 0.0405\n",
      "[Fold 1 Epoch 160] Train Loss: 0.0558 | Val Loss: 0.0384\n",
      "[Fold 1 Epoch 170] Train Loss: 0.0468 | Val Loss: 0.0374\n",
      "[Fold 1 Epoch 180] Train Loss: 0.0512 | Val Loss: 0.0356\n",
      "[Fold 1 Epoch 190] Train Loss: 0.0474 | Val Loss: 0.0354\n",
      "[Fold 1 Epoch 200] Train Loss: 0.0444 | Val Loss: 0.0359\n",
      "[Fold 1 Epoch 210] Train Loss: 0.0394 | Val Loss: 0.0336\n",
      "[Fold 1 Epoch 220] Train Loss: 0.0434 | Val Loss: 0.0326\n",
      "[Fold 1 Epoch 230] Train Loss: 0.0401 | Val Loss: 0.0314\n",
      "[Fold 1 Epoch 240] Train Loss: 0.0383 | Val Loss: 0.0488\n",
      "[Fold 1 Epoch 250] Train Loss: 0.0397 | Val Loss: 0.0350\n",
      "[Fold 1 Epoch 260] Train Loss: 0.0430 | Val Loss: 0.0327\n",
      "[Fold 1 Epoch 270] Train Loss: 0.0459 | Val Loss: 0.0335\n",
      "[Fold 1 Epoch 280] Train Loss: 0.0491 | Val Loss: 0.0332\n",
      "[Fold 1 Epoch 290] Train Loss: 0.0391 | Val Loss: 0.0318\n",
      "[Fold 1 Epoch 300] Train Loss: 0.0494 | Val Loss: 0.0369\n",
      "[Fold 1 Epoch 310] Train Loss: 0.0403 | Val Loss: 0.0331\n",
      "[Fold 1 Epoch 320] Train Loss: 0.0364 | Val Loss: 0.0322\n",
      "[Fold 1 Epoch 330] Train Loss: 0.0393 | Val Loss: 0.0375\n",
      "[Fold 1 Epoch 340] Train Loss: 0.0407 | Val Loss: 0.0330\n",
      "[Fold 1 Epoch 350] Train Loss: 0.0349 | Val Loss: 0.0328\n",
      "[Fold 1 Epoch 360] Train Loss: 0.0377 | Val Loss: 0.0331\n",
      "[Fold 1 Epoch 370] Train Loss: 0.0391 | Val Loss: 0.0359\n",
      "[Fold 1 Epoch 380] Train Loss: 0.0383 | Val Loss: 0.0335\n",
      "[Fold 1 Epoch 390] Train Loss: 0.0405 | Val Loss: 0.0356\n",
      "[Fold 1 Epoch 400] Train Loss: 0.0415 | Val Loss: 0.0372\n",
      "[Fold 1 Epoch 410] Train Loss: 0.0354 | Val Loss: 0.0356\n",
      "[Fold 1 Epoch 420] Train Loss: 0.0399 | Val Loss: 0.0331\n",
      "[Fold 1 Epoch 430] Train Loss: 0.0331 | Val Loss: 0.0365\n",
      "[Fold 1 Epoch 440] Train Loss: 0.0362 | Val Loss: 0.0337\n",
      "[Fold 1 Epoch 450] Train Loss: 0.0394 | Val Loss: 0.0401\n",
      "[Fold 1 Epoch 460] Train Loss: 0.0390 | Val Loss: 0.0325\n",
      "[Fold 1 Epoch 470] Train Loss: 0.0358 | Val Loss: 0.0340\n",
      "[Fold 1 Epoch 480] Train Loss: 0.0417 | Val Loss: 0.0393\n",
      "[Fold 1 Epoch 490] Train Loss: 0.0353 | Val Loss: 0.0337\n",
      "[Fold 1 Epoch 500] Train Loss: 0.0379 | Val Loss: 0.0332\n",
      "Evaluating fold 1 ...\n",
      "R²: 0.4096\n",
      "RMSE: 0.1821\n",
      "\n",
      "--- Fold 2 ---\n",
      "[Fold 2 Epoch 10] Train Loss: 0.1002 | Val Loss: 0.1067\n",
      "[Fold 2 Epoch 20] Train Loss: 0.0770 | Val Loss: 0.1244\n",
      "[Fold 2 Epoch 30] Train Loss: 0.0866 | Val Loss: 0.1121\n",
      "[Fold 2 Epoch 40] Train Loss: 0.0736 | Val Loss: 0.1034\n",
      "[Fold 2 Epoch 50] Train Loss: 0.0703 | Val Loss: 0.1125\n",
      "[Fold 2 Epoch 60] Train Loss: 0.0685 | Val Loss: 0.1055\n",
      "[Fold 2 Epoch 70] Train Loss: 0.0655 | Val Loss: 0.0950\n",
      "[Fold 2 Epoch 80] Train Loss: 0.0740 | Val Loss: 0.1013\n",
      "[Fold 2 Epoch 90] Train Loss: 0.0673 | Val Loss: 0.1009\n",
      "[Fold 2 Epoch 100] Train Loss: 0.0666 | Val Loss: 0.0974\n",
      "[Fold 2 Epoch 110] Train Loss: 0.0635 | Val Loss: 0.0966\n",
      "[Fold 2 Epoch 120] Train Loss: 0.0598 | Val Loss: 0.0973\n",
      "[Fold 2 Epoch 130] Train Loss: 0.0609 | Val Loss: 0.1010\n",
      "[Fold 2 Epoch 140] Train Loss: 0.0643 | Val Loss: 0.1122\n",
      "[Fold 2 Epoch 150] Train Loss: 0.0560 | Val Loss: 0.0978\n",
      "[Fold 2 Epoch 160] Train Loss: 0.0555 | Val Loss: 0.0931\n",
      "[Fold 2 Epoch 170] Train Loss: 0.0563 | Val Loss: 0.0863\n",
      "[Fold 2 Epoch 180] Train Loss: 0.0527 | Val Loss: 0.0878\n",
      "[Fold 2 Epoch 190] Train Loss: 0.0539 | Val Loss: 0.0913\n",
      "[Fold 2 Epoch 200] Train Loss: 0.0494 | Val Loss: 0.0848\n",
      "[Fold 2 Epoch 210] Train Loss: 0.0509 | Val Loss: 0.0840\n",
      "[Fold 2 Epoch 220] Train Loss: 0.0516 | Val Loss: 0.0746\n",
      "[Fold 2 Epoch 230] Train Loss: 0.0485 | Val Loss: 0.0723\n",
      "[Fold 2 Epoch 240] Train Loss: 0.0476 | Val Loss: 0.0778\n",
      "[Fold 2 Epoch 250] Train Loss: 0.0505 | Val Loss: 0.0706\n",
      "[Fold 2 Epoch 260] Train Loss: 0.0480 | Val Loss: 0.0796\n",
      "[Fold 2 Epoch 270] Train Loss: 0.0482 | Val Loss: 0.0779\n",
      "[Fold 2 Epoch 280] Train Loss: 0.0518 | Val Loss: 0.0822\n",
      "[Fold 2 Epoch 290] Train Loss: 0.0518 | Val Loss: 0.0647\n",
      "[Fold 2 Epoch 300] Train Loss: 0.0460 | Val Loss: 0.0610\n",
      "[Fold 2 Epoch 310] Train Loss: 0.0472 | Val Loss: 0.0626\n",
      "[Fold 2 Epoch 320] Train Loss: 0.0425 | Val Loss: 0.0634\n",
      "[Fold 2 Epoch 330] Train Loss: 0.0423 | Val Loss: 0.0620\n",
      "[Fold 2 Epoch 340] Train Loss: 0.0548 | Val Loss: 0.0768\n",
      "[Fold 2 Epoch 350] Train Loss: 0.0472 | Val Loss: 0.0605\n",
      "[Fold 2 Epoch 360] Train Loss: 0.0477 | Val Loss: 0.0938\n",
      "[Fold 2 Epoch 370] Train Loss: 0.0428 | Val Loss: 0.0465\n",
      "[Fold 2 Epoch 380] Train Loss: 0.0377 | Val Loss: 0.0587\n",
      "[Fold 2 Epoch 390] Train Loss: 0.0392 | Val Loss: 0.0487\n",
      "[Fold 2 Epoch 400] Train Loss: 0.0434 | Val Loss: 0.0542\n",
      "[Fold 2 Epoch 410] Train Loss: 0.0399 | Val Loss: 0.0574\n",
      "[Fold 2 Epoch 420] Train Loss: 0.0386 | Val Loss: 0.0506\n",
      "[Fold 2 Epoch 430] Train Loss: 0.0414 | Val Loss: 0.0580\n",
      "[Fold 2 Epoch 440] Train Loss: 0.0431 | Val Loss: 0.0675\n",
      "[Fold 2 Epoch 450] Train Loss: 0.0399 | Val Loss: 0.0634\n",
      "[Fold 2 Epoch 460] Train Loss: 0.0405 | Val Loss: 0.0719\n",
      "[Fold 2 Epoch 470] Train Loss: 0.0411 | Val Loss: 0.0584\n",
      "[Fold 2 Epoch 480] Train Loss: 0.0405 | Val Loss: 0.0614\n",
      "[Fold 2 Epoch 490] Train Loss: 0.0419 | Val Loss: 0.0615\n",
      "[Fold 2 Epoch 500] Train Loss: 0.0390 | Val Loss: 0.0603\n",
      "Evaluating fold 2 ...\n",
      "R²: 0.4225\n",
      "RMSE: 0.2456\n",
      "\n",
      "--- Fold 3 ---\n",
      "[Fold 3 Epoch 10] Train Loss: 0.1249 | Val Loss: 0.0701\n",
      "[Fold 3 Epoch 20] Train Loss: 0.0956 | Val Loss: 0.0708\n",
      "[Fold 3 Epoch 30] Train Loss: 0.0850 | Val Loss: 0.0544\n",
      "[Fold 3 Epoch 40] Train Loss: 0.0645 | Val Loss: 0.0568\n",
      "[Fold 3 Epoch 50] Train Loss: 0.0613 | Val Loss: 0.0536\n",
      "[Fold 3 Epoch 60] Train Loss: 0.0654 | Val Loss: 0.0507\n",
      "[Fold 3 Epoch 70] Train Loss: 0.0661 | Val Loss: 0.0506\n",
      "[Fold 3 Epoch 80] Train Loss: 0.0620 | Val Loss: 0.0551\n",
      "[Fold 3 Epoch 90] Train Loss: 0.0582 | Val Loss: 0.0506\n",
      "[Fold 3 Epoch 100] Train Loss: 0.0618 | Val Loss: 0.0524\n",
      "[Fold 3 Epoch 110] Train Loss: 0.0715 | Val Loss: 0.0512\n",
      "[Fold 3 Epoch 120] Train Loss: 0.0580 | Val Loss: 0.0548\n",
      "[Fold 3 Epoch 130] Train Loss: 0.0629 | Val Loss: 0.0501\n",
      "[Fold 3 Epoch 140] Train Loss: 0.0539 | Val Loss: 0.0523\n",
      "[Fold 3 Epoch 150] Train Loss: 0.0517 | Val Loss: 0.0514\n",
      "[Fold 3 Epoch 160] Train Loss: 0.0552 | Val Loss: 0.0495\n",
      "[Fold 3 Epoch 170] Train Loss: 0.0552 | Val Loss: 0.0563\n",
      "[Fold 3 Epoch 180] Train Loss: 0.0528 | Val Loss: 0.0494\n",
      "[Fold 3 Epoch 190] Train Loss: 0.0599 | Val Loss: 0.0460\n",
      "[Fold 3 Epoch 200] Train Loss: 0.0485 | Val Loss: 0.0488\n",
      "[Fold 3 Epoch 210] Train Loss: 0.0451 | Val Loss: 0.0523\n",
      "[Fold 3 Epoch 220] Train Loss: 0.0486 | Val Loss: 0.0487\n",
      "[Fold 3 Epoch 230] Train Loss: 0.0496 | Val Loss: 0.0428\n",
      "[Fold 3 Epoch 240] Train Loss: 0.0425 | Val Loss: 0.0414\n",
      "[Fold 3 Epoch 250] Train Loss: 0.0403 | Val Loss: 0.0449\n",
      "[Fold 3 Epoch 260] Train Loss: 0.0495 | Val Loss: 0.0453\n",
      "[Fold 3 Epoch 270] Train Loss: 0.0413 | Val Loss: 0.0429\n",
      "[Fold 3 Epoch 280] Train Loss: 0.0435 | Val Loss: 0.0379\n",
      "[Fold 3 Epoch 290] Train Loss: 0.0427 | Val Loss: 0.0393\n",
      "[Fold 3 Epoch 300] Train Loss: 0.0440 | Val Loss: 0.0377\n",
      "[Fold 3 Epoch 310] Train Loss: 0.0384 | Val Loss: 0.0385\n",
      "[Fold 3 Epoch 320] Train Loss: 0.0427 | Val Loss: 0.0403\n",
      "[Fold 3 Epoch 330] Train Loss: 0.0387 | Val Loss: 0.0448\n",
      "[Fold 3 Epoch 340] Train Loss: 0.0430 | Val Loss: 0.0439\n",
      "[Fold 3 Epoch 350] Train Loss: 0.0402 | Val Loss: 0.0357\n",
      "[Fold 3 Epoch 360] Train Loss: 0.0368 | Val Loss: 0.0353\n",
      "[Fold 3 Epoch 370] Train Loss: 0.0411 | Val Loss: 0.0431\n",
      "[Fold 3 Epoch 380] Train Loss: 0.0402 | Val Loss: 0.0364\n",
      "[Fold 3 Epoch 390] Train Loss: 0.0381 | Val Loss: 0.0357\n",
      "[Fold 3 Epoch 400] Train Loss: 0.0405 | Val Loss: 0.0375\n",
      "[Fold 3 Epoch 410] Train Loss: 0.0413 | Val Loss: 0.0340\n",
      "[Fold 3 Epoch 420] Train Loss: 0.0420 | Val Loss: 0.0343\n",
      "[Fold 3 Epoch 430] Train Loss: 0.0405 | Val Loss: 0.0343\n",
      "[Fold 3 Epoch 440] Train Loss: 0.0397 | Val Loss: 0.0340\n",
      "[Fold 3 Epoch 450] Train Loss: 0.0402 | Val Loss: 0.0360\n",
      "[Fold 3 Epoch 460] Train Loss: 0.0382 | Val Loss: 0.0396\n",
      "[Fold 3 Epoch 470] Train Loss: 0.0411 | Val Loss: 0.0350\n",
      "[Fold 3 Epoch 480] Train Loss: 0.0426 | Val Loss: 0.0347\n",
      "[Fold 3 Epoch 490] Train Loss: 0.0367 | Val Loss: 0.0351\n",
      "[Fold 3 Epoch 500] Train Loss: 0.0398 | Val Loss: 0.0314\n",
      "Evaluating fold 3 ...\n",
      "R²: 0.5500\n",
      "RMSE: 0.1772\n",
      "\n",
      "--- Fold 4 ---\n",
      "[Fold 4 Epoch 10] Train Loss: 0.1662 | Val Loss: 0.0956\n",
      "[Fold 4 Epoch 20] Train Loss: 0.1288 | Val Loss: 0.1279\n",
      "[Fold 4 Epoch 30] Train Loss: 0.0853 | Val Loss: 0.0974\n",
      "[Fold 4 Epoch 40] Train Loss: 0.0789 | Val Loss: 0.0863\n",
      "[Fold 4 Epoch 50] Train Loss: 0.0773 | Val Loss: 0.0990\n",
      "[Fold 4 Epoch 60] Train Loss: 0.0892 | Val Loss: 0.0881\n",
      "[Fold 4 Epoch 70] Train Loss: 0.0686 | Val Loss: 0.0940\n",
      "[Fold 4 Epoch 80] Train Loss: 0.0671 | Val Loss: 0.0921\n",
      "[Fold 4 Epoch 90] Train Loss: 0.0596 | Val Loss: 0.0935\n",
      "[Fold 4 Epoch 100] Train Loss: 0.0616 | Val Loss: 0.0897\n",
      "[Fold 4 Epoch 110] Train Loss: 0.0542 | Val Loss: 0.0850\n",
      "[Fold 4 Epoch 120] Train Loss: 0.0581 | Val Loss: 0.0852\n",
      "[Fold 4 Epoch 130] Train Loss: 0.0592 | Val Loss: 0.0866\n",
      "[Fold 4 Epoch 140] Train Loss: 0.0605 | Val Loss: 0.0879\n",
      "[Fold 4 Epoch 150] Train Loss: 0.0609 | Val Loss: 0.0823\n",
      "[Fold 4 Epoch 160] Train Loss: 0.0766 | Val Loss: 0.0854\n",
      "[Fold 4 Epoch 170] Train Loss: 0.0611 | Val Loss: 0.0833\n",
      "[Fold 4 Epoch 180] Train Loss: 0.0574 | Val Loss: 0.0807\n",
      "[Fold 4 Epoch 190] Train Loss: 0.0601 | Val Loss: 0.0803\n",
      "[Fold 4 Epoch 200] Train Loss: 0.0517 | Val Loss: 0.0768\n",
      "[Fold 4 Epoch 210] Train Loss: 0.0575 | Val Loss: 0.0729\n",
      "[Fold 4 Epoch 220] Train Loss: 0.0551 | Val Loss: 0.0723\n",
      "[Fold 4 Epoch 230] Train Loss: 0.0619 | Val Loss: 0.0737\n",
      "[Fold 4 Epoch 240] Train Loss: 0.0561 | Val Loss: 0.0690\n",
      "[Fold 4 Epoch 250] Train Loss: 0.0572 | Val Loss: 0.0686\n",
      "[Fold 4 Epoch 260] Train Loss: 0.0484 | Val Loss: 0.0686\n",
      "[Fold 4 Epoch 270] Train Loss: 0.0550 | Val Loss: 0.0718\n",
      "[Fold 4 Epoch 280] Train Loss: 0.0535 | Val Loss: 0.0788\n",
      "[Fold 4 Epoch 290] Train Loss: 0.0469 | Val Loss: 0.0696\n",
      "[Fold 4 Epoch 300] Train Loss: 0.0506 | Val Loss: 0.0700\n",
      "[Fold 4 Epoch 310] Train Loss: 0.0474 | Val Loss: 0.0665\n",
      "[Fold 4 Epoch 320] Train Loss: 0.0504 | Val Loss: 0.0686\n",
      "[Fold 4 Epoch 330] Train Loss: 0.0474 | Val Loss: 0.0685\n",
      "[Fold 4 Epoch 340] Train Loss: 0.0420 | Val Loss: 0.0556\n",
      "[Fold 4 Epoch 350] Train Loss: 0.0508 | Val Loss: 0.0637\n",
      "[Fold 4 Epoch 360] Train Loss: 0.0383 | Val Loss: 0.0653\n",
      "[Fold 4 Epoch 370] Train Loss: 0.0454 | Val Loss: 0.0551\n",
      "[Fold 4 Epoch 380] Train Loss: 0.0440 | Val Loss: 0.0576\n",
      "[Fold 4 Epoch 390] Train Loss: 0.0419 | Val Loss: 0.0491\n",
      "[Fold 4 Epoch 400] Train Loss: 0.0424 | Val Loss: 0.0499\n",
      "[Fold 4 Epoch 410] Train Loss: 0.0403 | Val Loss: 0.0552\n",
      "[Fold 4 Epoch 420] Train Loss: 0.0440 | Val Loss: 0.0538\n",
      "[Fold 4 Epoch 430] Train Loss: 0.0442 | Val Loss: 0.0508\n",
      "[Fold 4 Epoch 440] Train Loss: 0.0354 | Val Loss: 0.0539\n",
      "[Fold 4 Epoch 450] Train Loss: 0.0408 | Val Loss: 0.0457\n",
      "[Fold 4 Epoch 460] Train Loss: 0.0444 | Val Loss: 0.0500\n",
      "[Fold 4 Epoch 470] Train Loss: 0.0396 | Val Loss: 0.0494\n",
      "[Fold 4 Epoch 480] Train Loss: 0.0464 | Val Loss: 0.0494\n",
      "[Fold 4 Epoch 490] Train Loss: 0.0351 | Val Loss: 0.0448\n",
      "[Fold 4 Epoch 500] Train Loss: 0.0347 | Val Loss: 0.0472\n",
      "Evaluating fold 4 ...\n",
      "R²: 0.4746\n",
      "RMSE: 0.2172\n",
      "\n",
      "--- Fold 5 ---\n",
      "[Fold 5 Epoch 10] Train Loss: 0.2208 | Val Loss: 0.0688\n",
      "[Fold 5 Epoch 20] Train Loss: 0.1073 | Val Loss: 0.0548\n",
      "[Fold 5 Epoch 30] Train Loss: 0.1024 | Val Loss: 0.0553\n",
      "[Fold 5 Epoch 40] Train Loss: 0.0846 | Val Loss: 0.0596\n",
      "[Fold 5 Epoch 50] Train Loss: 0.0784 | Val Loss: 0.0583\n",
      "[Fold 5 Epoch 60] Train Loss: 0.0628 | Val Loss: 0.0872\n",
      "[Fold 5 Epoch 70] Train Loss: 0.0664 | Val Loss: 0.0566\n",
      "[Fold 5 Epoch 80] Train Loss: 0.0647 | Val Loss: 0.0594\n",
      "[Fold 5 Epoch 90] Train Loss: 0.0664 | Val Loss: 0.0573\n",
      "[Fold 5 Epoch 100] Train Loss: 0.0610 | Val Loss: 0.0547\n",
      "[Fold 5 Epoch 110] Train Loss: 0.0674 | Val Loss: 0.0570\n",
      "[Fold 5 Epoch 120] Train Loss: 0.0668 | Val Loss: 0.0541\n",
      "[Fold 5 Epoch 130] Train Loss: 0.0618 | Val Loss: 0.0540\n",
      "[Fold 5 Epoch 140] Train Loss: 0.0601 | Val Loss: 0.0535\n",
      "[Fold 5 Epoch 150] Train Loss: 0.0619 | Val Loss: 0.0549\n",
      "[Fold 5 Epoch 160] Train Loss: 0.0626 | Val Loss: 0.0563\n",
      "[Fold 5 Epoch 170] Train Loss: 0.0622 | Val Loss: 0.0554\n",
      "[Fold 5 Epoch 180] Train Loss: 0.0613 | Val Loss: 0.0542\n",
      "[Fold 5 Epoch 190] Train Loss: 0.0624 | Val Loss: 0.0529\n",
      "[Fold 5 Epoch 200] Train Loss: 0.0588 | Val Loss: 0.0523\n",
      "[Fold 5 Epoch 210] Train Loss: 0.0550 | Val Loss: 0.0527\n",
      "[Fold 5 Epoch 220] Train Loss: 0.0662 | Val Loss: 0.0518\n",
      "[Fold 5 Epoch 230] Train Loss: 0.0609 | Val Loss: 0.0505\n",
      "[Fold 5 Epoch 240] Train Loss: 0.0564 | Val Loss: 0.0507\n",
      "[Fold 5 Epoch 250] Train Loss: 0.0541 | Val Loss: 0.0484\n",
      "[Fold 5 Epoch 260] Train Loss: 0.0614 | Val Loss: 0.0423\n",
      "[Fold 5 Epoch 270] Train Loss: 0.0528 | Val Loss: 0.0388\n",
      "[Fold 5 Epoch 280] Train Loss: 0.0484 | Val Loss: 0.0369\n",
      "[Fold 5 Epoch 290] Train Loss: 0.0473 | Val Loss: 0.0471\n",
      "[Fold 5 Epoch 300] Train Loss: 0.0524 | Val Loss: 0.0341\n",
      "[Fold 5 Epoch 310] Train Loss: 0.0391 | Val Loss: 0.0348\n",
      "[Fold 5 Epoch 320] Train Loss: 0.0482 | Val Loss: 0.0361\n",
      "[Fold 5 Epoch 330] Train Loss: 0.0448 | Val Loss: 0.0362\n",
      "[Fold 5 Epoch 340] Train Loss: 0.0474 | Val Loss: 0.0350\n",
      "[Fold 5 Epoch 350] Train Loss: 0.0559 | Val Loss: 0.0320\n",
      "[Fold 5 Epoch 360] Train Loss: 0.0428 | Val Loss: 0.0342\n",
      "[Fold 5 Epoch 370] Train Loss: 0.0523 | Val Loss: 0.0376\n",
      "[Fold 5 Epoch 380] Train Loss: 0.0493 | Val Loss: 0.0307\n",
      "[Fold 5 Epoch 390] Train Loss: 0.0461 | Val Loss: 0.0366\n",
      "[Fold 5 Epoch 400] Train Loss: 0.0491 | Val Loss: 0.0328\n",
      "[Fold 5 Epoch 410] Train Loss: 0.0426 | Val Loss: 0.0337\n",
      "[Fold 5 Epoch 420] Train Loss: 0.0490 | Val Loss: 0.0325\n",
      "[Fold 5 Epoch 430] Train Loss: 0.0480 | Val Loss: 0.0325\n",
      "[Fold 5 Epoch 440] Train Loss: 0.0449 | Val Loss: 0.0347\n",
      "[Fold 5 Epoch 450] Train Loss: 0.0480 | Val Loss: 0.0317\n",
      "[Fold 5 Epoch 460] Train Loss: 0.0551 | Val Loss: 0.0334\n",
      "[Fold 5 Epoch 470] Train Loss: 0.0458 | Val Loss: 0.0361\n",
      "[Fold 5 Epoch 480] Train Loss: 0.0446 | Val Loss: 0.0313\n",
      "[Fold 5 Epoch 490] Train Loss: 0.0425 | Val Loss: 0.0332\n",
      "[Fold 5 Epoch 500] Train Loss: 0.0527 | Val Loss: 0.0336\n",
      "Evaluating fold 5 ...\n",
      "R²: 0.5154\n",
      "RMSE: 0.1833\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "Fold 1: R² = 0.4096, RMSE = 0.1821\n",
      "Fold 2: R² = 0.4225, RMSE = 0.2456\n",
      "Fold 3: R² = 0.5500, RMSE = 0.1772\n",
      "Fold 4: R² = 0.4746, RMSE = 0.2172\n",
      "Fold 5: R² = 0.5154, RMSE = 0.1833\n",
      "\n",
      "Average R²: 0.4744 ± 0.0535\n",
      "Average RMSE: 0.2011 ± 0.0264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold  #  k 折交叉验证\n",
    "\n",
    "# ---------------------\n",
    "# k-Fold Cross Validation\n",
    "# ---------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 5  # number of folds\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42) # shuffle = True 参数 随机打乱\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)): # 循环 kf.split的折叠 enumerate函数添加计数\n",
    "                                                                # fold 计数器 迭代次数\n",
    "    print(f\"\\n--- Fold {fold + 1} ---\")\n",
    "    \n",
    "    # Create train and validation subsets\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx) # 子集：训练集\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "    \n",
    "    train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True) # PyG 数据加载\n",
    "    val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Initialize a new instance of the model, optimizer, and loss function for each fold\n",
    "    model = GINE_Regression(\n",
    "        node_in_dim=1,\n",
    "        edge_in_dim=2,\n",
    "        external_in_dim=4,\n",
    "        hidden_dim=16,\n",
    "        num_layers=5,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    num_epochs = 500\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[Fold {fold + 1} Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    print(f\"Evaluating fold {fold + 1} ...\")\n",
    "    r2, rmse = evaluate_model(model, val_loader, device)\n",
    "    fold_results.append({\"fold\": fold + 1, \"r2\": r2, \"rmse\": rmse})\n",
    "\n",
    "# ---------------------\n",
    "# Summary of Cross-Validation Results\n",
    "# ---------------------\n",
    "r2_scores = [res[\"r2\"] for res in fold_results]\n",
    "rmse_scores = [res[\"rmse\"] for res in fold_results]\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "for res in fold_results:\n",
    "    print(f\"Fold {res['fold']}: R² = {res['r2']:.4f}, RMSE = {res['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfabet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
