{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the complete code with all the modifications integrated. In this version:\n",
    "\n",
    "**The number of layers is restricted to 2–6.\n",
    "For each trial, the hidden dimensions are sampled one by one so that they form a strictly decreasing (trapezoidal) sequence. For example, if you sample 128 for layer 0, then layer 1 will be chosen from values strictly less than 128.\n",
    "Hyperparameters (learning rate, dropout, num_layers, and each hidden dimension) are all logged so that they appear in the Optuna dashboard.\n",
    "During training, intermediate validation losses are reported to enable early-stopping tracking in the “Intermediate Values” plot of the dashboard.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN层遍历的架构逻辑\n",
    "\n",
    "1. **Use a Fixed Candidate Set:**  \n",
    "   Instead of sampling any integer between 16 and 256, we'll define a fixed set of allowable hidden dimensions. For example, the candidate set will be:  \n",
    "   ```python\n",
    "   candidate_values = [256, 128, 64, 32, 16]\n",
    "   ```  \n",
    "   This set contains exactly the values you mentioned in your examples.\n",
    "\n",
    "2. **Enforce a Non-Increasing (Trapezoidal) Structure:**  \n",
    "   For each layer, we will choose a hidden dimension from the candidate set with the following rules:\n",
    "   - **First Layer:**  \n",
    "     For layer 0, we simply choose one value from the entire candidate set.\n",
    "   - **Subsequent Layers:**  \n",
    "     For layer _i_ (where _i_ > 0), we restrict the available options to only those values that are less than or equal to the hidden dimension chosen for layer _i-1_.  \n",
    "     This means if the first layer was chosen as 256, then for the next layer, the allowed options will be [256, 128, 64, 32, 16].  \n",
    "     If the first layer is 128, then the second layer can only be one of [128, 64, 32, 16].  \n",
    "     This restriction ensures that the sequence is non-increasing (i.e., it \"gets narrower\" as you go deeper) and that only valid choices like [256,256,32,32] or [128,64,64] are considered.\n",
    "\n",
    "3. **Hyperparameter Sampling with Optuna:**  \n",
    "   - We use `trial.suggest_categorical` to sample the hidden dimension for each layer from the appropriate candidate list.\n",
    "   - The number of layers (between 2 and 6) is also a hyperparameter. For each layer in the chosen architecture, we apply the above logic to build the list of hidden dimensions.\n",
    "   - This way, every trial in the optimization will have a candidate architecture with a discrete, non-increasing set of hidden dimensions (for example, [256,256,32,32] or [128,64,64]) and will exclude choices like [256,155,17,16] because 155 and 17 are not in the candidate set.\n",
    "\n",
    "4. **Result Logging and Dashboard Integration:**  \n",
    "   - All hyperparameters (including the number of layers and each layer's hidden dimension) will be logged.\n",
    "   - Intermediate validation loss values will be reported during training so that you can track the early-stopping behavior on the Optuna dashboard (via the \"Intermediate Values\" plot).\n",
    "\n",
    "This logic guarantees that the search space will consist only of architectures that conform to a trapezoidal (non-increasing) pattern using your predefined candidate values. Once you confirm this approach, we can proceed to implement it in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 具体逻辑\n",
    "\n",
    "1. Fixed Candidate Set of Hidden Dimensions:\n",
    "We use candidate_values = [256, 128, 64, 32, 16].\n",
    "\n",
    "2. Non-Increasing (Trapezoidal) Architecture:\n",
    "\n",
    "- The first layer’s dimension is chosen from the full candidate set.\n",
    "- Each subsequent layer is chosen from those values less than or equal to the previous layer’s dimension, supporting valid patterns like [256, 256, 32, 32] or [128, 64, 64].\n",
    "\n",
    "3. Number of Layers Restriction (2–6):\n",
    "Controlled via trial.suggest_int(\"num_layers\", 2, 6).\n",
    "\n",
    "4. Hyperparameter Logging:\n",
    "\n",
    "- Learning rate, dropout, number of layers, and each hidden_dim_i (via trial.suggest_categorical).\n",
    "- All parameters automatically appear in the Optuna dashboard.\n",
    "\n",
    "5. Intermediate Validation Loss Reporting:\n",
    "\n",
    "trial.report(val_loss, step=epoch) is called each epoch, letting you track early stopping in the “Intermediate Values” plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How This Solves the Dynamic Value Space Error如何解决动态值空间错误\n",
    "Fixed Distribution:固定分布：\n",
    "Every hidden_dim_{i} parameter is sampled from the same list [256, 128, 64, 32, 16]. Thus, Optuna never sees a changing candidate list, which avoids the “CategoricalDistribution does not support dynamic value space.” error.每个hidden_dim_{i}参数都从同一个列表[256, 128, 64, 32, 16]中采样。因此，Optuna永远不会看到变化的候选列表，从而避免了“CategoricalDistribution 不支持动态值空间。”错误。\n",
    "\n",
    "Trapezoidal Constraint:梯形约束：\n",
    "After sampling each layer’s width, a quick check ensures subsequent layers’ widths do not exceed the previous layer’s. If no suitable options are found, the default is the smallest dimension (16). The net result is a non-increasing (trapezoidal) architecture, and Optuna does not complain because each parameter’s distribution remains constant.在对每层的宽度进行采样后，会进行快速检查，确保后续层的宽度不超过前一层的宽度。如果未找到合适的选项，则默认为最小尺寸（16）。最终结果是非递增（梯形）架构，Optuna 不会抱怨，因为每个参数的分布保持不变。\n",
    "\n",
    "Pruning & Logging:修剪和伐木：\n",
    "\n",
    "If needed, you can prune trials that violate the non-increasing rule—though the code above simply narrows the candidate options.如果需要，您可以修剪违反非增加规则的试验 - 尽管上述代码只是缩小了候选选项的范围。\n",
    "Hyperparameters (learning_rate, dropout, num_layers, hidden_dim_i) are all logged for dashboard inspection.超参数（ learning_rate 、 dropout 、 num_layers 、 hidden_dim_i ）均已记录以供仪表板检查。\n",
    "trial.report(val_loss, step=epoch) allows you to visualize intermediate values for early-stopping.trial.report(val_loss, step=epoch) 可让您直观地看到提前停止的中间值。\n",
    "By combining a fixed candidate set for every layer with a post-check that filters invalid configurations, you get your desired trapezoidal GNN structure and avoid the Optuna dynamic-distribution error.通过将每一层的固定候选集与过滤无效配置的后检查相结合，您可以获得所需的梯形 GNN 结构并避免 Optuna 动态分布错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\n",
      "Files in directory: ['gpickle_graph_0.pkl', 'gpickle_graph_1.pkl', 'gpickle_graph_10.pkl', 'gpickle_graph_11.pkl', 'gpickle_graph_12.pkl', 'gpickle_graph_13.pkl', 'gpickle_graph_14.pkl', 'gpickle_graph_15.pkl', 'gpickle_graph_16.pkl', 'gpickle_graph_17.pkl', 'gpickle_graph_18.pkl', 'gpickle_graph_19.pkl', 'gpickle_graph_2.pkl', 'gpickle_graph_3.pkl', 'gpickle_graph_4.pkl', 'gpickle_graph_5.pkl', 'gpickle_graph_6.pkl', 'gpickle_graph_7.pkl', 'gpickle_graph_8.pkl', 'gpickle_graph_9.pkl']\n",
      "\n",
      "--- Fold 1 ---\n",
      "[Fold 1 Epoch 10] Train Loss: 0.1368 | Val Loss: 0.0586\n",
      "[Fold 1 Epoch 20] Train Loss: 0.1047 | Val Loss: 0.0541\n",
      "[Fold 1 Epoch 30] Train Loss: 0.0926 | Val Loss: 0.0775\n",
      "[Fold 1 Epoch 40] Train Loss: 0.0855 | Val Loss: 0.0487\n",
      "[Fold 1 Epoch 50] Train Loss: 0.0862 | Val Loss: 0.0466\n",
      "[Fold 1 Epoch 60] Train Loss: 0.0738 | Val Loss: 0.0469\n",
      "[Fold 1 Epoch 70] Train Loss: 0.0643 | Val Loss: 0.0505\n",
      "[Fold 1 Epoch 80] Train Loss: 0.0683 | Val Loss: 0.0451\n",
      "[Fold 1 Epoch 90] Train Loss: 0.0662 | Val Loss: 0.0437\n",
      "[Fold 1 Epoch 100] Train Loss: 0.0621 | Val Loss: 0.0429\n",
      "[Fold 1 Epoch 110] Train Loss: 0.0549 | Val Loss: 0.0663\n",
      "[Fold 1 Epoch 120] Train Loss: 0.0511 | Val Loss: 0.0453\n",
      "[Fold 1 Epoch 130] Train Loss: 0.0550 | Val Loss: 0.0421\n",
      "[Fold 1 Epoch 140] Train Loss: 0.0529 | Val Loss: 0.0398\n",
      "[Fold 1 Epoch 150] Train Loss: 0.0417 | Val Loss: 0.0417\n",
      "[Fold 1 Epoch 160] Train Loss: 0.0453 | Val Loss: 0.0384\n",
      "[Fold 1 Epoch 170] Train Loss: 0.0492 | Val Loss: 0.0352\n",
      "[Fold 1 Epoch 180] Train Loss: 0.0478 | Val Loss: 0.0408\n",
      "[Fold 1 Epoch 190] Train Loss: 0.0517 | Val Loss: 0.0393\n",
      "[Fold 1 Epoch 200] Train Loss: 0.0437 | Val Loss: 0.0412\n",
      "[Fold 1 Epoch 210] Train Loss: 0.0427 | Val Loss: 0.0400\n",
      "[Fold 1 Epoch 220] Train Loss: 0.0456 | Val Loss: 0.0395\n",
      "[Fold 1 Epoch 230] Train Loss: 0.0412 | Val Loss: 0.0400\n",
      "[Fold 1 Epoch 240] Train Loss: 0.0527 | Val Loss: 0.0387\n",
      "[Fold 1 Epoch 250] Train Loss: 0.0438 | Val Loss: 0.0367\n",
      "[Fold 1 Epoch 260] Train Loss: 0.0450 | Val Loss: 0.0377\n",
      "[Fold 1 Epoch 270] Train Loss: 0.0435 | Val Loss: 0.0481\n",
      "[Fold 1 Epoch 280] Train Loss: 0.0413 | Val Loss: 0.0401\n",
      "[Fold 1 Epoch 290] Train Loss: 0.0369 | Val Loss: 0.0378\n",
      "[Fold 1 Epoch 300] Train Loss: 0.0432 | Val Loss: 0.0375\n",
      "[Fold 1 Epoch 310] Train Loss: 0.0392 | Val Loss: 0.0328\n",
      "[Fold 1 Epoch 320] Train Loss: 0.0366 | Val Loss: 0.0347\n",
      "[Fold 1 Epoch 330] Train Loss: 0.0392 | Val Loss: 0.0332\n",
      "[Fold 1 Epoch 340] Train Loss: 0.0447 | Val Loss: 0.0371\n",
      "[Fold 1 Epoch 350] Train Loss: 0.0404 | Val Loss: 0.0348\n",
      "[Fold 1 Epoch 360] Train Loss: 0.0425 | Val Loss: 0.0401\n",
      "[Fold 1 Epoch 370] Train Loss: 0.0394 | Val Loss: 0.0344\n",
      "[Fold 1 Epoch 380] Train Loss: 0.0409 | Val Loss: 0.0347\n",
      "[Fold 1 Epoch 390] Train Loss: 0.0419 | Val Loss: 0.0360\n",
      "[Fold 1 Epoch 400] Train Loss: 0.0440 | Val Loss: 0.0394\n",
      "[Fold 1 Epoch 410] Train Loss: 0.0409 | Val Loss: 0.0361\n",
      "[Fold 1 Epoch 420] Train Loss: 0.0437 | Val Loss: 0.0348\n",
      "[Fold 1 Epoch 430] Train Loss: 0.0373 | Val Loss: 0.0315\n",
      "[Fold 1 Epoch 440] Train Loss: 0.0353 | Val Loss: 0.0561\n",
      "[Fold 1 Epoch 450] Train Loss: 0.0339 | Val Loss: 0.0317\n",
      "[Fold 1 Epoch 460] Train Loss: 0.0436 | Val Loss: 0.0334\n",
      "[Fold 1 Epoch 470] Train Loss: 0.0386 | Val Loss: 0.0429\n",
      "[Fold 1 Epoch 480] Train Loss: 0.0372 | Val Loss: 0.0338\n",
      "[Fold 1 Epoch 490] Train Loss: 0.0421 | Val Loss: 0.0332\n",
      "[Fold 1 Epoch 500] Train Loss: 0.0402 | Val Loss: 0.0321\n",
      "Evaluating fold 1 ...\n",
      "R²: 0.4278\n",
      "RMSE: 0.1793\n",
      "\n",
      "--- Fold 2 ---\n",
      "[Fold 2 Epoch 10] Train Loss: 0.3577 | Val Loss: 0.1316\n",
      "[Fold 2 Epoch 20] Train Loss: 0.1273 | Val Loss: 0.1011\n",
      "[Fold 2 Epoch 30] Train Loss: 0.0861 | Val Loss: 0.0907\n",
      "[Fold 2 Epoch 40] Train Loss: 0.0808 | Val Loss: 0.1095\n",
      "[Fold 2 Epoch 50] Train Loss: 0.0757 | Val Loss: 0.0998\n",
      "[Fold 2 Epoch 60] Train Loss: 0.0749 | Val Loss: 0.0919\n",
      "[Fold 2 Epoch 70] Train Loss: 0.0729 | Val Loss: 0.0933\n",
      "[Fold 2 Epoch 80] Train Loss: 0.0752 | Val Loss: 0.1030\n",
      "[Fold 2 Epoch 90] Train Loss: 0.0600 | Val Loss: 0.0862\n",
      "[Fold 2 Epoch 100] Train Loss: 0.0549 | Val Loss: 0.0888\n",
      "[Fold 2 Epoch 110] Train Loss: 0.0465 | Val Loss: 0.0789\n",
      "[Fold 2 Epoch 120] Train Loss: 0.0563 | Val Loss: 0.0942\n",
      "[Fold 2 Epoch 130] Train Loss: 0.0555 | Val Loss: 0.0835\n",
      "[Fold 2 Epoch 140] Train Loss: 0.0624 | Val Loss: 0.0773\n",
      "[Fold 2 Epoch 150] Train Loss: 0.0572 | Val Loss: 0.0778\n",
      "[Fold 2 Epoch 160] Train Loss: 0.0579 | Val Loss: 0.0797\n",
      "[Fold 2 Epoch 170] Train Loss: 0.0582 | Val Loss: 0.0886\n",
      "[Fold 2 Epoch 180] Train Loss: 0.0586 | Val Loss: 0.0810\n",
      "[Fold 2 Epoch 190] Train Loss: 0.0554 | Val Loss: 0.0853\n",
      "[Fold 2 Epoch 200] Train Loss: 0.0547 | Val Loss: 0.0793\n",
      "[Fold 2 Epoch 210] Train Loss: 0.0587 | Val Loss: 0.0742\n",
      "[Fold 2 Epoch 220] Train Loss: 0.0529 | Val Loss: 0.0938\n",
      "[Fold 2 Epoch 230] Train Loss: 0.0534 | Val Loss: 0.0743\n",
      "[Fold 2 Epoch 240] Train Loss: 0.0500 | Val Loss: 0.0857\n",
      "[Fold 2 Epoch 250] Train Loss: 0.0516 | Val Loss: 0.0977\n",
      "[Fold 2 Epoch 260] Train Loss: 0.0595 | Val Loss: 0.0688\n",
      "[Fold 2 Epoch 270] Train Loss: 0.0500 | Val Loss: 0.0752\n",
      "[Fold 2 Epoch 280] Train Loss: 0.0536 | Val Loss: 0.0838\n",
      "[Fold 2 Epoch 290] Train Loss: 0.0480 | Val Loss: 0.0760\n",
      "[Fold 2 Epoch 300] Train Loss: 0.0489 | Val Loss: 0.0694\n",
      "[Fold 2 Epoch 310] Train Loss: 0.0448 | Val Loss: 0.0776\n",
      "[Fold 2 Epoch 320] Train Loss: 0.0412 | Val Loss: 0.0733\n",
      "[Fold 2 Epoch 330] Train Loss: 0.0508 | Val Loss: 0.0753\n",
      "[Fold 2 Epoch 340] Train Loss: 0.0372 | Val Loss: 0.0520\n",
      "[Fold 2 Epoch 350] Train Loss: 0.0448 | Val Loss: 0.0495\n",
      "[Fold 2 Epoch 360] Train Loss: 0.0458 | Val Loss: 0.0551\n",
      "[Fold 2 Epoch 370] Train Loss: 0.0430 | Val Loss: 0.0658\n",
      "[Fold 2 Epoch 380] Train Loss: 0.0358 | Val Loss: 0.0512\n",
      "[Fold 2 Epoch 390] Train Loss: 0.0427 | Val Loss: 0.0499\n",
      "[Fold 2 Epoch 400] Train Loss: 0.0387 | Val Loss: 0.0526\n",
      "[Fold 2 Epoch 410] Train Loss: 0.0370 | Val Loss: 0.0553\n",
      "[Fold 2 Epoch 420] Train Loss: 0.0415 | Val Loss: 0.0630\n",
      "[Fold 2 Epoch 430] Train Loss: 0.0460 | Val Loss: 0.0537\n",
      "[Fold 2 Epoch 440] Train Loss: 0.0382 | Val Loss: 0.0464\n",
      "[Fold 2 Epoch 450] Train Loss: 0.0408 | Val Loss: 0.0491\n",
      "[Fold 2 Epoch 460] Train Loss: 0.0392 | Val Loss: 0.0516\n",
      "[Fold 2 Epoch 470] Train Loss: 0.0383 | Val Loss: 0.0458\n",
      "[Fold 2 Epoch 480] Train Loss: 0.0425 | Val Loss: 0.0481\n",
      "[Fold 2 Epoch 490] Train Loss: 0.0386 | Val Loss: 0.0555\n",
      "[Fold 2 Epoch 500] Train Loss: 0.0350 | Val Loss: 0.0501\n",
      "Evaluating fold 2 ...\n",
      "R²: 0.5202\n",
      "RMSE: 0.2238\n",
      "\n",
      "--- Fold 3 ---\n",
      "[Fold 3 Epoch 10] Train Loss: 0.1039 | Val Loss: 0.0694\n",
      "[Fold 3 Epoch 20] Train Loss: 0.0892 | Val Loss: 0.0693\n",
      "[Fold 3 Epoch 30] Train Loss: 0.0838 | Val Loss: 0.0585\n",
      "[Fold 3 Epoch 40] Train Loss: 0.0788 | Val Loss: 0.0584\n",
      "[Fold 3 Epoch 50] Train Loss: 0.0555 | Val Loss: 0.0588\n",
      "[Fold 3 Epoch 60] Train Loss: 0.0695 | Val Loss: 0.0613\n",
      "[Fold 3 Epoch 70] Train Loss: 0.0698 | Val Loss: 0.0599\n",
      "[Fold 3 Epoch 80] Train Loss: 0.0671 | Val Loss: 0.0607\n",
      "[Fold 3 Epoch 90] Train Loss: 0.0658 | Val Loss: 0.0582\n",
      "[Fold 3 Epoch 100] Train Loss: 0.0632 | Val Loss: 0.0520\n",
      "[Fold 3 Epoch 110] Train Loss: 0.0663 | Val Loss: 0.0536\n",
      "[Fold 3 Epoch 120] Train Loss: 0.0618 | Val Loss: 0.0634\n",
      "[Fold 3 Epoch 130] Train Loss: 0.0644 | Val Loss: 0.0568\n",
      "[Fold 3 Epoch 140] Train Loss: 0.0621 | Val Loss: 0.0513\n",
      "[Fold 3 Epoch 150] Train Loss: 0.0588 | Val Loss: 0.0634\n",
      "[Fold 3 Epoch 160] Train Loss: 0.0589 | Val Loss: 0.0549\n",
      "[Fold 3 Epoch 170] Train Loss: 0.0559 | Val Loss: 0.0470\n",
      "[Fold 3 Epoch 180] Train Loss: 0.0661 | Val Loss: 0.0504\n",
      "[Fold 3 Epoch 190] Train Loss: 0.0591 | Val Loss: 0.0591\n",
      "[Fold 3 Epoch 200] Train Loss: 0.0527 | Val Loss: 0.0486\n",
      "[Fold 3 Epoch 210] Train Loss: 0.0486 | Val Loss: 0.0475\n",
      "[Fold 3 Epoch 220] Train Loss: 0.0536 | Val Loss: 0.0458\n",
      "[Fold 3 Epoch 230] Train Loss: 0.0532 | Val Loss: 0.0506\n",
      "[Fold 3 Epoch 240] Train Loss: 0.0523 | Val Loss: 0.0513\n",
      "[Fold 3 Epoch 250] Train Loss: 0.0548 | Val Loss: 0.0431\n",
      "[Fold 3 Epoch 260] Train Loss: 0.0491 | Val Loss: 0.0598\n",
      "[Fold 3 Epoch 270] Train Loss: 0.0499 | Val Loss: 0.0455\n",
      "[Fold 3 Epoch 280] Train Loss: 0.0531 | Val Loss: 0.0415\n",
      "[Fold 3 Epoch 290] Train Loss: 0.0424 | Val Loss: 0.0397\n",
      "[Fold 3 Epoch 300] Train Loss: 0.0508 | Val Loss: 0.0408\n",
      "[Fold 3 Epoch 310] Train Loss: 0.0476 | Val Loss: 0.0407\n",
      "[Fold 3 Epoch 320] Train Loss: 0.0384 | Val Loss: 0.0395\n",
      "[Fold 3 Epoch 330] Train Loss: 0.0526 | Val Loss: 0.0416\n",
      "[Fold 3 Epoch 340] Train Loss: 0.0492 | Val Loss: 0.0380\n",
      "[Fold 3 Epoch 350] Train Loss: 0.0457 | Val Loss: 0.0421\n",
      "[Fold 3 Epoch 360] Train Loss: 0.0467 | Val Loss: 0.0397\n",
      "[Fold 3 Epoch 370] Train Loss: 0.0486 | Val Loss: 0.0402\n",
      "[Fold 3 Epoch 380] Train Loss: 0.0457 | Val Loss: 0.0431\n",
      "[Fold 3 Epoch 390] Train Loss: 0.0529 | Val Loss: 0.0385\n",
      "[Fold 3 Epoch 400] Train Loss: 0.0458 | Val Loss: 0.0356\n",
      "[Fold 3 Epoch 410] Train Loss: 0.0461 | Val Loss: 0.0405\n",
      "[Fold 3 Epoch 420] Train Loss: 0.0420 | Val Loss: 0.0347\n",
      "[Fold 3 Epoch 430] Train Loss: 0.0370 | Val Loss: 0.0333\n",
      "[Fold 3 Epoch 440] Train Loss: 0.0459 | Val Loss: 0.0425\n",
      "[Fold 3 Epoch 450] Train Loss: 0.0370 | Val Loss: 0.0314\n",
      "[Fold 3 Epoch 460] Train Loss: 0.0395 | Val Loss: 0.0435\n",
      "[Fold 3 Epoch 470] Train Loss: 0.0426 | Val Loss: 0.0377\n",
      "[Fold 3 Epoch 480] Train Loss: 0.0402 | Val Loss: 0.0342\n",
      "[Fold 3 Epoch 490] Train Loss: 0.0341 | Val Loss: 0.0343\n",
      "[Fold 3 Epoch 500] Train Loss: 0.0385 | Val Loss: 0.0384\n",
      "Evaluating fold 3 ...\n",
      "R²: 0.4507\n",
      "RMSE: 0.1958\n",
      "\n",
      "--- Fold 4 ---\n",
      "[Fold 4 Epoch 10] Train Loss: 0.0955 | Val Loss: 0.0985\n",
      "[Fold 4 Epoch 20] Train Loss: 0.0688 | Val Loss: 0.0890\n",
      "[Fold 4 Epoch 30] Train Loss: 0.0595 | Val Loss: 0.0802\n",
      "[Fold 4 Epoch 40] Train Loss: 0.0556 | Val Loss: 0.0739\n",
      "[Fold 4 Epoch 50] Train Loss: 0.0585 | Val Loss: 0.0844\n",
      "[Fold 4 Epoch 60] Train Loss: 0.0556 | Val Loss: 0.0732\n",
      "[Fold 4 Epoch 70] Train Loss: 0.0544 | Val Loss: 0.0690\n",
      "[Fold 4 Epoch 80] Train Loss: 0.0484 | Val Loss: 0.0701\n",
      "[Fold 4 Epoch 90] Train Loss: 0.0502 | Val Loss: 0.0665\n",
      "[Fold 4 Epoch 100] Train Loss: 0.0480 | Val Loss: 0.0693\n",
      "[Fold 4 Epoch 110] Train Loss: 0.0538 | Val Loss: 0.0671\n",
      "[Fold 4 Epoch 120] Train Loss: 0.0498 | Val Loss: 0.0725\n",
      "[Fold 4 Epoch 130] Train Loss: 0.0496 | Val Loss: 0.0637\n",
      "[Fold 4 Epoch 140] Train Loss: 0.0498 | Val Loss: 0.0635\n",
      "[Fold 4 Epoch 150] Train Loss: 0.0448 | Val Loss: 0.0689\n",
      "[Fold 4 Epoch 160] Train Loss: 0.0492 | Val Loss: 0.0610\n",
      "[Fold 4 Epoch 170] Train Loss: 0.0473 | Val Loss: 0.0668\n",
      "[Fold 4 Epoch 180] Train Loss: 0.0493 | Val Loss: 0.0611\n",
      "[Fold 4 Epoch 190] Train Loss: 0.0451 | Val Loss: 0.0609\n",
      "[Fold 4 Epoch 200] Train Loss: 0.0563 | Val Loss: 0.0607\n",
      "[Fold 4 Epoch 210] Train Loss: 0.0505 | Val Loss: 0.0576\n",
      "[Fold 4 Epoch 220] Train Loss: 0.0457 | Val Loss: 0.0556\n",
      "[Fold 4 Epoch 230] Train Loss: 0.0441 | Val Loss: 0.0785\n",
      "[Fold 4 Epoch 240] Train Loss: 0.0393 | Val Loss: 0.0466\n",
      "[Fold 4 Epoch 250] Train Loss: 0.0429 | Val Loss: 0.0870\n",
      "[Fold 4 Epoch 260] Train Loss: 0.0386 | Val Loss: 0.0443\n",
      "[Fold 4 Epoch 270] Train Loss: 0.0445 | Val Loss: 0.0547\n",
      "[Fold 4 Epoch 280] Train Loss: 0.0402 | Val Loss: 0.0552\n",
      "[Fold 4 Epoch 290] Train Loss: 0.0371 | Val Loss: 0.0889\n",
      "[Fold 4 Epoch 300] Train Loss: 0.0396 | Val Loss: 0.0451\n",
      "[Fold 4 Epoch 310] Train Loss: 0.0410 | Val Loss: 0.1205\n",
      "[Fold 4 Epoch 320] Train Loss: 0.0363 | Val Loss: 0.1609\n",
      "[Fold 4 Epoch 330] Train Loss: 0.0392 | Val Loss: 0.2400\n",
      "[Fold 4 Epoch 340] Train Loss: 0.0349 | Val Loss: 0.0510\n",
      "[Fold 4 Epoch 350] Train Loss: 0.0454 | Val Loss: 0.0704\n",
      "[Fold 4 Epoch 360] Train Loss: 0.0449 | Val Loss: 0.0709\n",
      "[Fold 4 Epoch 370] Train Loss: 0.0341 | Val Loss: 0.0471\n",
      "[Fold 4 Epoch 380] Train Loss: 0.0398 | Val Loss: 0.0738\n",
      "[Fold 4 Epoch 390] Train Loss: 0.0436 | Val Loss: 0.1592\n",
      "[Fold 4 Epoch 400] Train Loss: 0.0427 | Val Loss: 1.2678\n",
      "[Fold 4 Epoch 410] Train Loss: 0.0422 | Val Loss: 6.4763\n",
      "[Fold 4 Epoch 420] Train Loss: 0.0361 | Val Loss: 0.0934\n",
      "[Fold 4 Epoch 430] Train Loss: 0.0398 | Val Loss: 0.0551\n",
      "[Fold 4 Epoch 440] Train Loss: 0.0322 | Val Loss: 0.0653\n",
      "[Fold 4 Epoch 450] Train Loss: 0.0411 | Val Loss: 0.0880\n",
      "[Fold 4 Epoch 460] Train Loss: 0.0450 | Val Loss: 0.1074\n",
      "[Fold 4 Epoch 470] Train Loss: 0.0323 | Val Loss: 0.0675\n",
      "[Fold 4 Epoch 480] Train Loss: 0.0348 | Val Loss: 0.1819\n",
      "[Fold 4 Epoch 490] Train Loss: 0.0333 | Val Loss: 0.0707\n",
      "[Fold 4 Epoch 500] Train Loss: 0.0479 | Val Loss: 0.0583\n",
      "Evaluating fold 4 ...\n",
      "R²: 0.3507\n",
      "RMSE: 0.2414\n",
      "\n",
      "--- Fold 5 ---\n",
      "[Fold 5 Epoch 10] Train Loss: 0.0997 | Val Loss: 0.0683\n",
      "[Fold 5 Epoch 20] Train Loss: 0.0810 | Val Loss: 0.0581\n",
      "[Fold 5 Epoch 30] Train Loss: 0.0713 | Val Loss: 0.0567\n",
      "[Fold 5 Epoch 40] Train Loss: 0.0675 | Val Loss: 0.0535\n",
      "[Fold 5 Epoch 50] Train Loss: 0.0643 | Val Loss: 0.0585\n",
      "[Fold 5 Epoch 60] Train Loss: 0.0660 | Val Loss: 0.0528\n",
      "[Fold 5 Epoch 70] Train Loss: 0.0663 | Val Loss: 0.0541\n",
      "[Fold 5 Epoch 80] Train Loss: 0.0580 | Val Loss: 0.0551\n",
      "[Fold 5 Epoch 90] Train Loss: 0.0609 | Val Loss: 0.0601\n",
      "[Fold 5 Epoch 100] Train Loss: 0.0621 | Val Loss: 0.0564\n",
      "[Fold 5 Epoch 110] Train Loss: 0.0605 | Val Loss: 0.0541\n",
      "[Fold 5 Epoch 120] Train Loss: 0.0556 | Val Loss: 0.0526\n",
      "[Fold 5 Epoch 130] Train Loss: 0.0612 | Val Loss: 0.0527\n",
      "[Fold 5 Epoch 140] Train Loss: 0.0511 | Val Loss: 0.0509\n",
      "[Fold 5 Epoch 150] Train Loss: 0.0579 | Val Loss: 0.0510\n",
      "[Fold 5 Epoch 160] Train Loss: 0.0542 | Val Loss: 0.0510\n",
      "[Fold 5 Epoch 170] Train Loss: 0.0517 | Val Loss: 0.0480\n",
      "[Fold 5 Epoch 180] Train Loss: 0.0531 | Val Loss: 0.0444\n",
      "[Fold 5 Epoch 190] Train Loss: 0.0483 | Val Loss: 0.0429\n",
      "[Fold 5 Epoch 200] Train Loss: 0.0428 | Val Loss: 0.0367\n",
      "[Fold 5 Epoch 210] Train Loss: 0.0462 | Val Loss: 0.0336\n",
      "[Fold 5 Epoch 220] Train Loss: 0.0472 | Val Loss: 0.0337\n",
      "[Fold 5 Epoch 230] Train Loss: 0.0447 | Val Loss: 0.0314\n",
      "[Fold 5 Epoch 240] Train Loss: 0.0525 | Val Loss: 0.0290\n",
      "[Fold 5 Epoch 250] Train Loss: 0.0411 | Val Loss: 0.0268\n",
      "[Fold 5 Epoch 260] Train Loss: 0.0358 | Val Loss: 0.0305\n",
      "[Fold 5 Epoch 270] Train Loss: 0.0433 | Val Loss: 0.0291\n",
      "[Fold 5 Epoch 280] Train Loss: 0.0409 | Val Loss: 0.0274\n",
      "[Fold 5 Epoch 290] Train Loss: 0.0450 | Val Loss: 0.0283\n",
      "[Fold 5 Epoch 300] Train Loss: 0.0455 | Val Loss: 0.0297\n",
      "[Fold 5 Epoch 310] Train Loss: 0.0459 | Val Loss: 0.0282\n",
      "[Fold 5 Epoch 320] Train Loss: 0.0416 | Val Loss: 0.0297\n",
      "[Fold 5 Epoch 330] Train Loss: 0.0415 | Val Loss: 0.0296\n",
      "[Fold 5 Epoch 340] Train Loss: 0.0413 | Val Loss: 0.0321\n",
      "[Fold 5 Epoch 350] Train Loss: 0.0365 | Val Loss: 0.0302\n",
      "[Fold 5 Epoch 360] Train Loss: 0.0360 | Val Loss: 0.0299\n",
      "[Fold 5 Epoch 370] Train Loss: 0.0407 | Val Loss: 0.0294\n",
      "[Fold 5 Epoch 380] Train Loss: 0.0397 | Val Loss: 0.0305\n",
      "[Fold 5 Epoch 390] Train Loss: 0.0496 | Val Loss: 0.0400\n",
      "[Fold 5 Epoch 400] Train Loss: 0.0533 | Val Loss: 0.0405\n",
      "[Fold 5 Epoch 410] Train Loss: 0.0393 | Val Loss: 0.0361\n",
      "[Fold 5 Epoch 420] Train Loss: 0.0442 | Val Loss: 0.0357\n",
      "[Fold 5 Epoch 430] Train Loss: 0.0495 | Val Loss: 0.0429\n",
      "[Fold 5 Epoch 440] Train Loss: 0.0369 | Val Loss: 0.0335\n",
      "[Fold 5 Epoch 450] Train Loss: 0.0384 | Val Loss: 0.0310\n",
      "[Fold 5 Epoch 460] Train Loss: 0.0345 | Val Loss: 0.0316\n",
      "[Fold 5 Epoch 470] Train Loss: 0.0421 | Val Loss: 0.0430\n",
      "[Fold 5 Epoch 480] Train Loss: 0.0472 | Val Loss: 0.0322\n",
      "[Fold 5 Epoch 490] Train Loss: 0.0490 | Val Loss: 0.0292\n",
      "[Fold 5 Epoch 500] Train Loss: 0.0367 | Val Loss: 0.0348\n",
      "Evaluating fold 5 ...\n",
      "R²: 0.4984\n",
      "RMSE: 0.1864\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "Fold 1: R² = 0.4278, RMSE = 0.1793\n",
      "Fold 2: R² = 0.5202, RMSE = 0.2238\n",
      "Fold 3: R² = 0.4507, RMSE = 0.1958\n",
      "Fold 4: R² = 0.3507, RMSE = 0.2414\n",
      "Fold 5: R² = 0.4984, RMSE = 0.1864\n",
      "\n",
      "Average R²: 0.4496 ± 0.0594\n",
      "Average RMSE: 0.2054 ± 0.0235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-21 11:16:27,131] A new study created in RDB with name: GNN-mixed model different layers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c306154891dc45ff9b1ed2244d6a6fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: -0.0015\n",
      "RMSE: 0.2372\n",
      "R²: -0.0043\n",
      "RMSE: 0.3238\n",
      "R²: 0.0001\n",
      "RMSE: 0.2642\n",
      "R²: -0.0072\n",
      "RMSE: 0.3007\n",
      "R²: -0.0141\n",
      "RMSE: 0.2651\n",
      "[I 2025-03-21 11:20:58,381] Trial 0 finished with value: 0.2782137355765075 and parameters: {'learning_rate': 0.003, 'dropout': 0.5, 'num_layers': 6, 'hidden_dim_0': 16, 'hidden_dim_1': 16, 'hidden_dim_2': 16, 'hidden_dim_3': 16, 'hidden_dim_4': 16, 'hidden_dim_5': 16}. Best is trial 0 with value: 0.2782137355765075.\n",
      "[W 2025-03-21 11:20:58,477] Trial 1 failed with parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 5, 'hidden_dim_0': 32} because of the following error: ValueError('CategoricalDistribution does not support dynamic value space.').\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\kang_\\AppData\\Local\\Temp\\ipykernel_53228\\2263250001.py\", line 517, in objective\n",
      "    hd = trial.suggest_categorical(f\"hidden_dim_{i}\", allowed_options)\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\trial\\_trial.py\", line 402, in suggest_categorical\n",
      "    return self._suggest(name, CategoricalDistribution(choices=choices))\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\trial\\_trial.py\", line 637, in _suggest\n",
      "    storage.set_trial_param(trial_id, name, param_value_in_internal_repr, distribution)\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\storages\\_cached_storage.py\", line 171, in set_trial_param\n",
      "    self._backend.set_trial_param(trial_id, param_name, param_value_internal, distribution)\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\storages\\_rdb\\storage.py\", line 579, in set_trial_param\n",
      "    self._set_trial_param_without_commit(\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\storages\\_rdb\\storage.py\", line 601, in _set_trial_param_without_commit\n",
      "    trial_param.check_and_add(session, trial.study_id)\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\storages\\_rdb\\models.py\", line 353, in check_and_add\n",
      "    self._check_compatibility_with_previous_trial_param_distributions(session, study_id)\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\storages\\_rdb\\models.py\", line 367, in _check_compatibility_with_previous_trial_param_distributions\n",
      "    distributions.check_distribution_compatibility(\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\distributions.py\", line 669, in check_distribution_compatibility\n",
      "    raise ValueError(\n",
      "ValueError: CategoricalDistribution does not support dynamic value space.\n",
      "[W 2025-03-21 11:20:58,478] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "CategoricalDistribution does not support dynamic value space.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 601\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    594\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m    595\u001b[0m         storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite:///gnn_mix_op08.sqlite3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    596\u001b[0m         study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGNN-mixed model different layers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    597\u001b[0m         direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    598\u001b[0m         load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    599\u001b[0m     )\n\u001b[1;32m--> 601\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m================= Optuna Study Results =================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    604\u001b[0m     best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[3], line 517\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    515\u001b[0m         hd \u001b[38;5;241m=\u001b[39m candidate_values[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 517\u001b[0m         hd \u001b[38;5;241m=\u001b[39m \u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_dim_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m     hidden_dims\u001b[38;5;241m.\u001b[39mappend(hd)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;66;03m# Early stopping parameters\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\trial\\_trial.py:402\u001b[0m, in \u001b[0;36mTrial.suggest_categorical\u001b[1;34m(self, name, choices)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Suggest a value for the categorical parameter.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mThe value is sampled from ``choices``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    :ref:`configurations` tutorial describes more details and flexible usages.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;66;03m# There is no need to call self._check_distribution because\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;66;03m# CategoricalDistribution does not support dynamic value space.\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_suggest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCategoricalDistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\trial\\_trial.py:637\u001b[0m, in \u001b[0;36mTrial._suggest\u001b[1;34m(self, name, distribution)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;66;03m# `param_value` is validated here (invalid value like `np.nan` raises ValueError).\u001b[39;00m\n\u001b[0;32m    636\u001b[0m param_value_in_internal_repr \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mto_internal_repr(param_value)\n\u001b[1;32m--> 637\u001b[0m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_trial_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_value_in_internal_repr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistribution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_frozen_trial\u001b[38;5;241m.\u001b[39mdistributions[name] \u001b[38;5;241m=\u001b[39m distribution\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_frozen_trial\u001b[38;5;241m.\u001b[39mparams[name] \u001b[38;5;241m=\u001b[39m param_value\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\storages\\_cached_storage.py:171\u001b[0m, in \u001b[0;36m_CachedStorage.set_trial_param\u001b[1;34m(self, trial_id, param_name, param_value_internal, distribution)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_trial_param\u001b[39m(\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    166\u001b[0m     trial_id: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m     distribution: distributions\u001b[38;5;241m.\u001b[39mBaseDistribution,\n\u001b[0;32m    170\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_trial_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_value_internal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistribution\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\storages\\_rdb\\storage.py:579\u001b[0m, in \u001b[0;36mRDBStorage.set_trial_param\u001b[1;34m(self, trial_id, param_name, param_value_internal, distribution)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_trial_param\u001b[39m(\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    573\u001b[0m     trial_id: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m     distribution: distributions\u001b[38;5;241m.\u001b[39mBaseDistribution,\n\u001b[0;32m    577\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _create_scoped_session(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoped_session, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m--> 579\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_trial_param_without_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m            \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_value_internal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistribution\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\storages\\_rdb\\storage.py:601\u001b[0m, in \u001b[0;36mRDBStorage._set_trial_param_without_commit\u001b[1;34m(self, session, trial_id, param_name, param_value_internal, distribution)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_trial_is_updatable(trial_id, trial\u001b[38;5;241m.\u001b[39mstate)\n\u001b[0;32m    594\u001b[0m trial_param \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mTrialParamModel(\n\u001b[0;32m    595\u001b[0m     trial_id\u001b[38;5;241m=\u001b[39mtrial_id,\n\u001b[0;32m    596\u001b[0m     param_name\u001b[38;5;241m=\u001b[39mparam_name,\n\u001b[0;32m    597\u001b[0m     param_value\u001b[38;5;241m=\u001b[39mparam_value_internal,\n\u001b[0;32m    598\u001b[0m     distribution_json\u001b[38;5;241m=\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mdistribution_to_json(distribution),\n\u001b[0;32m    599\u001b[0m )\n\u001b[1;32m--> 601\u001b[0m \u001b[43mtrial_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_and_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstudy_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\storages\\_rdb\\models.py:353\u001b[0m, in \u001b[0;36mTrialParamModel.check_and_add\u001b[1;34m(self, session, study_id)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_and_add\u001b[39m(\u001b[38;5;28mself\u001b[39m, session: orm\u001b[38;5;241m.\u001b[39mSession, study_id: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_compatibility_with_previous_trial_param_distributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudy_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m     session\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\storages\\_rdb\\models.py:367\u001b[0m, in \u001b[0;36mTrialParamModel._check_compatibility_with_previous_trial_param_distributions\u001b[1;34m(self, session, study_id)\u001b[0m\n\u001b[0;32m    359\u001b[0m previous_record \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    360\u001b[0m     session\u001b[38;5;241m.\u001b[39mquery(TrialParamModel)\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;241m.\u001b[39mjoin(TrialModel)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;241m.\u001b[39mfirst()\n\u001b[0;32m    365\u001b[0m )\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_record \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 367\u001b[0m     \u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_distribution_compatibility\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson_to_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevious_record\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution_json\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson_to_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution_json\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\distributions.py:669\u001b[0m, in \u001b[0;36mcheck_distribution_compatibility\u001b[1;34m(dist_old, dist_new)\u001b[0m\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dist_old \u001b[38;5;241m!=\u001b[39m dist_new:\n\u001b[1;32m--> 669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    670\u001b[0m         CategoricalDistribution\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not support dynamic value space.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    671\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: CategoricalDistribution does not support dynamic value space."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data, DataLoader as PyGDataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool, BatchNorm\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "#                             DATASET & PREP\n",
    "# =============================================================================\n",
    "\n",
    "class MolDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset that:\n",
    "      - Reads external factors from CSV\n",
    "      - Loads the corresponding pickle for the molecule's graph\n",
    "      - Converts it into a PyG Data object\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 raw_dataframe: pd.DataFrame,\n",
    "                 nx_graph_dict: dict,\n",
    "                 *,\n",
    "                 component_col: str,\n",
    "                 global_state_cols: list[str],\n",
    "                 label_col: str,\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_dataframe: The input dataframe containing molecule info.\n",
    "            nx_graph_dict: Dictionary mapping component names to networkx graphs.\n",
    "            component_col: Column name for the component.\n",
    "            global_state_cols: List of columns representing external factors.\n",
    "            label_col: Column name for the regression target.\n",
    "            transform: Any transform to apply to each PyG Data object.\n",
    "        \"\"\"\n",
    "        self.raw_dataframe = raw_dataframe\n",
    "        self.nx_graph_dict = nx_graph_dict\n",
    "        self.component_col = [component_col] if type(component_col) is str else component_col\n",
    "        self.global_state_cols = global_state_cols\n",
    "        self.label_col = [label_col] if type(label_col) is str else label_col\n",
    "        self.transform = transform\n",
    "        \n",
    "        required_cols = set(self.global_state_cols + self.label_col + self.component_col)\n",
    "        for col in required_cols:\n",
    "            if col not in self.raw_dataframe.columns:\n",
    "                raise ValueError(f\"Missing column in DataFrame: '{col}'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.raw_dataframe.iloc[idx]\n",
    "        \n",
    "        # 1. Load the molecule graph\n",
    "        component_name = row[self.component_col[0]]  # e.g. \"C23\"\n",
    "        pyg_data = self.nx_graph_dict[component_name]\n",
    "\n",
    "        # 2. Prepare the external factors\n",
    "        externals = torch.tensor(row[self.global_state_cols].values.astype(float), dtype=torch.float)\n",
    "        externals = externals.unsqueeze(0)\n",
    "\n",
    "        # 3. Prepare the label (regression target)\n",
    "        label = torch.tensor([row[self.label_col][0]], dtype=torch.float)\n",
    "\n",
    "        # 4. Attach externals & label to the Data object\n",
    "        pyg_data.externals = externals  # shape [1, external_in_dim]\n",
    "        pyg_data.y = label  # shape [1]\n",
    "\n",
    "        if self.transform:\n",
    "            pyg_data = self.transform(pyg_data)\n",
    "\n",
    "        return pyg_data\n",
    "\n",
    "\n",
    "def networkx_to_pyg(nx_graph):\n",
    "    \"\"\"\n",
    "    Convert a networkx graph to a torch_geometric.data.Data object.\n",
    "    This is a basic template; adjust for your actual node/edge features.\n",
    "    \"\"\"\n",
    "    # Sort nodes to ensure consistent ordering\n",
    "    node_mapping = {node: i for i, node in enumerate(nx_graph.nodes())}\n",
    "\n",
    "    x_list = []\n",
    "    edge_index_list = []\n",
    "    edge_attr_list = []\n",
    "\n",
    "    # Node features\n",
    "    for node in nx_graph.nodes(data=True):\n",
    "        original_id = node[0]\n",
    "        attrs = node[1]\n",
    "        symbol = attrs.get(\"symbol\", \"C\")\n",
    "        symbol_id = 0 if symbol == \"C\" else 1 if symbol == \"H\" else 2\n",
    "        x_list.append([symbol_id])\n",
    "\n",
    "    # Edge features\n",
    "    for u, v, edge_attrs in nx_graph.edges(data=True):\n",
    "        u_idx = node_mapping[u]\n",
    "        v_idx = node_mapping[v]\n",
    "        edge_index_list.append((u_idx, v_idx))\n",
    "        bde_pred = edge_attrs.get(\"bde_pred\", 0.0) or 0.0\n",
    "        bdfe_pred = edge_attrs.get(\"bdfe_pred\", 0.0) or 0.0\n",
    "        edge_attr_list.append([bde_pred, bdfe_pred])\n",
    "\n",
    "    x = torch.tensor(x_list, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                     BASE GNN MODEL (WITH DIM MATCH)\n",
    "# =============================================================================\n",
    "\n",
    "class GINE_Regression(nn.Module):\n",
    "    \"\"\"\n",
    "    A GNN for regression using GINEConv layers + edge attributes,\n",
    "    where all layers have the same hidden_dim (no dimension mismatch).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dim: int = 128,\n",
    "                 num_layers: int = 3,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encode edges from edge_in_dim to hidden_dim\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Encode nodes from node_in_dim to hidden_dim\n",
    "        self.node_encoder = nn.Linear(node_in_dim, hidden_dim)\n",
    "        \n",
    "        # Multiple GINEConv layers & corresponding BatchNorm\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Process external factors\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Encode\n",
    "        x = self.node_encoder(x)\n",
    "        edge_emb = self.edge_encoder(edge_attr)\n",
    "        \n",
    "        # Pass through GINEConv layers\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Global pooling\n",
    "        graph_emb = global_mean_pool(x, batch)\n",
    "\n",
    "        # Process external factors\n",
    "        ext_emb = self.externals_mlp(data.externals)\n",
    "\n",
    "        # Combine & regress\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)\n",
    "        out = self.final_regressor(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                   TRAIN/VALID/EVALUATION UTILS\n",
    "# =============================================================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_data)\n",
    "        y = batch_data.y.to(device).view(-1)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "        count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            preds = model(batch_data)\n",
    "            y = batch_data.y.to(device).view(-1)\n",
    "            loss = criterion(preds, y)\n",
    "            total_loss += loss.item() * batch_data.num_graphs\n",
    "            count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset loader and compute R² and RMSE.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            preds = model(batch)\n",
    "            y_true.append(batch.y.cpu())\n",
    "            y_pred.append(preds.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true).numpy().squeeze()\n",
    "    y_pred = torch.cat(y_pred).numpy().squeeze()\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return r2, rmse\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                             DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "env_file = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\dataset02.xlsx\"\n",
    "data = pd.read_excel(env_file, engine='openpyxl').dropna(subset=['degradation_rate'])\n",
    "data['seawater'] = data['seawater'].map({'art': 1, 'sea': 0})\n",
    "\n",
    "folder_path = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\"\n",
    "graph_pickles = [f for f in os.listdir(folder_path) if f.endswith(\".pkl\")]\n",
    "\n",
    "base_dir = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\"\n",
    "if os.path.exists(base_dir):\n",
    "    print(\"Directory exists:\", base_dir)\n",
    "    print(\"Files in directory:\", os.listdir(base_dir))\n",
    "else:\n",
    "    print(f\"Error: Directory {base_dir} does not exist!\")\n",
    "\n",
    "compounds = data.component.unique()\n",
    "graphs_dict = {}\n",
    "for compound, graph_pickle in zip(compounds, graph_pickles):\n",
    "    with open(os.path.join(base_dir, graph_pickle), 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "        graphs_dict[compound] = networkx_to_pyg(graph)\n",
    "\n",
    "dataset = MolDataset(\n",
    "    raw_dataframe=data,\n",
    "    nx_graph_dict=graphs_dict,\n",
    "    component_col=\"component\",\n",
    "    global_state_cols=[\"temperature\", \"concentration\", \"time\", \"seawater\"],\n",
    "    label_col=\"degradation_rate\",\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "#                      CROSS-VALIDATION (FIXED MODEL)\n",
    "# =============================================================================\n",
    "from torch_geometric.data import DataLoader as PyGDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 5  # number of folds\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"\\n--- Fold {fold + 1} ---\")\n",
    "\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "    val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = GINE_Regression(\n",
    "        node_in_dim=1,\n",
    "        edge_in_dim=2,\n",
    "        external_in_dim=4,\n",
    "        hidden_dim=16,  # Example hidden_dim\n",
    "        num_layers=5,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    num_epochs = 500\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[Fold {fold + 1} Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    print(f\"Evaluating fold {fold + 1} ...\")\n",
    "    r2, rmse = evaluate_model(model, val_loader, device)\n",
    "    fold_results.append({\"fold\": fold + 1, \"r2\": r2, \"rmse\": rmse})\n",
    "\n",
    "r2_scores = [res[\"r2\"] for res in fold_results]\n",
    "rmse_scores = [res[\"rmse\"] for res in fold_results]\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "for res in fold_results:\n",
    "    print(f\"Fold {res['fold']}: R² = {res['r2']:.4f}, RMSE = {res['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#             IMPROVED MODEL WITH TRAPEZOID DIMENSIONS & PROJECTIONS\n",
    "# =============================================================================\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "\n",
    "def build_trapezoid_dims(num_layers):\n",
    "    \"\"\"\n",
    "    Build a list of hidden dimensions in a trapezoid manner:\n",
    "    up to the first 4 layers: [128, 64, 32, 16]\n",
    "    if num_layers > 4, extend with 16 for extra layers.\n",
    "    \"\"\"\n",
    "    base_dims = [128, 64, 32, 16]\n",
    "    if num_layers > 4:\n",
    "        extra_layers = num_layers - 4\n",
    "        return base_dims + [16] * extra_layers\n",
    "    else:\n",
    "        return base_dims[:num_layers]\n",
    "\n",
    "\n",
    "class GINE_RegressionTrapezoid(nn.Module):\n",
    "    \"\"\"\n",
    "    A GINEConv-based regression model that uses a list of hidden dimensions\n",
    "    to build layers with decreasing size (trapezoid architecture),\n",
    "    ensuring dimension consistency with projection layers between convs.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dims: list,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # For the first layer, encode edges to hidden_dims[0], and encode nodes as well\n",
    "        self.initial_edge_encoder = nn.Linear(edge_in_dim, hidden_dims[0])\n",
    "        self.initial_node_encoder = nn.Linear(node_in_dim, hidden_dims[0])\n",
    "\n",
    "        # We'll build each GINEConv to transform dimension: hidden_dims[i] -> hidden_dims[i].\n",
    "        # After each conv i, if i < len(hidden_dims)-1, we project to hidden_dims[i+1].\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.projections = nn.ModuleList()  # for node features\n",
    "        self.edge_projections = nn.ModuleList()  # for edge features\n",
    "\n",
    "        for i in range(len(hidden_dims)):\n",
    "            # GINEConv's internal MLP\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i])\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dims[i]))\n",
    "\n",
    "            # If there's a next layer, we need a projection from hidden_dims[i] -> hidden_dims[i+1]\n",
    "            if i < len(hidden_dims) - 1:\n",
    "                self.projections.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "                self.edge_projections.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            else:\n",
    "                # No projection needed for the last layer\n",
    "                self.projections.append(None)\n",
    "                self.edge_projections.append(None)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "\n",
    "        # We'll process external factors to the final dimension\n",
    "        final_dim = hidden_dims[-1]\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, final_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(final_dim, final_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression: combine final node features + final external features\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(final_dim + final_dim, final_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(final_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # 1) Encode node/edge to hidden_dims[0]\n",
    "        x = self.initial_node_encoder(x)\n",
    "        edge_emb = self.initial_edge_encoder(edge_attr)\n",
    "\n",
    "        # 2) Pass through each GINEConv\n",
    "        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout_layer(x)\n",
    "\n",
    "            # If there's a next layer, project node features and edge_emb to hidden_dims[i+1]\n",
    "            if i < len(self.projections) - 1 and self.projections[i] is not None:\n",
    "                x = self.projections[i](x)\n",
    "                edge_emb = self.edge_projections[i](edge_emb)\n",
    "\n",
    "        # 3) Global mean pooling\n",
    "        graph_emb = global_mean_pool(x, batch)\n",
    "\n",
    "        # 4) Process external factors into final dimension\n",
    "        ext_emb = self.externals_mlp(data.externals)\n",
    "\n",
    "        # 5) Concatenate and final regression\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)\n",
    "        out = self.final_regressor(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                          OPTUNA OBJECTIVE FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna.\n",
    "    We do k-fold cross validation using a new GNN model with a non-increasing\n",
    "    (trapezoidal) architecture. The hyperparameters include learning rate, dropout,\n",
    "    number of layers, and for each layer, a hidden dimension chosen from a fixed\n",
    "    candidate set so that each subsequent layer's dimension is <= the previous layer's.\n",
    "    We return the average RMSE (lower = better). R² is stored in user_attrs.\n",
    "    Intermediate validation losses are reported for early stopping & dashboard visualization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameter search space\n",
    "    lr = trial.suggest_categorical(\"learning_rate\", [1e-3, 3e-3, 1e-4, 3e-4])\n",
    "    dropout = trial.suggest_categorical(\"dropout\", [0.1, 0.5])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "\n",
    "    # Fixed candidate set of hidden dimensions\n",
    "    candidate_values = [256, 128, 64, 32, 16]\n",
    "\n",
    "    # Build a non-increasing list of hidden dimensions\n",
    "    hidden_dims = []\n",
    "    # First layer can be any value from candidate_values\n",
    "    hd0 = trial.suggest_categorical(\"hidden_dim_0\", candidate_values)\n",
    "    hidden_dims.append(hd0)\n",
    "\n",
    "    # For each subsequent layer, allow only values <= the previous layer's dimension\n",
    "    for i in range(1, num_layers):\n",
    "        allowed_options = [val for val in candidate_values if val <= hidden_dims[i-1]]\n",
    "        # If for some reason no options are valid (unlikely with [16] in the list),\n",
    "        # default to the smallest dimension\n",
    "        if len(allowed_options) == 0:\n",
    "            hd = candidate_values[-1]\n",
    "        else:\n",
    "            hd = trial.suggest_categorical(f\"hidden_dim_{i}\", allowed_options)\n",
    "        hidden_dims.append(hd)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    max_epochs = 500\n",
    "    patience = 10\n",
    "    min_delta = 1e-5\n",
    "\n",
    "    # Prepare 5-fold CV\n",
    "    kf_local = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf_local.split(dataset)):\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "        val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "        # Build the GNN model with the chosen trapezoidal hidden dimensions\n",
    "        model = GINE_RegressionTrapezoid(\n",
    "            node_in_dim=1,\n",
    "            edge_in_dim=2,\n",
    "            external_in_dim=4,\n",
    "            hidden_dims=hidden_dims,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Early-stopping tracking\n",
    "        best_val_loss = float(\"inf\")\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            # Train + Validate\n",
    "            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "            val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "            # Report intermediate metric to Optuna for pruning & dashboard visualization\n",
    "            trial.report(val_loss, step=epoch)\n",
    "\n",
    "            # Check if Optuna wants to prune the trial early\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "            # Custom Early Stopping\n",
    "            if val_loss < (best_val_loss - min_delta):\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    break\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        r2_fold, rmse_fold = evaluate_model(model, val_loader, device)\n",
    "        rmse_scores.append(rmse_fold)\n",
    "        r2_scores.append(r2_fold)\n",
    "\n",
    "    # Average metrics over folds\n",
    "    avg_rmse = float(np.mean(rmse_scores))\n",
    "    avg_r2 = float(np.mean(r2_scores))\n",
    "\n",
    "    # Log the average R² so it appears in the dashboard's user attributes\n",
    "    trial.set_user_attr(\"avg_r2\", avg_r2)\n",
    "\n",
    "    # Return RMSE (the main metric to minimize)\n",
    "    return avg_rmse\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                           OPTUNA STUDY & DASHBOARD\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(\n",
    "        storage=\"sqlite:///gnn_mix_op08.sqlite3\",\n",
    "        study_name=\"GNN-mixed model different layers\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "    print(\"\\n================= Optuna Study Results =================\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Best Trial Value (RMSE): {best_trial.value}\")\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for key, val in best_trial.params.items():\n",
    "        print(f\"  {key}: {val}\")\n",
    "    print(f\"User Attrs (R², etc.): {best_trial.user_attrs}\")\n",
    "\n",
    "    try:\n",
    "        fig1 = vis.plot_optimization_history(study)\n",
    "        fig1.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate optimization history plot: {e}\")\n",
    "\n",
    "    try:\n",
    "        fig2 = vis.plot_param_importances(study)\n",
    "        fig2.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate hyperparameter importance plot: {e}\")\n",
    "\n",
    "    try:\n",
    "        fig3 = vis.plot_intermediate_values(study)\n",
    "        fig3.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate intermediate values plot: {e}\")\n",
    "\n",
    "    print(\"\\n================= End of Optuna Tuning =================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfabet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
