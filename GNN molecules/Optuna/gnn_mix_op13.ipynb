{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\n",
      "Files in directory: ['gpickle_graph_0.pkl', 'gpickle_graph_1.pkl', 'gpickle_graph_10.pkl', 'gpickle_graph_11.pkl', 'gpickle_graph_12.pkl', 'gpickle_graph_13.pkl', 'gpickle_graph_14.pkl', 'gpickle_graph_15.pkl', 'gpickle_graph_16.pkl', 'gpickle_graph_17.pkl', 'gpickle_graph_18.pkl', 'gpickle_graph_19.pkl', 'gpickle_graph_2.pkl', 'gpickle_graph_3.pkl', 'gpickle_graph_4.pkl', 'gpickle_graph_5.pkl', 'gpickle_graph_6.pkl', 'gpickle_graph_7.pkl', 'gpickle_graph_8.pkl', 'gpickle_graph_9.pkl']\n",
      "\n",
      "--- Fold 1 ---\n",
      "[Fold 1 Epoch 10] Train Loss: 0.1144 | Val Loss: 0.0663\n",
      "[Fold 1 Epoch 20] Train Loss: 0.0885 | Val Loss: 0.0509\n",
      "[Fold 1 Epoch 30] Train Loss: 0.0759 | Val Loss: 0.0628\n",
      "[Fold 1 Epoch 40] Train Loss: 0.0697 | Val Loss: 0.0517\n",
      "[Fold 1 Epoch 50] Train Loss: 0.0603 | Val Loss: 0.0473\n",
      "[Fold 1 Epoch 60] Train Loss: 0.0607 | Val Loss: 0.0463\n",
      "[Fold 1 Epoch 70] Train Loss: 0.0624 | Val Loss: 0.0440\n",
      "[Fold 1 Epoch 80] Train Loss: 0.0533 | Val Loss: 0.0444\n",
      "[Fold 1 Epoch 90] Train Loss: 0.0532 | Val Loss: 0.0480\n",
      "[Fold 1 Epoch 100] Train Loss: 0.0538 | Val Loss: 0.0438\n",
      "[Fold 1 Epoch 110] Train Loss: 0.0658 | Val Loss: 0.0433\n",
      "[Fold 1 Epoch 120] Train Loss: 0.0497 | Val Loss: 0.0436\n",
      "[Fold 1 Epoch 130] Train Loss: 0.0596 | Val Loss: 0.0424\n",
      "[Fold 1 Epoch 140] Train Loss: 0.0487 | Val Loss: 0.0420\n",
      "[Fold 1 Epoch 150] Train Loss: 0.0537 | Val Loss: 0.0406\n",
      "[Fold 1 Epoch 160] Train Loss: 0.0539 | Val Loss: 0.0402\n",
      "[Fold 1 Epoch 170] Train Loss: 0.0612 | Val Loss: 0.0403\n",
      "[Fold 1 Epoch 180] Train Loss: 0.0554 | Val Loss: 0.0385\n",
      "[Fold 1 Epoch 190] Train Loss: 0.0502 | Val Loss: 0.0401\n",
      "[Fold 1 Epoch 200] Train Loss: 0.0492 | Val Loss: 0.0392\n",
      "[Fold 1 Epoch 210] Train Loss: 0.0445 | Val Loss: 0.0370\n",
      "[Fold 1 Epoch 220] Train Loss: 0.0526 | Val Loss: 0.0369\n",
      "[Fold 1 Epoch 230] Train Loss: 0.0479 | Val Loss: 0.0349\n",
      "[Fold 1 Epoch 240] Train Loss: 0.0435 | Val Loss: 0.0335\n",
      "[Fold 1 Epoch 250] Train Loss: 0.0398 | Val Loss: 0.0336\n",
      "[Fold 1 Epoch 260] Train Loss: 0.0381 | Val Loss: 0.0342\n",
      "[Fold 1 Epoch 270] Train Loss: 0.0381 | Val Loss: 0.0319\n",
      "[Fold 1 Epoch 280] Train Loss: 0.0453 | Val Loss: 0.0327\n",
      "[Fold 1 Epoch 290] Train Loss: 0.0340 | Val Loss: 0.0317\n",
      "[Fold 1 Epoch 300] Train Loss: 0.0397 | Val Loss: 0.0320\n",
      "[Fold 1 Epoch 310] Train Loss: 0.0378 | Val Loss: 0.0316\n",
      "[Fold 1 Epoch 320] Train Loss: 0.0582 | Val Loss: 0.0384\n",
      "[Fold 1 Epoch 330] Train Loss: 0.0413 | Val Loss: 0.0348\n",
      "[Fold 1 Epoch 340] Train Loss: 0.0400 | Val Loss: 0.0354\n",
      "[Fold 1 Epoch 350] Train Loss: 0.0427 | Val Loss: 0.0475\n",
      "[Fold 1 Epoch 360] Train Loss: 0.0407 | Val Loss: 0.0352\n",
      "[Fold 1 Epoch 370] Train Loss: 0.0360 | Val Loss: 0.0346\n",
      "[Fold 1 Epoch 380] Train Loss: 0.0356 | Val Loss: 0.0351\n",
      "[Fold 1 Epoch 390] Train Loss: 0.0354 | Val Loss: 0.0361\n",
      "[Fold 1 Epoch 400] Train Loss: 0.0410 | Val Loss: 0.0381\n",
      "[Fold 1 Epoch 410] Train Loss: 0.0322 | Val Loss: 0.0326\n",
      "[Fold 1 Epoch 420] Train Loss: 0.0364 | Val Loss: 0.0430\n",
      "[Fold 1 Epoch 430] Train Loss: 0.0417 | Val Loss: 0.0377\n",
      "[Fold 1 Epoch 440] Train Loss: 0.0351 | Val Loss: 0.0340\n",
      "[Fold 1 Epoch 450] Train Loss: 0.0410 | Val Loss: 0.0342\n",
      "[Fold 1 Epoch 460] Train Loss: 0.0390 | Val Loss: 0.0329\n",
      "[Fold 1 Epoch 470] Train Loss: 0.0389 | Val Loss: 0.0333\n",
      "[Fold 1 Epoch 480] Train Loss: 0.0337 | Val Loss: 0.0354\n",
      "[Fold 1 Epoch 490] Train Loss: 0.0317 | Val Loss: 0.0330\n",
      "[Fold 1 Epoch 500] Train Loss: 0.0418 | Val Loss: 0.0338\n",
      "Evaluating fold 1 ...\n",
      "R²: 0.3987\n",
      "RMSE: 0.1838\n",
      "\n",
      "--- Fold 2 ---\n",
      "[Fold 2 Epoch 10] Train Loss: 0.3286 | Val Loss: 0.1408\n",
      "[Fold 2 Epoch 20] Train Loss: 0.1254 | Val Loss: 0.0966\n",
      "[Fold 2 Epoch 30] Train Loss: 0.0910 | Val Loss: 0.1128\n",
      "[Fold 2 Epoch 40] Train Loss: 0.0870 | Val Loss: 0.1036\n",
      "[Fold 2 Epoch 50] Train Loss: 0.0729 | Val Loss: 0.1089\n",
      "[Fold 2 Epoch 60] Train Loss: 0.0657 | Val Loss: 0.0982\n",
      "[Fold 2 Epoch 70] Train Loss: 0.0664 | Val Loss: 0.0801\n",
      "[Fold 2 Epoch 80] Train Loss: 0.0686 | Val Loss: 0.0891\n",
      "[Fold 2 Epoch 90] Train Loss: 0.0684 | Val Loss: 0.0830\n",
      "[Fold 2 Epoch 100] Train Loss: 0.0671 | Val Loss: 0.1076\n",
      "[Fold 2 Epoch 110] Train Loss: 0.0669 | Val Loss: 0.1079\n",
      "[Fold 2 Epoch 120] Train Loss: 0.0546 | Val Loss: 0.0896\n",
      "[Fold 2 Epoch 130] Train Loss: 0.0574 | Val Loss: 0.0793\n",
      "[Fold 2 Epoch 140] Train Loss: 0.0582 | Val Loss: 0.0788\n",
      "[Fold 2 Epoch 150] Train Loss: 0.0519 | Val Loss: 0.0760\n",
      "[Fold 2 Epoch 160] Train Loss: 0.0578 | Val Loss: 0.0893\n",
      "[Fold 2 Epoch 170] Train Loss: 0.0607 | Val Loss: 0.0824\n",
      "[Fold 2 Epoch 180] Train Loss: 0.0458 | Val Loss: 0.0743\n",
      "[Fold 2 Epoch 190] Train Loss: 0.0504 | Val Loss: 0.0855\n",
      "[Fold 2 Epoch 200] Train Loss: 0.0493 | Val Loss: 0.0876\n",
      "[Fold 2 Epoch 210] Train Loss: 0.0523 | Val Loss: 0.0682\n",
      "[Fold 2 Epoch 220] Train Loss: 0.0455 | Val Loss: 0.0774\n",
      "[Fold 2 Epoch 230] Train Loss: 0.0529 | Val Loss: 0.0807\n",
      "[Fold 2 Epoch 240] Train Loss: 0.0472 | Val Loss: 0.0805\n",
      "[Fold 2 Epoch 250] Train Loss: 0.0640 | Val Loss: 0.0714\n",
      "[Fold 2 Epoch 260] Train Loss: 0.0503 | Val Loss: 0.0684\n",
      "[Fold 2 Epoch 270] Train Loss: 0.0492 | Val Loss: 0.0643\n",
      "[Fold 2 Epoch 280] Train Loss: 0.0508 | Val Loss: 0.0677\n",
      "[Fold 2 Epoch 290] Train Loss: 0.0404 | Val Loss: 0.0697\n",
      "[Fold 2 Epoch 300] Train Loss: 0.0444 | Val Loss: 0.0669\n",
      "[Fold 2 Epoch 310] Train Loss: 0.0425 | Val Loss: 0.0649\n",
      "[Fold 2 Epoch 320] Train Loss: 0.0486 | Val Loss: 0.0724\n",
      "[Fold 2 Epoch 330] Train Loss: 0.0422 | Val Loss: 0.0869\n",
      "[Fold 2 Epoch 340] Train Loss: 0.0396 | Val Loss: 0.0602\n",
      "[Fold 2 Epoch 350] Train Loss: 0.0395 | Val Loss: 0.0544\n",
      "[Fold 2 Epoch 360] Train Loss: 0.0356 | Val Loss: 0.0580\n",
      "[Fold 2 Epoch 370] Train Loss: 0.0396 | Val Loss: 0.0574\n",
      "[Fold 2 Epoch 380] Train Loss: 0.0465 | Val Loss: 0.0573\n",
      "[Fold 2 Epoch 390] Train Loss: 0.0383 | Val Loss: 0.0556\n",
      "[Fold 2 Epoch 400] Train Loss: 0.0392 | Val Loss: 0.0525\n",
      "[Fold 2 Epoch 410] Train Loss: 0.0357 | Val Loss: 0.0554\n",
      "[Fold 2 Epoch 420] Train Loss: 0.0379 | Val Loss: 0.0547\n",
      "[Fold 2 Epoch 430] Train Loss: 0.0463 | Val Loss: 0.0585\n",
      "[Fold 2 Epoch 440] Train Loss: 0.0391 | Val Loss: 0.0579\n",
      "[Fold 2 Epoch 450] Train Loss: 0.0421 | Val Loss: 0.0566\n",
      "[Fold 2 Epoch 460] Train Loss: 0.0387 | Val Loss: 0.0535\n",
      "[Fold 2 Epoch 470] Train Loss: 0.0385 | Val Loss: 0.0570\n",
      "[Fold 2 Epoch 480] Train Loss: 0.0412 | Val Loss: 0.0620\n",
      "[Fold 2 Epoch 490] Train Loss: 0.0350 | Val Loss: 0.0579\n",
      "[Fold 2 Epoch 500] Train Loss: 0.0422 | Val Loss: 0.0543\n",
      "Evaluating fold 2 ...\n",
      "R²: 0.4796\n",
      "RMSE: 0.2331\n",
      "\n",
      "--- Fold 3 ---\n",
      "[Fold 3 Epoch 10] Train Loss: 0.1418 | Val Loss: 0.0759\n",
      "[Fold 3 Epoch 20] Train Loss: 0.0970 | Val Loss: 0.0706\n",
      "[Fold 3 Epoch 30] Train Loss: 0.0903 | Val Loss: 0.0843\n",
      "[Fold 3 Epoch 40] Train Loss: 0.0865 | Val Loss: 0.0665\n",
      "[Fold 3 Epoch 50] Train Loss: 0.0958 | Val Loss: 0.0667\n",
      "[Fold 3 Epoch 60] Train Loss: 0.0867 | Val Loss: 0.0599\n",
      "[Fold 3 Epoch 70] Train Loss: 0.0824 | Val Loss: 0.0646\n",
      "[Fold 3 Epoch 80] Train Loss: 0.0759 | Val Loss: 0.0711\n",
      "[Fold 3 Epoch 90] Train Loss: 0.0651 | Val Loss: 0.0633\n",
      "[Fold 3 Epoch 100] Train Loss: 0.0626 | Val Loss: 0.0588\n",
      "[Fold 3 Epoch 110] Train Loss: 0.0690 | Val Loss: 0.0518\n",
      "[Fold 3 Epoch 120] Train Loss: 0.0636 | Val Loss: 0.0717\n",
      "[Fold 3 Epoch 130] Train Loss: 0.0654 | Val Loss: 0.0622\n",
      "[Fold 3 Epoch 140] Train Loss: 0.0590 | Val Loss: 0.0548\n",
      "[Fold 3 Epoch 150] Train Loss: 0.0556 | Val Loss: 0.0548\n",
      "[Fold 3 Epoch 160] Train Loss: 0.0545 | Val Loss: 0.0493\n",
      "[Fold 3 Epoch 170] Train Loss: 0.0577 | Val Loss: 0.0566\n",
      "[Fold 3 Epoch 180] Train Loss: 0.0479 | Val Loss: 0.0531\n",
      "[Fold 3 Epoch 190] Train Loss: 0.0495 | Val Loss: 0.0578\n",
      "[Fold 3 Epoch 200] Train Loss: 0.0529 | Val Loss: 0.0464\n",
      "[Fold 3 Epoch 210] Train Loss: 0.0415 | Val Loss: 0.0377\n",
      "[Fold 3 Epoch 220] Train Loss: 0.0481 | Val Loss: 0.0394\n",
      "[Fold 3 Epoch 230] Train Loss: 0.0457 | Val Loss: 0.0431\n",
      "[Fold 3 Epoch 240] Train Loss: 0.0473 | Val Loss: 0.0433\n",
      "[Fold 3 Epoch 250] Train Loss: 0.0448 | Val Loss: 0.0444\n",
      "[Fold 3 Epoch 260] Train Loss: 0.0425 | Val Loss: 0.0332\n",
      "[Fold 3 Epoch 270] Train Loss: 0.0443 | Val Loss: 0.0329\n",
      "[Fold 3 Epoch 280] Train Loss: 0.0443 | Val Loss: 0.0450\n",
      "[Fold 3 Epoch 290] Train Loss: 0.0432 | Val Loss: 0.0459\n",
      "[Fold 3 Epoch 300] Train Loss: 0.0386 | Val Loss: 0.0321\n",
      "[Fold 3 Epoch 310] Train Loss: 0.0422 | Val Loss: 0.0324\n",
      "[Fold 3 Epoch 320] Train Loss: 0.0483 | Val Loss: 0.0320\n",
      "[Fold 3 Epoch 330] Train Loss: 0.0446 | Val Loss: 0.0323\n",
      "[Fold 3 Epoch 340] Train Loss: 0.0420 | Val Loss: 0.0330\n",
      "[Fold 3 Epoch 350] Train Loss: 0.0389 | Val Loss: 0.0347\n",
      "[Fold 3 Epoch 360] Train Loss: 0.0362 | Val Loss: 0.0304\n",
      "[Fold 3 Epoch 370] Train Loss: 0.0367 | Val Loss: 0.0346\n",
      "[Fold 3 Epoch 380] Train Loss: 0.0389 | Val Loss: 0.0296\n",
      "[Fold 3 Epoch 390] Train Loss: 0.0330 | Val Loss: 0.0303\n",
      "[Fold 3 Epoch 400] Train Loss: 0.0382 | Val Loss: 0.0285\n",
      "[Fold 3 Epoch 410] Train Loss: 0.0366 | Val Loss: 0.0320\n",
      "[Fold 3 Epoch 420] Train Loss: 0.0363 | Val Loss: 0.0323\n",
      "[Fold 3 Epoch 430] Train Loss: 0.0394 | Val Loss: 0.0288\n",
      "[Fold 3 Epoch 440] Train Loss: 0.0389 | Val Loss: 0.0331\n",
      "[Fold 3 Epoch 450] Train Loss: 0.0377 | Val Loss: 0.0313\n",
      "[Fold 3 Epoch 460] Train Loss: 0.0463 | Val Loss: 0.0416\n",
      "[Fold 3 Epoch 470] Train Loss: 0.0414 | Val Loss: 0.0347\n",
      "[Fold 3 Epoch 480] Train Loss: 0.0423 | Val Loss: 0.0331\n",
      "[Fold 3 Epoch 490] Train Loss: 0.0411 | Val Loss: 0.0303\n",
      "[Fold 3 Epoch 500] Train Loss: 0.0405 | Val Loss: 0.0306\n",
      "Evaluating fold 3 ...\n",
      "R²: 0.5618\n",
      "RMSE: 0.1749\n",
      "\n",
      "--- Fold 4 ---\n",
      "[Fold 4 Epoch 10] Train Loss: 0.1118 | Val Loss: 0.1021\n",
      "[Fold 4 Epoch 20] Train Loss: 0.0785 | Val Loss: 0.1007\n",
      "[Fold 4 Epoch 30] Train Loss: 0.0693 | Val Loss: 0.1084\n",
      "[Fold 4 Epoch 40] Train Loss: 0.0675 | Val Loss: 0.0801\n",
      "[Fold 4 Epoch 50] Train Loss: 0.0686 | Val Loss: 0.0797\n",
      "[Fold 4 Epoch 60] Train Loss: 0.0691 | Val Loss: 0.0799\n",
      "[Fold 4 Epoch 70] Train Loss: 0.0571 | Val Loss: 0.0843\n",
      "[Fold 4 Epoch 80] Train Loss: 0.0569 | Val Loss: 0.0798\n",
      "[Fold 4 Epoch 90] Train Loss: 0.0491 | Val Loss: 0.0749\n",
      "[Fold 4 Epoch 100] Train Loss: 0.0483 | Val Loss: 0.0781\n",
      "[Fold 4 Epoch 110] Train Loss: 0.0464 | Val Loss: 0.0751\n",
      "[Fold 4 Epoch 120] Train Loss: 0.0551 | Val Loss: 0.0713\n",
      "[Fold 4 Epoch 130] Train Loss: 0.0489 | Val Loss: 0.0736\n",
      "[Fold 4 Epoch 140] Train Loss: 0.0529 | Val Loss: 0.0769\n",
      "[Fold 4 Epoch 150] Train Loss: 0.0518 | Val Loss: 0.0710\n",
      "[Fold 4 Epoch 160] Train Loss: 0.0639 | Val Loss: 0.0781\n",
      "[Fold 4 Epoch 170] Train Loss: 0.0519 | Val Loss: 0.0708\n",
      "[Fold 4 Epoch 180] Train Loss: 0.0635 | Val Loss: 0.0736\n",
      "[Fold 4 Epoch 190] Train Loss: 0.0492 | Val Loss: 0.0711\n",
      "[Fold 4 Epoch 200] Train Loss: 0.0589 | Val Loss: 0.0748\n",
      "[Fold 4 Epoch 210] Train Loss: 0.0473 | Val Loss: 0.0720\n",
      "[Fold 4 Epoch 220] Train Loss: 0.0545 | Val Loss: 0.0792\n",
      "[Fold 4 Epoch 230] Train Loss: 0.0531 | Val Loss: 0.0689\n",
      "[Fold 4 Epoch 240] Train Loss: 0.0476 | Val Loss: 0.0680\n",
      "[Fold 4 Epoch 250] Train Loss: 0.0543 | Val Loss: 0.0883\n",
      "[Fold 4 Epoch 260] Train Loss: 0.0448 | Val Loss: 0.0657\n",
      "[Fold 4 Epoch 270] Train Loss: 0.0450 | Val Loss: 0.0610\n",
      "[Fold 4 Epoch 280] Train Loss: 0.0519 | Val Loss: 0.0689\n",
      "[Fold 4 Epoch 290] Train Loss: 0.0419 | Val Loss: 0.0612\n",
      "[Fold 4 Epoch 300] Train Loss: 0.0380 | Val Loss: 0.0571\n",
      "[Fold 4 Epoch 310] Train Loss: 0.0441 | Val Loss: 0.0647\n",
      "[Fold 4 Epoch 320] Train Loss: 0.0375 | Val Loss: 0.0565\n",
      "[Fold 4 Epoch 330] Train Loss: 0.0363 | Val Loss: 0.0609\n",
      "[Fold 4 Epoch 340] Train Loss: 0.0367 | Val Loss: 0.0552\n",
      "[Fold 4 Epoch 350] Train Loss: 0.0350 | Val Loss: 0.0515\n",
      "[Fold 4 Epoch 360] Train Loss: 0.0402 | Val Loss: 0.0656\n",
      "[Fold 4 Epoch 370] Train Loss: 0.0369 | Val Loss: 0.0452\n",
      "[Fold 4 Epoch 380] Train Loss: 0.0395 | Val Loss: 0.0512\n",
      "[Fold 4 Epoch 390] Train Loss: 0.0407 | Val Loss: 0.0484\n",
      "[Fold 4 Epoch 400] Train Loss: 0.0364 | Val Loss: 0.0551\n",
      "[Fold 4 Epoch 410] Train Loss: 0.0403 | Val Loss: 0.0478\n",
      "[Fold 4 Epoch 420] Train Loss: 0.0347 | Val Loss: 0.0552\n",
      "[Fold 4 Epoch 430] Train Loss: 0.0368 | Val Loss: 0.0584\n",
      "[Fold 4 Epoch 440] Train Loss: 0.0331 | Val Loss: 0.0505\n",
      "[Fold 4 Epoch 450] Train Loss: 0.0344 | Val Loss: 0.0526\n",
      "[Fold 4 Epoch 460] Train Loss: 0.0375 | Val Loss: 0.0623\n",
      "[Fold 4 Epoch 470] Train Loss: 0.0417 | Val Loss: 0.0463\n",
      "[Fold 4 Epoch 480] Train Loss: 0.0392 | Val Loss: 0.0460\n",
      "[Fold 4 Epoch 490] Train Loss: 0.0380 | Val Loss: 0.0591\n",
      "[Fold 4 Epoch 500] Train Loss: 0.0377 | Val Loss: 0.0619\n",
      "Evaluating fold 4 ...\n",
      "R²: 0.3102\n",
      "RMSE: 0.2488\n",
      "\n",
      "--- Fold 5 ---\n",
      "[Fold 5 Epoch 10] Train Loss: 0.2473 | Val Loss: 0.0756\n",
      "[Fold 5 Epoch 20] Train Loss: 0.0999 | Val Loss: 0.0648\n",
      "[Fold 5 Epoch 30] Train Loss: 0.0943 | Val Loss: 0.0624\n",
      "[Fold 5 Epoch 40] Train Loss: 0.0702 | Val Loss: 0.0610\n",
      "[Fold 5 Epoch 50] Train Loss: 0.0719 | Val Loss: 0.0608\n",
      "[Fold 5 Epoch 60] Train Loss: 0.0672 | Val Loss: 0.0614\n",
      "[Fold 5 Epoch 70] Train Loss: 0.0720 | Val Loss: 0.0587\n",
      "[Fold 5 Epoch 80] Train Loss: 0.0708 | Val Loss: 0.0580\n",
      "[Fold 5 Epoch 90] Train Loss: 0.0674 | Val Loss: 0.0565\n",
      "[Fold 5 Epoch 100] Train Loss: 0.0657 | Val Loss: 0.0564\n",
      "[Fold 5 Epoch 110] Train Loss: 0.0589 | Val Loss: 0.0547\n",
      "[Fold 5 Epoch 120] Train Loss: 0.0706 | Val Loss: 0.0554\n",
      "[Fold 5 Epoch 130] Train Loss: 0.0634 | Val Loss: 0.0538\n",
      "[Fold 5 Epoch 140] Train Loss: 0.0617 | Val Loss: 0.0549\n",
      "[Fold 5 Epoch 150] Train Loss: 0.0582 | Val Loss: 0.0525\n",
      "[Fold 5 Epoch 160] Train Loss: 0.0598 | Val Loss: 0.0529\n",
      "[Fold 5 Epoch 170] Train Loss: 0.0586 | Val Loss: 0.0525\n",
      "[Fold 5 Epoch 180] Train Loss: 0.0619 | Val Loss: 0.0556\n",
      "[Fold 5 Epoch 190] Train Loss: 0.0592 | Val Loss: 0.0536\n",
      "[Fold 5 Epoch 200] Train Loss: 0.0631 | Val Loss: 0.0529\n",
      "[Fold 5 Epoch 210] Train Loss: 0.0511 | Val Loss: 0.0524\n",
      "[Fold 5 Epoch 220] Train Loss: 0.0569 | Val Loss: 0.0511\n",
      "[Fold 5 Epoch 230] Train Loss: 0.0598 | Val Loss: 0.0486\n",
      "[Fold 5 Epoch 240] Train Loss: 0.0568 | Val Loss: 0.0628\n",
      "[Fold 5 Epoch 250] Train Loss: 0.0520 | Val Loss: 0.0496\n",
      "[Fold 5 Epoch 260] Train Loss: 0.0543 | Val Loss: 0.0487\n",
      "[Fold 5 Epoch 270] Train Loss: 0.0560 | Val Loss: 0.0467\n",
      "[Fold 5 Epoch 280] Train Loss: 0.0541 | Val Loss: 0.0442\n",
      "[Fold 5 Epoch 290] Train Loss: 0.0428 | Val Loss: 0.0443\n",
      "[Fold 5 Epoch 300] Train Loss: 0.0503 | Val Loss: 0.0404\n",
      "[Fold 5 Epoch 310] Train Loss: 0.0463 | Val Loss: 0.0390\n",
      "[Fold 5 Epoch 320] Train Loss: 0.0534 | Val Loss: 0.0389\n",
      "[Fold 5 Epoch 330] Train Loss: 0.0496 | Val Loss: 0.0436\n",
      "[Fold 5 Epoch 340] Train Loss: 0.0411 | Val Loss: 0.0365\n",
      "[Fold 5 Epoch 350] Train Loss: 0.0530 | Val Loss: 0.0394\n",
      "[Fold 5 Epoch 360] Train Loss: 0.0486 | Val Loss: 0.0358\n",
      "[Fold 5 Epoch 370] Train Loss: 0.0459 | Val Loss: 0.0382\n",
      "[Fold 5 Epoch 380] Train Loss: 0.0462 | Val Loss: 0.0366\n",
      "[Fold 5 Epoch 390] Train Loss: 0.0421 | Val Loss: 0.0351\n",
      "[Fold 5 Epoch 400] Train Loss: 0.0377 | Val Loss: 0.0327\n",
      "[Fold 5 Epoch 410] Train Loss: 0.0446 | Val Loss: 0.0372\n",
      "[Fold 5 Epoch 420] Train Loss: 0.0455 | Val Loss: 0.0368\n",
      "[Fold 5 Epoch 430] Train Loss: 0.0489 | Val Loss: 0.0320\n",
      "[Fold 5 Epoch 440] Train Loss: 0.0422 | Val Loss: 0.0310\n",
      "[Fold 5 Epoch 450] Train Loss: 0.0421 | Val Loss: 0.0306\n",
      "[Fold 5 Epoch 460] Train Loss: 0.0383 | Val Loss: 0.0310\n",
      "[Fold 5 Epoch 470] Train Loss: 0.0382 | Val Loss: 0.0329\n",
      "[Fold 5 Epoch 480] Train Loss: 0.0416 | Val Loss: 0.0285\n",
      "[Fold 5 Epoch 490] Train Loss: 0.0439 | Val Loss: 0.0286\n",
      "[Fold 5 Epoch 500] Train Loss: 0.0416 | Val Loss: 0.0297\n",
      "Evaluating fold 5 ...\n",
      "R²: 0.5711\n",
      "RMSE: 0.1724\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "Fold 1: R² = 0.3987, RMSE = 0.1838\n",
      "Fold 2: R² = 0.4796, RMSE = 0.2331\n",
      "Fold 3: R² = 0.5618, RMSE = 0.1749\n",
      "Fold 4: R² = 0.3102, RMSE = 0.2488\n",
      "Fold 5: R² = 0.5711, RMSE = 0.1724\n",
      "\n",
      "Average R²: 0.4643 ± 0.0992\n",
      "Average RMSE: 0.2026 ± 0.0319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-11 16:21:00,051] A new study created in RDB with name: GNN-mixed model 200 with 500 epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499d1231230e4d7eb715bdd0ea21d6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.1133\n",
      "RMSE: 0.2232\n",
      "R²: -0.5560\n",
      "RMSE: 0.4031\n",
      "R²: 0.0751\n",
      "RMSE: 0.2541\n",
      "R²: 0.1627\n",
      "RMSE: 0.2742\n",
      "R²: 0.1931\n",
      "RMSE: 0.2365\n",
      "[I 2025-03-12 09:38:20,442] Trial 0 finished with value: 0.27821025830169965 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 3}. Best is trial 0 with value: 0.27821025830169965.\n",
      "R²: 0.0836\n",
      "RMSE: 0.2269\n",
      "R²: 0.1593\n",
      "RMSE: 0.2963\n",
      "R²: 0.2892\n",
      "RMSE: 0.2228\n",
      "R²: 0.1552\n",
      "RMSE: 0.2754\n",
      "R²: 0.2203\n",
      "RMSE: 0.2325\n",
      "[I 2025-03-12 09:51:46,750] Trial 1 finished with value: 0.2507669411863892 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 4}. Best is trial 1 with value: 0.2507669411863892.\n",
      "R²: -5.6403\n",
      "RMSE: 0.6108\n",
      "R²: -4.4486\n",
      "RMSE: 0.7543\n",
      "R²: -3.9926\n",
      "RMSE: 0.5904\n",
      "R²: -6.3612\n",
      "RMSE: 0.8129\n",
      "R²: -5.1589\n",
      "RMSE: 0.6533\n",
      "[I 2025-03-12 09:54:49,077] Trial 2 finished with value: 0.6843488811659746 and parameters: {'learning_rate': 0.0001, 'dropout': 0.5, 'num_layers': 6}. Best is trial 1 with value: 0.2507669411863892.\n",
      "R²: -10.9130\n",
      "RMSE: 0.8182\n",
      "R²: -8.5286\n",
      "RMSE: 0.9975\n",
      "R²: -8.0873\n",
      "RMSE: 0.7965\n",
      "R²: -4.9534\n",
      "RMSE: 0.7311\n",
      "R²: -8.0297\n",
      "RMSE: 0.7911\n",
      "[I 2025-03-12 09:56:34,296] Trial 3 finished with value: 0.8268588016919904 and parameters: {'learning_rate': 0.0001, 'dropout': 0.5, 'num_layers': 2}. Best is trial 1 with value: 0.2507669411863892.\n",
      "R²: 0.0997\n",
      "RMSE: 0.2249\n",
      "R²: 0.0955\n",
      "RMSE: 0.3073\n",
      "R²: 0.2501\n",
      "RMSE: 0.2288\n",
      "R²: 0.1846\n",
      "RMSE: 0.2706\n",
      "R²: 0.1748\n",
      "RMSE: 0.2391\n",
      "[I 2025-03-12 10:10:22,178] Trial 4 finished with value: 0.25414923938009815 and parameters: {'learning_rate': 0.003, 'dropout': 0.1, 'num_layers': 6}. Best is trial 1 with value: 0.2507669411863892.\n",
      "[I 2025-03-12 10:10:24,589] Trial 5 pruned. \n",
      "[I 2025-03-12 10:10:26,754] Trial 6 pruned. \n",
      "[I 2025-03-12 10:10:29,149] Trial 7 pruned. \n",
      "[I 2025-03-12 10:10:32,904] Trial 8 pruned. \n",
      "[I 2025-03-12 10:10:34,842] Trial 9 pruned. \n",
      "[I 2025-03-12 10:10:36,510] Trial 10 pruned. \n",
      "[I 2025-03-12 10:10:38,686] Trial 11 pruned. \n",
      "[I 2025-03-12 10:10:40,607] Trial 12 pruned. \n",
      "[I 2025-03-12 10:10:55,095] Trial 13 pruned. \n",
      "[I 2025-03-12 10:10:56,776] Trial 14 pruned. \n",
      "[I 2025-03-12 10:10:58,924] Trial 15 pruned. \n",
      "[I 2025-03-12 10:11:00,399] Trial 16 pruned. \n",
      "[I 2025-03-12 10:11:02,109] Trial 17 pruned. \n",
      "[I 2025-03-12 10:11:04,276] Trial 18 pruned. \n",
      "[I 2025-03-12 10:11:06,194] Trial 19 pruned. \n",
      "[I 2025-03-12 10:11:08,745] Trial 20 pruned. \n",
      "[I 2025-03-12 10:11:10,426] Trial 21 pruned. \n",
      "[I 2025-03-12 10:11:28,608] Trial 22 pruned. \n",
      "[I 2025-03-12 10:11:31,855] Trial 23 pruned. \n",
      "[I 2025-03-12 10:11:33,276] Trial 24 pruned. \n",
      "[I 2025-03-12 10:11:35,192] Trial 25 pruned. \n",
      "[I 2025-03-12 10:11:36,827] Trial 26 pruned. \n",
      "[I 2025-03-12 10:11:38,752] Trial 27 pruned. \n",
      "[I 2025-03-12 10:11:40,903] Trial 28 pruned. \n",
      "[I 2025-03-12 10:11:43,288] Trial 29 pruned. \n",
      "\n",
      "================= Optuna Study Results =================\n",
      "Best Trial Value (RMSE): 0.2507669411863892\n",
      "Best Hyperparameters:\n",
      "  learning_rate: 0.003\n",
      "  dropout: 0.1\n",
      "  num_layers: 4\n",
      "User Attrs (R², etc.): {'avg_r2': 0.18149898052215577}\n",
      "Could not generate optimization history plot: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\n",
      "Could not generate hyperparameter importance plot: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\n",
      "Could not generate intermediate values plot: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\n",
      "\n",
      "================= End of Optuna Tuning =================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data, DataLoader as PyGDataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool, BatchNorm\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "#                             DATASET & PREP\n",
    "# =============================================================================\n",
    "\n",
    "class MolDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset that:\n",
    "      - Reads external factors from CSV\n",
    "      - Loads the corresponding pickle for the molecule's graph\n",
    "      - Converts it into a PyG Data object\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 raw_dataframe: pd.DataFrame,\n",
    "                 nx_graph_dict: dict,\n",
    "                 *,\n",
    "                 component_col: str,\n",
    "                 global_state_cols: list[str],\n",
    "                 label_col: str,\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_dataframe: The input dataframe containing molecule info.\n",
    "            nx_graph_dict: Dictionary mapping component names to networkx graphs.\n",
    "            component_col: Column name for the component.\n",
    "            global_state_cols: List of columns representing external factors.\n",
    "            label_col: Column name for the regression target.\n",
    "            transform: Any transform to apply to each PyG Data object.\n",
    "        \"\"\"\n",
    "        self.raw_dataframe = raw_dataframe\n",
    "        self.nx_graph_dict = nx_graph_dict\n",
    "        self.component_col = [component_col] if type(component_col) is str else component_col\n",
    "        self.global_state_cols = global_state_cols\n",
    "        self.label_col = [label_col] if type(label_col) is str else label_col\n",
    "        self.transform = transform\n",
    "        \n",
    "        required_cols = set(self.global_state_cols + self.label_col + self.component_col)\n",
    "        for col in required_cols:\n",
    "            if col not in self.raw_dataframe.columns:\n",
    "                raise ValueError(f\"Missing column in DataFrame: '{col}'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.raw_dataframe.iloc[idx]\n",
    "        \n",
    "        # 1. Load the molecule graph\n",
    "        component_name = row[self.component_col[0]]  # e.g. \"C23\"\n",
    "        pyg_data = self.nx_graph_dict[component_name]\n",
    "\n",
    "        # 2. Prepare the external factors\n",
    "        externals = torch.tensor(row[self.global_state_cols].values.astype(float), dtype=torch.float)\n",
    "        externals = externals.unsqueeze(0)\n",
    "\n",
    "        # 3. Prepare the label (regression target)\n",
    "        label = torch.tensor([row[self.label_col][0]], dtype=torch.float)\n",
    "\n",
    "        # 4. Attach externals & label to the Data object\n",
    "        pyg_data.externals = externals  # shape [1, external_in_dim]\n",
    "        pyg_data.y = label  # shape [1]\n",
    "\n",
    "        if self.transform:\n",
    "            pyg_data = self.transform(pyg_data)\n",
    "\n",
    "        return pyg_data\n",
    "\n",
    "\n",
    "def networkx_to_pyg(nx_graph):\n",
    "    \"\"\"\n",
    "    Convert a networkx graph to a torch_geometric.data.Data object.\n",
    "    This is a basic template; adjust for your actual node/edge features.\n",
    "    \"\"\"\n",
    "    # Sort nodes to ensure consistent ordering\n",
    "    node_mapping = {node: i for i, node in enumerate(nx_graph.nodes())}\n",
    "\n",
    "    x_list = []\n",
    "    edge_index_list = []\n",
    "    edge_attr_list = []\n",
    "\n",
    "    # Node features\n",
    "    for node in nx_graph.nodes(data=True):\n",
    "        original_id = node[0]\n",
    "        attrs = node[1]\n",
    "        symbol = attrs.get(\"symbol\", \"C\")\n",
    "        symbol_id = 0 if symbol == \"C\" else 1 if symbol == \"H\" else 2\n",
    "        x_list.append([symbol_id])\n",
    "\n",
    "    # Edge features\n",
    "    for u, v, edge_attrs in nx_graph.edges(data=True):\n",
    "        u_idx = node_mapping[u]\n",
    "        v_idx = node_mapping[v]\n",
    "        edge_index_list.append((u_idx, v_idx))\n",
    "        bde_pred = edge_attrs.get(\"bde_pred\", 0.0) or 0.0\n",
    "        bdfe_pred = edge_attrs.get(\"bdfe_pred\", 0.0) or 0.0\n",
    "        edge_attr_list.append([bde_pred, bdfe_pred])\n",
    "\n",
    "    x = torch.tensor(x_list, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                     BASE GNN MODEL (WITH DIM MATCH)\n",
    "# =============================================================================\n",
    "\n",
    "class GINE_Regression(nn.Module):\n",
    "    \"\"\"\n",
    "    A GNN for regression using GINEConv layers + edge attributes,\n",
    "    where all layers have the same hidden_dim (no dimension mismatch).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dim: int = 128,\n",
    "                 num_layers: int = 3,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encode edges from edge_in_dim to hidden_dim\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Encode nodes from node_in_dim to hidden_dim\n",
    "        self.node_encoder = nn.Linear(node_in_dim, hidden_dim)\n",
    "        \n",
    "        # Multiple GINEConv layers & corresponding BatchNorm\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Process external factors\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Encode\n",
    "        x = self.node_encoder(x)\n",
    "        edge_emb = self.edge_encoder(edge_attr)\n",
    "        \n",
    "        # Pass through GINEConv layers\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Global pooling\n",
    "        graph_emb = global_mean_pool(x, batch)\n",
    "\n",
    "        # Process external factors\n",
    "        ext_emb = self.externals_mlp(data.externals)\n",
    "\n",
    "        # Combine & regress\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)\n",
    "        out = self.final_regressor(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                   TRAIN/VALID/EVALUATION UTILS\n",
    "# =============================================================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_data)\n",
    "        y = batch_data.y.to(device).view(-1)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "        count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            preds = model(batch_data)\n",
    "            y = batch_data.y.to(device).view(-1)\n",
    "            loss = criterion(preds, y)\n",
    "            total_loss += loss.item() * batch_data.num_graphs\n",
    "            count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset loader and compute R² and RMSE.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            preds = model(batch)\n",
    "            y_true.append(batch.y.cpu())\n",
    "            y_pred.append(preds.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true).numpy().squeeze()\n",
    "    y_pred = torch.cat(y_pred).numpy().squeeze()\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return r2, rmse\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                             DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "env_file = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\dataset02.xlsx\"\n",
    "data = pd.read_excel(env_file, engine='openpyxl').dropna(subset=['degradation_rate'])\n",
    "data['seawater'] = data['seawater'].map({'art': 1, 'sea': 0})\n",
    "\n",
    "folder_path = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\"\n",
    "graph_pickles = [f for f in os.listdir(folder_path) if f.endswith(\".pkl\")]\n",
    "\n",
    "base_dir = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\"\n",
    "if os.path.exists(base_dir):\n",
    "    print(\"Directory exists:\", base_dir)\n",
    "    print(\"Files in directory:\", os.listdir(base_dir))\n",
    "else:\n",
    "    print(f\"Error: Directory {base_dir} does not exist!\")\n",
    "\n",
    "compounds = data.component.unique()\n",
    "graphs_dict = {}\n",
    "for compound, graph_pickle in zip(compounds, graph_pickles):\n",
    "    with open(os.path.join(base_dir, graph_pickle), 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "        graphs_dict[compound] = networkx_to_pyg(graph)\n",
    "\n",
    "dataset = MolDataset(\n",
    "    raw_dataframe=data,\n",
    "    nx_graph_dict=graphs_dict,\n",
    "    component_col=\"component\",\n",
    "    global_state_cols=[\"temperature\", \"concentration\", \"time\", \"seawater\"],\n",
    "    label_col=\"degradation_rate\",\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "#                      CROSS-VALIDATION (FIXED MODEL)\n",
    "# =============================================================================\n",
    "from torch_geometric.data import DataLoader as PyGDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 5  # number of folds\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"\\n--- Fold {fold + 1} ---\")\n",
    "\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "    val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = GINE_Regression(\n",
    "        node_in_dim=1,\n",
    "        edge_in_dim=2,\n",
    "        external_in_dim=4,\n",
    "        hidden_dim=16,  # Example hidden_dim\n",
    "        num_layers=5,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    num_epochs = 500\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[Fold {fold + 1} Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    print(f\"Evaluating fold {fold + 1} ...\")\n",
    "    r2, rmse = evaluate_model(model, val_loader, device)\n",
    "    fold_results.append({\"fold\": fold + 1, \"r2\": r2, \"rmse\": rmse})\n",
    "\n",
    "r2_scores = [res[\"r2\"] for res in fold_results]\n",
    "rmse_scores = [res[\"rmse\"] for res in fold_results]\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "for res in fold_results:\n",
    "    print(f\"Fold {res['fold']}: R² = {res['r2']:.4f}, RMSE = {res['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#             IMPROVED MODEL WITH TRAPEZOID DIMENSIONS & PROJECTIONS\n",
    "# =============================================================================\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "\n",
    "def build_trapezoid_dims(num_layers):\n",
    "    \"\"\"\n",
    "    Build a list of hidden dimensions in a trapezoid manner:\n",
    "    up to the first 4 layers: [128, 64, 32, 16]\n",
    "    if num_layers > 4, extend with 16 for extra layers.\n",
    "    \"\"\"\n",
    "    base_dims = [128, 64, 32, 16]\n",
    "    if num_layers > 4:\n",
    "        extra_layers = num_layers - 4\n",
    "        return base_dims + [16] * extra_layers\n",
    "    else:\n",
    "        return base_dims[:num_layers]\n",
    "\n",
    "\n",
    "class GINE_RegressionTrapezoid(nn.Module):\n",
    "    \"\"\"\n",
    "    A GINEConv-based regression model that uses a list of hidden dimensions\n",
    "    to build layers with decreasing size (trapezoid architecture),\n",
    "    ensuring dimension consistency with projection layers between convs.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dims: list,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # For the first layer, encode edges to hidden_dims[0], and encode nodes as well\n",
    "        self.initial_edge_encoder = nn.Linear(edge_in_dim, hidden_dims[0])\n",
    "        self.initial_node_encoder = nn.Linear(node_in_dim, hidden_dims[0])\n",
    "\n",
    "        # We'll build each GINEConv to transform dimension: hidden_dims[i] -> hidden_dims[i].\n",
    "        # After each conv i, if i < len(hidden_dims)-1, we project to hidden_dims[i+1].\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.projections = nn.ModuleList()  # for node features\n",
    "        self.edge_projections = nn.ModuleList()  # for edge features\n",
    "\n",
    "        for i in range(len(hidden_dims)):\n",
    "            # GINEConv's internal MLP\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i])\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dims[i]))\n",
    "\n",
    "            # If there's a next layer, we need a projection from hidden_dims[i] -> hidden_dims[i+1]\n",
    "            if i < len(hidden_dims) - 1:\n",
    "                self.projections.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "                self.edge_projections.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            else:\n",
    "                # No projection needed for the last layer\n",
    "                self.projections.append(None)\n",
    "                self.edge_projections.append(None)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "\n",
    "        # We'll process external factors to the final dimension\n",
    "        final_dim = hidden_dims[-1]\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, final_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(final_dim, final_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression: combine final node features + final external features\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(final_dim + final_dim, final_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(final_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # 1) Encode node/edge to hidden_dims[0]\n",
    "        x = self.initial_node_encoder(x)\n",
    "        edge_emb = self.initial_edge_encoder(edge_attr)\n",
    "\n",
    "        # 2) Pass through each GINEConv\n",
    "        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):\n",
    "            # GINEConv forward\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout_layer(x)\n",
    "\n",
    "            # If there's a next layer, project node features and edge_emb to hidden_dims[i+1]\n",
    "            if i < len(self.projections) - 1 and self.projections[i] is not None:\n",
    "                x = self.projections[i](x)\n",
    "                edge_emb = self.edge_projections[i](edge_emb)\n",
    "\n",
    "        # 3) Global mean pooling\n",
    "        graph_emb = global_mean_pool(x, batch)\n",
    "\n",
    "        # 4) Process external factors into final dimension\n",
    "        ext_emb = self.externals_mlp(data.externals)\n",
    "\n",
    "        # 5) Concatenate and final regression\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)\n",
    "        out = self.final_regressor(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                          OPTUNA OBJECTIVE FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna.\n",
    "    We do k-fold cross validation using a new GNN model that\n",
    "    sets hidden_dims = [128]*num_layers (all layers = 128).\n",
    "    Return the average RMSE (lower = better).\n",
    "    We still compute R² for reference and store it in user_attrs.\n",
    "    Includes both Optuna pruning and custom early stopping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameter search space\n",
    "    lr = trial.suggest_categorical(\"learning_rate\", [1e-3, 3e-3, 1e-4, 3e-4])\n",
    "    dropout = trial.suggest_categorical(\"dropout\", [0.1, 0.5])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "\n",
    "    # ==========================\n",
    "    # CHANGE: all hidden_dims=200\n",
    "    # ==========================\n",
    "    hidden_dims = [200] * num_layers\n",
    "\n",
    "    # Early stopping parameters\n",
    "    max_epochs = 500\n",
    "    patience = 10\n",
    "    min_delta = 1e-5\n",
    "\n",
    "    # Prepare 5-fold CV\n",
    "    kf_local = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf_local.split(dataset)):\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "        val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "        # Build the GNN model with uniform 128-dim layers\n",
    "        model = GINE_RegressionTrapezoid(\n",
    "            node_in_dim=1,\n",
    "            edge_in_dim=2,\n",
    "            external_in_dim=4,\n",
    "            hidden_dims=hidden_dims,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Early-stopping tracking\n",
    "        best_val_loss = float(\"inf\")\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            # Train + Validate\n",
    "            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "            val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "            # Report intermediate metric to Optuna (for pruning visualization)\n",
    "            trial.report(val_loss, step=epoch)\n",
    "\n",
    "            # Check if Optuna wants to prune (e.g., median pruner)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "            # Custom Early Stopping\n",
    "            if val_loss < (best_val_loss - min_delta):\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    # stop training early, but complete the trial\n",
    "                    break\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        r2_fold, rmse_fold = evaluate_model(model, val_loader, device)\n",
    "        rmse_scores.append(rmse_fold)\n",
    "        r2_scores.append(r2_fold)\n",
    "\n",
    "    # Averages over folds\n",
    "    avg_rmse = float(np.mean(rmse_scores))\n",
    "    avg_r2 = float(np.mean(r2_scores))\n",
    "\n",
    "    # Log the R² so it appears in the dashboard's user_attrs\n",
    "    trial.set_user_attr(\"avg_r2\", avg_r2)\n",
    "\n",
    "    # Return RMSE (the main metric we want to minimize)\n",
    "    return avg_rmse\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                           OPTUNA STUDY & DASHBOARD\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(\n",
    "        storage=\"sqlite:///gnn_mix_op07.sqlite3\",\n",
    "        study_name=\"GNN-mixed model 200 with 500 epoch\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "    print(\"\\n================= Optuna Study Results =================\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Best Trial Value (RMSE): {best_trial.value}\")\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for key, val in best_trial.params.items():\n",
    "        print(f\"  {key}: {val}\")\n",
    "    print(f\"User Attrs (R², etc.): {best_trial.user_attrs}\")\n",
    "\n",
    "    try:\n",
    "        fig1 = vis.plot_optimization_history(study)\n",
    "        fig1.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate optimization history plot: {e}\")\n",
    "\n",
    "    try:\n",
    "        fig2 = vis.plot_param_importances(study)\n",
    "        fig2.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate hyperparameter importance plot: {e}\")\n",
    "\n",
    "    try:\n",
    "        fig3 = vis.plot_intermediate_values(study)\n",
    "        fig3.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate intermediate values plot: {e}\")\n",
    "\n",
    "    print(\"\\n================= End of Optuna Tuning =================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfabet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
