{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### op16 新增部分\n",
    "    # 固定定义6个隐藏层参数\n",
    "    hd0 = trial.suggest_categorical(\"NEW_hidden_dim_0\", candidate_values)\n",
    "    hd1 = trial.suggest_categorical(\"NEW_hidden_dim_1\", candidate_values)\n",
    "    hd2 = trial.suggest_categorical(\"NEW_hidden_dim_2\", candidate_values)\n",
    "    hd3 = trial.suggest_categorical(\"NEW_hidden_dim_3\", candidate_values)\n",
    "    hd4 = trial.suggest_categorical(\"NEW_hidden_dim_4\", candidate_values)\n",
    "    hd5 = trial.suggest_categorical(\"NEW_hidden_dim_5\", candidate_values)\n",
    "    hidden_dims_all = [hd0, hd1, hd2, hd3, hd4, hd5]\n",
    "\n",
    "    # 只使用前 num_layers 个隐藏层\n",
    "    hidden_dims = hidden_dims_all[:num_layers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the complete code with all the modifications integrated. In this version:\n",
    "\n",
    "**The number of layers is restricted to 2–6.\n",
    "For each trial, the hidden dimensions are sampled one by one so that they form a strictly decreasing (trapezoidal) sequence. For example, if you sample 128 for layer 0, then layer 1 will be chosen from values strictly less than 128.\n",
    "Hyperparameters (learning rate, dropout, num_layers, and each hidden dimension) are all logged so that they appear in the Optuna dashboard.\n",
    "During training, intermediate validation losses are reported to enable early-stopping tracking in the “Intermediate Values” plot of the dashboard.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN层遍历的架构逻辑\n",
    "\n",
    "1. **Use a Fixed Candidate Set:**  \n",
    "   Instead of sampling any integer between 16 and 256, we'll define a fixed set of allowable hidden dimensions. For example, the candidate set will be:  \n",
    "   ```python\n",
    "   candidate_values = [256, 128, 64, 32, 16]\n",
    "   ```  \n",
    "   This set contains exactly the values you mentioned in your examples.\n",
    "\n",
    "2. **Enforce a Non-Increasing (Trapezoidal) Structure:**  \n",
    "   For each layer, we will choose a hidden dimension from the candidate set with the following rules:\n",
    "   - **First Layer:**  \n",
    "     For layer 0, we simply choose one value from the entire candidate set.\n",
    "   - **Subsequent Layers:**  \n",
    "     For layer _i_ (where _i_ > 0), we restrict the available options to only those values that are less than or equal to the hidden dimension chosen for layer _i-1_.  \n",
    "     This means if the first layer was chosen as 256, then for the next layer, the allowed options will be [256, 128, 64, 32, 16].  \n",
    "     If the first layer is 128, then the second layer can only be one of [128, 64, 32, 16].  \n",
    "     This restriction ensures that the sequence is non-increasing (i.e., it \"gets narrower\" as you go deeper) and that only valid choices like [256,256,32,32] or [128,64,64] are considered.\n",
    "\n",
    "3. **Hyperparameter Sampling with Optuna:**  \n",
    "   - We use `trial.suggest_categorical` to sample the hidden dimension for each layer from the appropriate candidate list.\n",
    "   - The number of layers (between 2 and 6) is also a hyperparameter. For each layer in the chosen architecture, we apply the above logic to build the list of hidden dimensions.\n",
    "   - This way, every trial in the optimization will have a candidate architecture with a discrete, non-increasing set of hidden dimensions (for example, [256,256,32,32] or [128,64,64]) and will exclude choices like [256,155,17,16] because 155 and 17 are not in the candidate set.\n",
    "\n",
    "4. **Result Logging and Dashboard Integration:**  \n",
    "   - All hyperparameters (including the number of layers and each layer's hidden dimension) will be logged.\n",
    "   - Intermediate validation loss values will be reported during training so that you can track the early-stopping behavior on the Optuna dashboard (via the \"Intermediate Values\" plot).\n",
    "\n",
    "This logic guarantees that the search space will consist only of architectures that conform to a trapezoidal (non-increasing) pattern using your predefined candidate values. Once you confirm this approach, we can proceed to implement it in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 具体逻辑\n",
    "\n",
    "1. Fixed Candidate Set of Hidden Dimensions:\n",
    "We use candidate_values = [256, 128, 64, 32, 16].\n",
    "\n",
    "2. Non-Increasing (Trapezoidal) Architecture:\n",
    "\n",
    "- The first layer’s dimension is chosen from the full candidate set.\n",
    "- Each subsequent layer is chosen from those values less than or equal to the previous layer’s dimension, supporting valid patterns like [256, 256, 32, 32] or [128, 64, 64].\n",
    "\n",
    "3. Number of Layers Restriction (2–6):\n",
    "Controlled via trial.suggest_int(\"num_layers\", 2, 6).\n",
    "\n",
    "4. Hyperparameter Logging:\n",
    "\n",
    "- Learning rate, dropout, number of layers, and each hidden_dim_i (via trial.suggest_categorical).\n",
    "- All parameters automatically appear in the Optuna dashboard.\n",
    "\n",
    "5. Intermediate Validation Loss Reporting:\n",
    "\n",
    "trial.report(val_loss, step=epoch) is called each epoch, letting you track early stopping in the “Intermediate Values” plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How This Solves the Dynamic Value Space Error如何解决动态值空间错误\n",
    "Fixed Distribution:固定分布：\n",
    "Every hidden_dim_{i} parameter is sampled from the same list [256, 128, 64, 32, 16]. Thus, Optuna never sees a changing candidate list, which avoids the “CategoricalDistribution does not support dynamic value space.” error.每个hidden_dim_{i}参数都从同一个列表[256, 128, 64, 32, 16]中采样。因此，Optuna永远不会看到变化的候选列表，从而避免了“CategoricalDistribution 不支持动态值空间。”错误。\n",
    "\n",
    "Trapezoidal Constraint:梯形约束：\n",
    "After sampling each layer’s width, a quick check ensures subsequent layers’ widths do not exceed the previous layer’s. If no suitable options are found, the default is the smallest dimension (16). The net result is a non-increasing (trapezoidal) architecture, and Optuna does not complain because each parameter’s distribution remains constant.在对每层的宽度进行采样后，会进行快速检查，确保后续层的宽度不超过前一层的宽度。如果未找到合适的选项，则默认为最小尺寸（16）。最终结果是非递增（梯形）架构，Optuna 不会抱怨，因为每个参数的分布保持不变。\n",
    "\n",
    "Pruning & Logging:修剪和伐木：\n",
    "\n",
    "If needed, you can prune trials that violate the non-increasing rule—though the code above simply narrows the candidate options.如果需要，您可以修剪违反非增加规则的试验 - 尽管上述代码只是缩小了候选选项的范围。\n",
    "Hyperparameters (learning_rate, dropout, num_layers, hidden_dim_i) are all logged for dashboard inspection.超参数（ learning_rate 、 dropout 、 num_layers 、 hidden_dim_i ）均已记录以供仪表板检查。\n",
    "trial.report(val_loss, step=epoch) allows you to visualize intermediate values for early-stopping.trial.report(val_loss, step=epoch) 可让您直观地看到提前停止的中间值。\n",
    "By combining a fixed candidate set for every layer with a post-check that filters invalid configurations, you get your desired trapezoidal GNN structure and avoid the Optuna dynamic-distribution error.通过将每一层的固定候选集与过滤无效配置的后检查相结合，您可以获得所需的梯形 GNN 结构并避免 Optuna 动态分布错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\n",
      "Files in directory: ['gpickle_graph_0.pkl', 'gpickle_graph_1.pkl', 'gpickle_graph_10.pkl', 'gpickle_graph_11.pkl', 'gpickle_graph_12.pkl', 'gpickle_graph_13.pkl', 'gpickle_graph_14.pkl', 'gpickle_graph_15.pkl', 'gpickle_graph_16.pkl', 'gpickle_graph_17.pkl', 'gpickle_graph_18.pkl', 'gpickle_graph_19.pkl', 'gpickle_graph_2.pkl', 'gpickle_graph_3.pkl', 'gpickle_graph_4.pkl', 'gpickle_graph_5.pkl', 'gpickle_graph_6.pkl', 'gpickle_graph_7.pkl', 'gpickle_graph_8.pkl', 'gpickle_graph_9.pkl']\n",
      "\n",
      "--- Fold 1 ---\n",
      "[Fold 1 Epoch 10] Train Loss: 0.2102 | Val Loss: 0.0691\n",
      "[Fold 1 Epoch 20] Train Loss: 0.1101 | Val Loss: 0.0513\n",
      "[Fold 1 Epoch 30] Train Loss: 0.0830 | Val Loss: 0.0509\n",
      "[Fold 1 Epoch 40] Train Loss: 0.0788 | Val Loss: 0.0476\n",
      "[Fold 1 Epoch 50] Train Loss: 0.0925 | Val Loss: 0.0548\n",
      "[Fold 1 Epoch 60] Train Loss: 0.0706 | Val Loss: 0.0571\n",
      "[Fold 1 Epoch 70] Train Loss: 0.0722 | Val Loss: 0.0557\n",
      "[Fold 1 Epoch 80] Train Loss: 0.0718 | Val Loss: 0.0491\n",
      "[Fold 1 Epoch 90] Train Loss: 0.0718 | Val Loss: 0.0466\n",
      "[Fold 1 Epoch 100] Train Loss: 0.0715 | Val Loss: 0.0507\n",
      "[Fold 1 Epoch 110] Train Loss: 0.0622 | Val Loss: 0.0463\n",
      "[Fold 1 Epoch 120] Train Loss: 0.0601 | Val Loss: 0.0472\n",
      "[Fold 1 Epoch 130] Train Loss: 0.0574 | Val Loss: 0.0501\n",
      "[Fold 1 Epoch 140] Train Loss: 0.0622 | Val Loss: 0.0435\n",
      "[Fold 1 Epoch 150] Train Loss: 0.0587 | Val Loss: 0.0447\n",
      "[Fold 1 Epoch 160] Train Loss: 0.0669 | Val Loss: 0.0440\n",
      "[Fold 1 Epoch 170] Train Loss: 0.0562 | Val Loss: 0.0429\n",
      "[Fold 1 Epoch 180] Train Loss: 0.0610 | Val Loss: 0.0446\n",
      "[Fold 1 Epoch 190] Train Loss: 0.0533 | Val Loss: 0.0450\n",
      "[Fold 1 Epoch 200] Train Loss: 0.0528 | Val Loss: 0.0430\n",
      "[Fold 1 Epoch 210] Train Loss: 0.0559 | Val Loss: 0.0431\n",
      "[Fold 1 Epoch 220] Train Loss: 0.0520 | Val Loss: 0.0468\n",
      "[Fold 1 Epoch 230] Train Loss: 0.0538 | Val Loss: 0.0442\n",
      "[Fold 1 Epoch 240] Train Loss: 0.0582 | Val Loss: 0.0407\n",
      "[Fold 1 Epoch 250] Train Loss: 0.0478 | Val Loss: 0.0429\n",
      "[Fold 1 Epoch 260] Train Loss: 0.0546 | Val Loss: 0.0366\n",
      "[Fold 1 Epoch 270] Train Loss: 0.0514 | Val Loss: 0.0393\n",
      "[Fold 1 Epoch 280] Train Loss: 0.0461 | Val Loss: 0.0336\n",
      "[Fold 1 Epoch 290] Train Loss: 0.0541 | Val Loss: 0.0389\n",
      "[Fold 1 Epoch 300] Train Loss: 0.0510 | Val Loss: 0.0410\n",
      "[Fold 1 Epoch 310] Train Loss: 0.0443 | Val Loss: 0.0393\n",
      "[Fold 1 Epoch 320] Train Loss: 0.0516 | Val Loss: 0.0389\n",
      "[Fold 1 Epoch 330] Train Loss: 0.0594 | Val Loss: 0.0405\n",
      "[Fold 1 Epoch 340] Train Loss: 0.0492 | Val Loss: 0.0426\n",
      "[Fold 1 Epoch 350] Train Loss: 0.0474 | Val Loss: 0.0789\n",
      "[Fold 1 Epoch 360] Train Loss: 0.0462 | Val Loss: 0.3345\n",
      "[Fold 1 Epoch 370] Train Loss: 0.0473 | Val Loss: 0.0424\n",
      "[Fold 1 Epoch 380] Train Loss: 0.0495 | Val Loss: 0.0871\n",
      "[Fold 1 Epoch 390] Train Loss: 0.0429 | Val Loss: 0.0709\n",
      "[Fold 1 Epoch 400] Train Loss: 0.0466 | Val Loss: 0.0420\n",
      "[Fold 1 Epoch 410] Train Loss: 0.0510 | Val Loss: 0.0443\n",
      "[Fold 1 Epoch 420] Train Loss: 0.0425 | Val Loss: 0.0323\n",
      "[Fold 1 Epoch 430] Train Loss: 0.0486 | Val Loss: 0.0449\n",
      "[Fold 1 Epoch 440] Train Loss: 0.0445 | Val Loss: 0.0476\n",
      "[Fold 1 Epoch 450] Train Loss: 0.0586 | Val Loss: 0.0364\n",
      "[Fold 1 Epoch 460] Train Loss: 0.0427 | Val Loss: 0.0413\n",
      "[Fold 1 Epoch 470] Train Loss: 0.0420 | Val Loss: 0.0361\n",
      "[Fold 1 Epoch 480] Train Loss: 0.0381 | Val Loss: 0.3103\n",
      "[Fold 1 Epoch 490] Train Loss: 0.0522 | Val Loss: 0.0313\n",
      "[Fold 1 Epoch 500] Train Loss: 0.0397 | Val Loss: 0.0380\n",
      "Evaluating fold 1 ...\n",
      "R²: 0.3242\n",
      "RMSE: 0.1949\n",
      "\n",
      "--- Fold 2 ---\n",
      "[Fold 2 Epoch 10] Train Loss: 0.2549 | Val Loss: 0.1628\n",
      "[Fold 2 Epoch 20] Train Loss: 0.1610 | Val Loss: 0.1132\n",
      "[Fold 2 Epoch 30] Train Loss: 0.1021 | Val Loss: 0.1186\n",
      "[Fold 2 Epoch 40] Train Loss: 0.0873 | Val Loss: 0.1145\n",
      "[Fold 2 Epoch 50] Train Loss: 0.0912 | Val Loss: 0.1060\n",
      "[Fold 2 Epoch 60] Train Loss: 0.0758 | Val Loss: 0.0997\n",
      "[Fold 2 Epoch 70] Train Loss: 0.0729 | Val Loss: 0.1042\n",
      "[Fold 2 Epoch 80] Train Loss: 0.0700 | Val Loss: 0.0983\n",
      "[Fold 2 Epoch 90] Train Loss: 0.0725 | Val Loss: 0.1039\n",
      "[Fold 2 Epoch 100] Train Loss: 0.0673 | Val Loss: 0.1051\n",
      "[Fold 2 Epoch 110] Train Loss: 0.0689 | Val Loss: 0.0972\n",
      "[Fold 2 Epoch 120] Train Loss: 0.0686 | Val Loss: 0.1010\n",
      "[Fold 2 Epoch 130] Train Loss: 0.0620 | Val Loss: 0.1024\n",
      "[Fold 2 Epoch 140] Train Loss: 0.0657 | Val Loss: 0.1001\n",
      "[Fold 2 Epoch 150] Train Loss: 0.0733 | Val Loss: 0.0987\n",
      "[Fold 2 Epoch 160] Train Loss: 0.0568 | Val Loss: 0.1017\n",
      "[Fold 2 Epoch 170] Train Loss: 0.0671 | Val Loss: 0.1021\n",
      "[Fold 2 Epoch 180] Train Loss: 0.0745 | Val Loss: 0.0955\n",
      "[Fold 2 Epoch 190] Train Loss: 0.0615 | Val Loss: 0.0929\n",
      "[Fold 2 Epoch 200] Train Loss: 0.0559 | Val Loss: 0.1011\n",
      "[Fold 2 Epoch 210] Train Loss: 0.0719 | Val Loss: 0.1051\n",
      "[Fold 2 Epoch 220] Train Loss: 0.0548 | Val Loss: 0.0950\n",
      "[Fold 2 Epoch 230] Train Loss: 0.0621 | Val Loss: 0.0892\n",
      "[Fold 2 Epoch 240] Train Loss: 0.0623 | Val Loss: 0.0862\n",
      "[Fold 2 Epoch 250] Train Loss: 0.0583 | Val Loss: 0.0852\n",
      "[Fold 2 Epoch 260] Train Loss: 0.0621 | Val Loss: 0.0822\n",
      "[Fold 2 Epoch 270] Train Loss: 0.0622 | Val Loss: 0.0837\n",
      "[Fold 2 Epoch 280] Train Loss: 0.0547 | Val Loss: 0.0866\n",
      "[Fold 2 Epoch 290] Train Loss: 0.0511 | Val Loss: 0.0864\n",
      "[Fold 2 Epoch 300] Train Loss: 0.0607 | Val Loss: 0.0791\n",
      "[Fold 2 Epoch 310] Train Loss: 0.0471 | Val Loss: 0.0817\n",
      "[Fold 2 Epoch 320] Train Loss: 0.0590 | Val Loss: 0.0853\n",
      "[Fold 2 Epoch 330] Train Loss: 0.0550 | Val Loss: 0.0809\n",
      "[Fold 2 Epoch 340] Train Loss: 0.0560 | Val Loss: 0.1097\n",
      "[Fold 2 Epoch 350] Train Loss: 0.0442 | Val Loss: 0.0797\n",
      "[Fold 2 Epoch 360] Train Loss: 0.0509 | Val Loss: 0.0771\n",
      "[Fold 2 Epoch 370] Train Loss: 0.0529 | Val Loss: 0.0781\n",
      "[Fold 2 Epoch 380] Train Loss: 0.0472 | Val Loss: 0.0818\n",
      "[Fold 2 Epoch 390] Train Loss: 0.0496 | Val Loss: 0.0801\n",
      "[Fold 2 Epoch 400] Train Loss: 0.0645 | Val Loss: 0.0755\n",
      "[Fold 2 Epoch 410] Train Loss: 0.0453 | Val Loss: 0.0784\n",
      "[Fold 2 Epoch 420] Train Loss: 0.0528 | Val Loss: 0.0735\n",
      "[Fold 2 Epoch 430] Train Loss: 0.0502 | Val Loss: 0.0750\n",
      "[Fold 2 Epoch 440] Train Loss: 0.0507 | Val Loss: 0.0832\n",
      "[Fold 2 Epoch 450] Train Loss: 0.0489 | Val Loss: 0.0777\n",
      "[Fold 2 Epoch 460] Train Loss: 0.0506 | Val Loss: 0.0777\n",
      "[Fold 2 Epoch 470] Train Loss: 0.0444 | Val Loss: 0.0794\n",
      "[Fold 2 Epoch 480] Train Loss: 0.0523 | Val Loss: 0.0824\n",
      "[Fold 2 Epoch 490] Train Loss: 0.0602 | Val Loss: 0.1046\n",
      "[Fold 2 Epoch 500] Train Loss: 0.0618 | Val Loss: 0.1048\n",
      "Evaluating fold 2 ...\n",
      "R²: -0.0036\n",
      "RMSE: 0.3237\n",
      "\n",
      "--- Fold 3 ---\n",
      "[Fold 3 Epoch 10] Train Loss: 0.2357 | Val Loss: 0.0913\n",
      "[Fold 3 Epoch 20] Train Loss: 0.1218 | Val Loss: 0.0665\n",
      "[Fold 3 Epoch 30] Train Loss: 0.1045 | Val Loss: 0.0720\n",
      "[Fold 3 Epoch 40] Train Loss: 0.0905 | Val Loss: 0.0632\n",
      "[Fold 3 Epoch 50] Train Loss: 0.0903 | Val Loss: 0.0576\n",
      "[Fold 3 Epoch 60] Train Loss: 0.0867 | Val Loss: 0.0575\n",
      "[Fold 3 Epoch 70] Train Loss: 0.0847 | Val Loss: 0.0736\n",
      "[Fold 3 Epoch 80] Train Loss: 0.0685 | Val Loss: 0.0703\n",
      "[Fold 3 Epoch 90] Train Loss: 0.0728 | Val Loss: 0.0562\n",
      "[Fold 3 Epoch 100] Train Loss: 0.0713 | Val Loss: 0.0541\n",
      "[Fold 3 Epoch 110] Train Loss: 0.0669 | Val Loss: 0.0516\n",
      "[Fold 3 Epoch 120] Train Loss: 0.0574 | Val Loss: 0.0470\n",
      "[Fold 3 Epoch 130] Train Loss: 0.0563 | Val Loss: 0.0456\n",
      "[Fold 3 Epoch 140] Train Loss: 0.0623 | Val Loss: 0.0580\n",
      "[Fold 3 Epoch 150] Train Loss: 0.0598 | Val Loss: 0.0571\n",
      "[Fold 3 Epoch 160] Train Loss: 0.0558 | Val Loss: 0.0450\n",
      "[Fold 3 Epoch 170] Train Loss: 0.0492 | Val Loss: 0.0517\n",
      "[Fold 3 Epoch 180] Train Loss: 0.0542 | Val Loss: 0.0614\n",
      "[Fold 3 Epoch 190] Train Loss: 0.0599 | Val Loss: 0.0482\n",
      "[Fold 3 Epoch 200] Train Loss: 0.0527 | Val Loss: 0.0419\n",
      "[Fold 3 Epoch 210] Train Loss: 0.0509 | Val Loss: 0.0401\n",
      "[Fold 3 Epoch 220] Train Loss: 0.0547 | Val Loss: 0.0467\n",
      "[Fold 3 Epoch 230] Train Loss: 0.0501 | Val Loss: 0.0391\n",
      "[Fold 3 Epoch 240] Train Loss: 0.0541 | Val Loss: 0.0445\n",
      "[Fold 3 Epoch 250] Train Loss: 0.0565 | Val Loss: 0.0478\n",
      "[Fold 3 Epoch 260] Train Loss: 0.0489 | Val Loss: 0.0420\n",
      "[Fold 3 Epoch 270] Train Loss: 0.0457 | Val Loss: 0.0400\n",
      "[Fold 3 Epoch 280] Train Loss: 0.0525 | Val Loss: 0.0419\n",
      "[Fold 3 Epoch 290] Train Loss: 0.0520 | Val Loss: 0.0362\n",
      "[Fold 3 Epoch 300] Train Loss: 0.0514 | Val Loss: 0.0364\n",
      "[Fold 3 Epoch 310] Train Loss: 0.0474 | Val Loss: 0.0394\n",
      "[Fold 3 Epoch 320] Train Loss: 0.0502 | Val Loss: 0.0362\n",
      "[Fold 3 Epoch 330] Train Loss: 0.0390 | Val Loss: 0.0338\n",
      "[Fold 3 Epoch 340] Train Loss: 0.0402 | Val Loss: 0.0560\n",
      "[Fold 3 Epoch 350] Train Loss: 0.0469 | Val Loss: 0.0377\n",
      "[Fold 3 Epoch 360] Train Loss: 0.0408 | Val Loss: 0.0415\n",
      "[Fold 3 Epoch 370] Train Loss: 0.0393 | Val Loss: 0.0384\n",
      "[Fold 3 Epoch 380] Train Loss: 0.0433 | Val Loss: 0.0345\n",
      "[Fold 3 Epoch 390] Train Loss: 0.0401 | Val Loss: 0.0338\n",
      "[Fold 3 Epoch 400] Train Loss: 0.0423 | Val Loss: 0.0382\n",
      "[Fold 3 Epoch 410] Train Loss: 0.0550 | Val Loss: 0.0393\n",
      "[Fold 3 Epoch 420] Train Loss: 0.0376 | Val Loss: 0.0334\n",
      "[Fold 3 Epoch 430] Train Loss: 0.0363 | Val Loss: 0.0338\n",
      "[Fold 3 Epoch 440] Train Loss: 0.0395 | Val Loss: 0.0360\n",
      "[Fold 3 Epoch 450] Train Loss: 0.0400 | Val Loss: 0.0333\n",
      "[Fold 3 Epoch 460] Train Loss: 0.0415 | Val Loss: 0.0322\n",
      "[Fold 3 Epoch 470] Train Loss: 0.0442 | Val Loss: 0.0341\n",
      "[Fold 3 Epoch 480] Train Loss: 0.0412 | Val Loss: 0.0360\n",
      "[Fold 3 Epoch 490] Train Loss: 0.0364 | Val Loss: 0.0329\n",
      "[Fold 3 Epoch 500] Train Loss: 0.0416 | Val Loss: 0.0443\n",
      "Evaluating fold 3 ...\n",
      "R²: 0.3660\n",
      "RMSE: 0.2104\n",
      "\n",
      "--- Fold 4 ---\n",
      "[Fold 4 Epoch 10] Train Loss: 0.1389 | Val Loss: 0.1174\n",
      "[Fold 4 Epoch 20] Train Loss: 0.0872 | Val Loss: 0.0889\n",
      "[Fold 4 Epoch 30] Train Loss: 0.0701 | Val Loss: 0.0809\n",
      "[Fold 4 Epoch 40] Train Loss: 0.0787 | Val Loss: 0.0833\n",
      "[Fold 4 Epoch 50] Train Loss: 0.0609 | Val Loss: 0.0918\n",
      "[Fold 4 Epoch 60] Train Loss: 0.0645 | Val Loss: 0.0774\n",
      "[Fold 4 Epoch 70] Train Loss: 0.0596 | Val Loss: 0.0742\n",
      "[Fold 4 Epoch 80] Train Loss: 0.0583 | Val Loss: 0.0773\n",
      "[Fold 4 Epoch 90] Train Loss: 0.0593 | Val Loss: 0.0732\n",
      "[Fold 4 Epoch 100] Train Loss: 0.0566 | Val Loss: 0.0794\n",
      "[Fold 4 Epoch 110] Train Loss: 0.0624 | Val Loss: 0.0747\n",
      "[Fold 4 Epoch 120] Train Loss: 0.0610 | Val Loss: 0.0773\n",
      "[Fold 4 Epoch 130] Train Loss: 0.0558 | Val Loss: 0.0762\n",
      "[Fold 4 Epoch 140] Train Loss: 0.0604 | Val Loss: 0.0777\n",
      "[Fold 4 Epoch 150] Train Loss: 0.0481 | Val Loss: 0.0651\n",
      "[Fold 4 Epoch 160] Train Loss: 0.0480 | Val Loss: 0.0742\n",
      "[Fold 4 Epoch 170] Train Loss: 0.0502 | Val Loss: 0.0719\n",
      "[Fold 4 Epoch 180] Train Loss: 0.0498 | Val Loss: 0.0683\n",
      "[Fold 4 Epoch 190] Train Loss: 0.0511 | Val Loss: 0.0658\n",
      "[Fold 4 Epoch 200] Train Loss: 0.0534 | Val Loss: 0.0659\n",
      "[Fold 4 Epoch 210] Train Loss: 0.0516 | Val Loss: 0.0762\n",
      "[Fold 4 Epoch 220] Train Loss: 0.0499 | Val Loss: 0.0668\n",
      "[Fold 4 Epoch 230] Train Loss: 0.0435 | Val Loss: 0.0811\n",
      "[Fold 4 Epoch 240] Train Loss: 0.0498 | Val Loss: 0.0628\n",
      "[Fold 4 Epoch 250] Train Loss: 0.0437 | Val Loss: 0.0466\n",
      "[Fold 4 Epoch 260] Train Loss: 0.0360 | Val Loss: 0.0595\n",
      "[Fold 4 Epoch 270] Train Loss: 0.0439 | Val Loss: 0.0481\n",
      "[Fold 4 Epoch 280] Train Loss: 0.0379 | Val Loss: 0.0632\n",
      "[Fold 4 Epoch 290] Train Loss: 0.0412 | Val Loss: 0.0519\n",
      "[Fold 4 Epoch 300] Train Loss: 0.0416 | Val Loss: 0.0422\n",
      "[Fold 4 Epoch 310] Train Loss: 0.0386 | Val Loss: 0.0454\n",
      "[Fold 4 Epoch 320] Train Loss: 0.0375 | Val Loss: 0.0477\n",
      "[Fold 4 Epoch 330] Train Loss: 0.0387 | Val Loss: 0.0447\n",
      "[Fold 4 Epoch 340] Train Loss: 0.0381 | Val Loss: 0.0681\n",
      "[Fold 4 Epoch 350] Train Loss: 0.0394 | Val Loss: 0.0557\n",
      "[Fold 4 Epoch 360] Train Loss: 0.0379 | Val Loss: 0.0534\n",
      "[Fold 4 Epoch 370] Train Loss: 0.0380 | Val Loss: 0.0734\n",
      "[Fold 4 Epoch 380] Train Loss: 0.0362 | Val Loss: 0.0607\n",
      "[Fold 4 Epoch 390] Train Loss: 0.0358 | Val Loss: 0.0514\n",
      "[Fold 4 Epoch 400] Train Loss: 0.0393 | Val Loss: 0.0569\n",
      "[Fold 4 Epoch 410] Train Loss: 0.0398 | Val Loss: 0.0405\n",
      "[Fold 4 Epoch 420] Train Loss: 0.0363 | Val Loss: 0.0504\n",
      "[Fold 4 Epoch 430] Train Loss: 0.0360 | Val Loss: 0.0508\n",
      "[Fold 4 Epoch 440] Train Loss: 0.0372 | Val Loss: 0.0641\n",
      "[Fold 4 Epoch 450] Train Loss: 0.0328 | Val Loss: 0.0529\n",
      "[Fold 4 Epoch 460] Train Loss: 0.0397 | Val Loss: 0.0548\n",
      "[Fold 4 Epoch 470] Train Loss: 0.0356 | Val Loss: 0.0528\n",
      "[Fold 4 Epoch 480] Train Loss: 0.0366 | Val Loss: 0.0653\n",
      "[Fold 4 Epoch 490] Train Loss: 0.0385 | Val Loss: 0.0497\n",
      "[Fold 4 Epoch 500] Train Loss: 0.0344 | Val Loss: 0.0474\n",
      "Evaluating fold 4 ...\n",
      "R²: 0.4720\n",
      "RMSE: 0.2177\n",
      "\n",
      "--- Fold 5 ---\n",
      "[Fold 5 Epoch 10] Train Loss: 0.1471 | Val Loss: 0.0717\n",
      "[Fold 5 Epoch 20] Train Loss: 0.0923 | Val Loss: 0.0678\n",
      "[Fold 5 Epoch 30] Train Loss: 0.0825 | Val Loss: 0.0635\n",
      "[Fold 5 Epoch 40] Train Loss: 0.0910 | Val Loss: 0.0624\n",
      "[Fold 5 Epoch 50] Train Loss: 0.0788 | Val Loss: 0.0633\n",
      "[Fold 5 Epoch 60] Train Loss: 0.0672 | Val Loss: 0.0628\n",
      "[Fold 5 Epoch 70] Train Loss: 0.0656 | Val Loss: 0.0635\n",
      "[Fold 5 Epoch 80] Train Loss: 0.0758 | Val Loss: 0.0621\n",
      "[Fold 5 Epoch 90] Train Loss: 0.0659 | Val Loss: 0.0633\n",
      "[Fold 5 Epoch 100] Train Loss: 0.0748 | Val Loss: 0.0613\n",
      "[Fold 5 Epoch 110] Train Loss: 0.0699 | Val Loss: 0.0633\n",
      "[Fold 5 Epoch 120] Train Loss: 0.0709 | Val Loss: 0.0641\n",
      "[Fold 5 Epoch 130] Train Loss: 0.0690 | Val Loss: 0.0620\n",
      "[Fold 5 Epoch 140] Train Loss: 0.0654 | Val Loss: 0.0587\n",
      "[Fold 5 Epoch 150] Train Loss: 0.0682 | Val Loss: 0.0604\n",
      "[Fold 5 Epoch 160] Train Loss: 0.0503 | Val Loss: 0.0588\n",
      "[Fold 5 Epoch 170] Train Loss: 0.0651 | Val Loss: 0.0604\n",
      "[Fold 5 Epoch 180] Train Loss: 0.0566 | Val Loss: 0.0572\n",
      "[Fold 5 Epoch 190] Train Loss: 0.0658 | Val Loss: 0.0567\n",
      "[Fold 5 Epoch 200] Train Loss: 0.0669 | Val Loss: 0.0557\n",
      "[Fold 5 Epoch 210] Train Loss: 0.0566 | Val Loss: 0.0537\n",
      "[Fold 5 Epoch 220] Train Loss: 0.0548 | Val Loss: 0.0517\n",
      "[Fold 5 Epoch 230] Train Loss: 0.0562 | Val Loss: 0.0513\n",
      "[Fold 5 Epoch 240] Train Loss: 0.0601 | Val Loss: 0.0494\n",
      "[Fold 5 Epoch 250] Train Loss: 0.0537 | Val Loss: 0.0528\n",
      "[Fold 5 Epoch 260] Train Loss: 0.0557 | Val Loss: 0.0455\n",
      "[Fold 5 Epoch 270] Train Loss: 0.0477 | Val Loss: 0.0419\n",
      "[Fold 5 Epoch 280] Train Loss: 0.0467 | Val Loss: 0.0392\n",
      "[Fold 5 Epoch 290] Train Loss: 0.0430 | Val Loss: 0.0404\n",
      "[Fold 5 Epoch 300] Train Loss: 0.0459 | Val Loss: 0.0380\n",
      "[Fold 5 Epoch 310] Train Loss: 0.0457 | Val Loss: 0.0466\n",
      "[Fold 5 Epoch 320] Train Loss: 0.0410 | Val Loss: 0.0308\n",
      "[Fold 5 Epoch 330] Train Loss: 0.0494 | Val Loss: 0.0310\n",
      "[Fold 5 Epoch 340] Train Loss: 0.0530 | Val Loss: 0.0428\n",
      "[Fold 5 Epoch 350] Train Loss: 0.0539 | Val Loss: 0.0468\n",
      "[Fold 5 Epoch 360] Train Loss: 0.0614 | Val Loss: 0.0428\n",
      "[Fold 5 Epoch 370] Train Loss: 0.0540 | Val Loss: 0.0478\n",
      "[Fold 5 Epoch 380] Train Loss: 0.0490 | Val Loss: 0.0450\n",
      "[Fold 5 Epoch 390] Train Loss: 0.0542 | Val Loss: 0.0477\n",
      "[Fold 5 Epoch 400] Train Loss: 0.0502 | Val Loss: 0.0472\n",
      "[Fold 5 Epoch 410] Train Loss: 0.0531 | Val Loss: 0.0455\n",
      "[Fold 5 Epoch 420] Train Loss: 0.0619 | Val Loss: 0.0466\n",
      "[Fold 5 Epoch 430] Train Loss: 0.0520 | Val Loss: 0.0448\n",
      "[Fold 5 Epoch 440] Train Loss: 0.0533 | Val Loss: 0.0444\n",
      "[Fold 5 Epoch 450] Train Loss: 0.0534 | Val Loss: 0.0440\n",
      "[Fold 5 Epoch 460] Train Loss: 0.0553 | Val Loss: 0.0426\n",
      "[Fold 5 Epoch 470] Train Loss: 0.0582 | Val Loss: 0.0451\n",
      "[Fold 5 Epoch 480] Train Loss: 0.0552 | Val Loss: 0.0414\n",
      "[Fold 5 Epoch 490] Train Loss: 0.0506 | Val Loss: 0.0383\n",
      "[Fold 5 Epoch 500] Train Loss: 0.0479 | Val Loss: 0.0321\n",
      "Evaluating fold 5 ...\n",
      "R²: 0.5368\n",
      "RMSE: 0.1792\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "Fold 1: R² = 0.3242, RMSE = 0.1949\n",
      "Fold 2: R² = -0.0036, RMSE = 0.3237\n",
      "Fold 3: R² = 0.3660, RMSE = 0.2104\n",
      "Fold 4: R² = 0.4720, RMSE = 0.2177\n",
      "Fold 5: R² = 0.5368, RMSE = 0.1792\n",
      "\n",
      "Average R²: 0.3391 ± 0.1872\n",
      "Average RMSE: 0.2252 ± 0.0510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-02 17:19:18,971] A new study created in RDB with name: GNN-mixed model different layers02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ff7287031440bbbd381e82d31e5d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-04-02 17:19:19,136] Trial 0 pruned. \n",
      "[I 2025-04-02 17:19:19,299] Trial 1 pruned. \n",
      "[I 2025-04-02 17:19:19,466] Trial 2 pruned. \n",
      "[I 2025-04-02 17:19:19,625] Trial 3 pruned. \n",
      "[I 2025-04-02 17:19:19,786] Trial 4 pruned. \n",
      "[I 2025-04-02 17:19:19,955] Trial 5 pruned. \n",
      "R²: -0.4289\n",
      "RMSE: 0.2833\n",
      "R²: -0.0930\n",
      "RMSE: 0.3378\n",
      "R²: -0.5349\n",
      "RMSE: 0.3273\n",
      "R²: -0.9162\n",
      "RMSE: 0.4148\n",
      "R²: 0.0558\n",
      "RMSE: 0.2558\n",
      "[I 2025-04-02 17:22:53,338] Trial 6 finished with value: 0.3238190534668016 and parameters: {'learning_rate': 0.0001, 'dropout': 0.1, 'num_layers': 2, 'NEW_hidden_dim_0': 32, 'NEW_hidden_dim_1': 16, 'NEW_hidden_dim_2': 256, 'NEW_hidden_dim_3': 16, 'NEW_hidden_dim_4': 32, 'NEW_hidden_dim_5': 256}. Best is trial 6 with value: 0.3238190534668016.\n",
      "[I 2025-04-02 17:22:53,497] Trial 7 pruned. \n",
      "[I 2025-04-02 17:22:53,656] Trial 8 pruned. \n",
      "R²: -0.7431\n",
      "RMSE: 0.3130\n",
      "R²: -2.6059\n",
      "RMSE: 0.6136\n",
      "R²: -34.0888\n",
      "RMSE: 1.5651\n",
      "R²: -18.1290\n",
      "RMSE: 1.3104\n",
      "R²: -2.9611\n",
      "RMSE: 0.5239\n",
      "[I 2025-04-02 17:33:17,731] Trial 9 finished with value: 0.8652197242150924 and parameters: {'learning_rate': 0.0001, 'dropout': 0.5, 'num_layers': 4, 'NEW_hidden_dim_0': 256, 'NEW_hidden_dim_1': 256, 'NEW_hidden_dim_2': 128, 'NEW_hidden_dim_3': 16, 'NEW_hidden_dim_4': 16, 'NEW_hidden_dim_5': 64}. Best is trial 6 with value: 0.3238190534668016.\n",
      "R²: -0.0543\n",
      "RMSE: 0.2434\n",
      "R²: -2.2424\n",
      "RMSE: 0.5819\n",
      "R²: -0.9602\n",
      "RMSE: 0.3699\n",
      "R²: -5.2378\n",
      "RMSE: 0.7483\n",
      "R²: -12.2283\n",
      "RMSE: 0.9575\n",
      "[I 2025-04-02 17:38:04,639] Trial 10 finished with value: 0.5801988671217918 and parameters: {'learning_rate': 0.0001, 'dropout': 0.5, 'num_layers': 2, 'NEW_hidden_dim_0': 32, 'NEW_hidden_dim_1': 16, 'NEW_hidden_dim_2': 256, 'NEW_hidden_dim_3': 128, 'NEW_hidden_dim_4': 64, 'NEW_hidden_dim_5': 16}. Best is trial 6 with value: 0.3238190534668016.\n",
      "R²: -6.9318\n",
      "RMSE: 0.6676\n",
      "R²: -6.7797\n",
      "RMSE: 0.9013\n",
      "R²: -1.6916\n",
      "RMSE: 0.4335\n",
      "R²: -11.5097\n",
      "RMSE: 1.0597\n",
      "R²: -0.0429\n",
      "RMSE: 0.2688\n",
      "[I 2025-04-02 17:43:13,920] Trial 11 finished with value: 0.6661954461596393 and parameters: {'learning_rate': 0.0001, 'dropout': 0.5, 'num_layers': 2, 'NEW_hidden_dim_0': 32, 'NEW_hidden_dim_1': 16, 'NEW_hidden_dim_2': 256, 'NEW_hidden_dim_3': 128, 'NEW_hidden_dim_4': 64, 'NEW_hidden_dim_5': 16}. Best is trial 6 with value: 0.3238190534668016.\n",
      "R²: -6.2219\n",
      "RMSE: 0.6370\n",
      "R²: -8.8869\n",
      "RMSE: 1.0161\n",
      "R²: -4.8978\n",
      "RMSE: 0.6417\n",
      "R²: -2.4861\n",
      "RMSE: 0.5594\n",
      "R²: -4.1589\n",
      "RMSE: 0.5979\n",
      "[I 2025-04-02 17:45:17,534] Trial 12 finished with value: 0.6904294428616393 and parameters: {'learning_rate': 0.0001, 'dropout': 0.5, 'num_layers': 2, 'NEW_hidden_dim_0': 32, 'NEW_hidden_dim_1': 16, 'NEW_hidden_dim_2': 256, 'NEW_hidden_dim_3': 128, 'NEW_hidden_dim_4': 64, 'NEW_hidden_dim_5': 16}. Best is trial 6 with value: 0.3238190534668016.\n",
      "[I 2025-04-02 17:45:17,693] Trial 13 pruned. \n",
      "[I 2025-04-02 17:45:17,864] Trial 14 pruned. \n",
      "[I 2025-04-02 17:45:19,286] Trial 15 pruned. \n",
      "[I 2025-04-02 17:45:20,747] Trial 16 pruned. \n",
      "[I 2025-04-02 17:45:20,912] Trial 17 pruned. \n",
      "[I 2025-04-02 17:45:21,082] Trial 18 pruned. \n",
      "[I 2025-04-02 17:45:25,600] Trial 19 pruned. \n",
      "[I 2025-04-02 17:45:25,807] Trial 20 pruned. \n",
      "[I 2025-04-02 17:45:27,088] Trial 21 pruned. \n",
      "[I 2025-04-02 17:45:28,340] Trial 22 pruned. \n",
      "[I 2025-04-02 17:45:28,507] Trial 23 pruned. \n",
      "[I 2025-04-02 17:45:28,683] Trial 24 pruned. \n",
      "[I 2025-04-02 17:45:32,682] Trial 25 pruned. \n",
      "R²: 0.0902\n",
      "RMSE: 0.2261\n",
      "R²: 0.0785\n",
      "RMSE: 0.3102\n",
      "R²: 0.1329\n",
      "RMSE: 0.2460\n",
      "R²: 0.0520\n",
      "RMSE: 0.2917\n",
      "R²: 0.0476\n",
      "RMSE: 0.2569\n",
      "[I 2025-04-02 17:48:56,129] Trial 26 finished with value: 0.26619852796905674 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 2, 'NEW_hidden_dim_0': 32, 'NEW_hidden_dim_1': 16, 'NEW_hidden_dim_2': 16, 'NEW_hidden_dim_3': 128, 'NEW_hidden_dim_4': 32, 'NEW_hidden_dim_5': 16}. Best is trial 26 with value: 0.26619852796905674.\n",
      "[I 2025-04-02 17:48:56,291] Trial 27 pruned. \n",
      "R²: 0.1534\n",
      "RMSE: 0.2181\n",
      "R²: -0.2252\n",
      "RMSE: 0.3577\n",
      "R²: 0.0173\n",
      "RMSE: 0.2619\n",
      "R²: 0.0493\n",
      "RMSE: 0.2921\n",
      "R²: 0.0860\n",
      "RMSE: 0.2517\n",
      "[I 2025-04-02 17:52:04,236] Trial 28 finished with value: 0.27630831868876543 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 2, 'NEW_hidden_dim_0': 32, 'NEW_hidden_dim_1': 16, 'NEW_hidden_dim_2': 16, 'NEW_hidden_dim_3': 16, 'NEW_hidden_dim_4': 32, 'NEW_hidden_dim_5': 256}. Best is trial 26 with value: 0.26619852796905674.\n",
      "R²: 0.0335\n",
      "RMSE: 0.2330\n",
      "R²: 0.1556\n",
      "RMSE: 0.2969\n",
      "R²: -0.0297\n",
      "RMSE: 0.2681\n",
      "R²: -0.0075\n",
      "RMSE: 0.3007\n",
      "R²: 0.1252\n",
      "RMSE: 0.2462\n",
      "[I 2025-04-02 17:54:30,459] Trial 29 finished with value: 0.2690138062327435 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 3, 'NEW_hidden_dim_0': 64, 'NEW_hidden_dim_1': 16, 'NEW_hidden_dim_2': 16, 'NEW_hidden_dim_3': 16, 'NEW_hidden_dim_4': 32, 'NEW_hidden_dim_5': 256}. Best is trial 26 with value: 0.26619852796905674.\n",
      "R²: 0.1099\n",
      "RMSE: 0.2236\n",
      "R²: 0.0152\n",
      "RMSE: 0.3207\n",
      "R²: 0.0747\n",
      "RMSE: 0.2542\n",
      "R²: 0.0565\n",
      "RMSE: 0.2910\n",
      "R²: 0.1072\n",
      "RMSE: 0.2487\n",
      "[I 2025-04-02 17:59:57,536] Trial 30 finished with value: 0.2676525682225398 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 3, 'NEW_hidden_dim_0': 64, 'NEW_hidden_dim_1': 16, 'NEW_hidden_dim_2': 16, 'NEW_hidden_dim_3': 16, 'NEW_hidden_dim_4': 32, 'NEW_hidden_dim_5': 256}. Best is trial 26 with value: 0.26619852796905674.\n",
      "R²: 0.0615\n",
      "RMSE: 0.2296\n",
      "R²: 0.0670\n",
      "RMSE: 0.3121\n",
      "R²: 0.0379\n",
      "RMSE: 0.2592\n",
      "R²: -0.1460\n",
      "RMSE: 0.3207\n",
      "R²: 0.2165\n",
      "RMSE: 0.2330\n",
      "[I 2025-04-02 18:04:40,335] Trial 31 finished with value: 0.2709421901362581 and parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 3, 'NEW_hidden_dim_0': 64, 'NEW_hidden_dim_1': 16, 'NEW_hidden_dim_2': 16, 'NEW_hidden_dim_3': 16, 'NEW_hidden_dim_4': 32, 'NEW_hidden_dim_5': 256}. Best is trial 26 with value: 0.26619852796905674.\n",
      "R²: 0.1537\n",
      "RMSE: 0.2181\n",
      "R²: -0.0211\n",
      "RMSE: 0.3265\n",
      "R²: 0.0096\n",
      "RMSE: 0.2629\n",
      "[W 2025-04-02 18:07:36,446] Trial 32 failed with parameters: {'learning_rate': 0.001, 'dropout': 0.1, 'num_layers': 3, 'NEW_hidden_dim_0': 64, 'NEW_hidden_dim_1': 16, 'NEW_hidden_dim_2': 16, 'NEW_hidden_dim_3': 16, 'NEW_hidden_dim_4': 32, 'NEW_hidden_dim_5': 256} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\kang_\\AppData\\Local\\Temp\\ipykernel_34244\\991601730.py\", line 520, in objective\n",
      "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
      "  File \"C:\\Users\\kang_\\AppData\\Local\\Temp\\ipykernel_34244\\991601730.py\", line 221, in train_one_epoch\n",
      "    for batch_data in loader:\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 708, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 764, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 50, in fetch\n",
      "    data = self.dataset.__getitems__(possibly_batched_index)\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 420, in __getitems__\n",
      "    return [self.dataset[self.indices[idx]] for idx in indices]\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 420, in <listcomp>\n",
      "    return [self.dataset[self.indices[idx]] for idx in indices]\n",
      "  File \"C:\\Users\\kang_\\AppData\\Local\\Temp\\ipykernel_34244\\991601730.py\", line 81, in __getitem__\n",
      "    pyg_data.externals = externals  # shape [1, external_in_dim]\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch_geometric\\data\\data.py\", line 568, in __setattr__\n",
      "    setattr(self._store, key, value)\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch_geometric\\data\\storage.py\", line 109, in __setattr__\n",
      "    self[key] = value\n",
      "  File \"f:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch_geometric\\data\\storage.py\", line 125, in __setitem__\n",
      "    self._mapping[key] = value\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-02 18:07:36,457] Trial 32 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 559\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;66;03m# 使用 load_if_exists=False 以确保使用全新 study，避免旧数据冲突\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m    553\u001b[0m         storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite:///gnn_mix_op09.sqlite3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    554\u001b[0m         study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGNN-mixed model different layers02\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    555\u001b[0m         direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    556\u001b[0m         load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    557\u001b[0m     )\n\u001b[1;32m--> 559\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m================= Optuna Study Results =================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    562\u001b[0m     best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[2], line 520\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    517\u001b[0m patience_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 520\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n\u001b[0;32m    523\u001b[0m     trial\u001b[38;5;241m.\u001b[39mreport(val_loss, step\u001b[38;5;241m=\u001b[39mepoch)\n",
      "Cell \u001b[1;32mIn[2], line 221\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m    219\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    220\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m    222\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m batch_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    223\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[2], line 81\u001b[0m, in \u001b[0;36mMolDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     78\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([row[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_col][\u001b[38;5;241m0\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# 4. Attach externals & label to the Data object\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[43mpyg_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexternals\u001b[49m \u001b[38;5;241m=\u001b[39m externals  \u001b[38;5;66;03m# shape [1, external_in_dim]\u001b[39;00m\n\u001b[0;32m     82\u001b[0m pyg_data\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m label  \u001b[38;5;66;03m# shape [1]\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch_geometric\\data\\data.py:568\u001b[0m, in \u001b[0;36mData.__setattr__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    566\u001b[0m     propobj\u001b[38;5;241m.\u001b[39mfset(\u001b[38;5;28mself\u001b[39m, value)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 568\u001b[0m     \u001b[38;5;28;43msetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch_geometric\\data\\storage.py:109\u001b[0m, in \u001b[0;36mBaseStorage.__setattr__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32mf:\\Miniconda3\\envs\\alfabet_env\\lib\\site-packages\\torch_geometric\\data\\storage.py:125\u001b[0m, in \u001b[0;36mBaseStorage.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data, DataLoader as PyGDataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool, BatchNorm\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "#                             DATASET & PREP\n",
    "# =============================================================================\n",
    "\n",
    "class MolDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset that:\n",
    "      - Reads external factors from CSV\n",
    "      - Loads the corresponding pickle for the molecule's graph\n",
    "      - Converts it into a PyG Data object\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 raw_dataframe: pd.DataFrame,\n",
    "                 nx_graph_dict: dict,\n",
    "                 *,\n",
    "                 component_col: str,\n",
    "                 global_state_cols: list[str],\n",
    "                 label_col: str,\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_dataframe: The input dataframe containing molecule info.\n",
    "            nx_graph_dict: Dictionary mapping component names to networkx graphs.\n",
    "            component_col: Column name for the component.\n",
    "            global_state_cols: List of columns representing external factors.\n",
    "            label_col: Column name for the regression target.\n",
    "            transform: Any transform to apply to each PyG Data object.\n",
    "        \"\"\"\n",
    "        self.raw_dataframe = raw_dataframe\n",
    "        self.nx_graph_dict = nx_graph_dict\n",
    "        self.component_col = [component_col] if type(component_col) is str else component_col\n",
    "        self.global_state_cols = global_state_cols\n",
    "        self.label_col = [label_col] if type(label_col) is str else label_col\n",
    "        self.transform = transform\n",
    "        \n",
    "        required_cols = set(self.global_state_cols + self.label_col + self.component_col)\n",
    "        for col in required_cols:\n",
    "            if col not in self.raw_dataframe.columns:\n",
    "                raise ValueError(f\"Missing column in DataFrame: '{col}'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.raw_dataframe.iloc[idx]\n",
    "        \n",
    "        # 1. Load the molecule graph\n",
    "        component_name = row[self.component_col[0]]  # e.g. \"C23\"\n",
    "        pyg_data = self.nx_graph_dict[component_name]\n",
    "\n",
    "        # 2. Prepare the external factors\n",
    "        externals = torch.tensor(row[self.global_state_cols].values.astype(float), dtype=torch.float)\n",
    "        externals = externals.unsqueeze(0)\n",
    "\n",
    "        # 3. Prepare the label (regression target)\n",
    "        label = torch.tensor([row[self.label_col][0]], dtype=torch.float)\n",
    "\n",
    "        # 4. Attach externals & label to the Data object\n",
    "        pyg_data.externals = externals  # shape [1, external_in_dim]\n",
    "        pyg_data.y = label  # shape [1]\n",
    "\n",
    "        if self.transform:\n",
    "            pyg_data = self.transform(pyg_data)\n",
    "\n",
    "        return pyg_data\n",
    "\n",
    "\n",
    "def networkx_to_pyg(nx_graph):\n",
    "    \"\"\"\n",
    "    Convert a networkx graph to a torch_geometric.data.Data object.\n",
    "    This is a basic template; adjust for your actual node/edge features.\n",
    "    \"\"\"\n",
    "    # Sort nodes to ensure consistent ordering\n",
    "    node_mapping = {node: i for i, node in enumerate(nx_graph.nodes())}\n",
    "\n",
    "    x_list = []\n",
    "    edge_index_list = []\n",
    "    edge_attr_list = []\n",
    "\n",
    "    # Node features\n",
    "    for node in nx_graph.nodes(data=True):\n",
    "        original_id = node[0]\n",
    "        attrs = node[1]\n",
    "        symbol = attrs.get(\"symbol\", \"C\")\n",
    "        symbol_id = 0 if symbol == \"C\" else 1 if symbol == \"H\" else 2\n",
    "        x_list.append([symbol_id])\n",
    "\n",
    "    # Edge features\n",
    "    for u, v, edge_attrs in nx_graph.edges(data=True):\n",
    "        u_idx = node_mapping[u]\n",
    "        v_idx = node_mapping[v]\n",
    "        edge_index_list.append((u_idx, v_idx))\n",
    "        bde_pred = edge_attrs.get(\"bde_pred\", 0.0) or 0.0\n",
    "        bdfe_pred = edge_attrs.get(\"bdfe_pred\", 0.0) or 0.0\n",
    "        edge_attr_list.append([bde_pred, bdfe_pred])\n",
    "\n",
    "    x = torch.tensor(x_list, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                     BASE GNN MODEL (WITH DIM MATCH)\n",
    "# =============================================================================\n",
    "\n",
    "class GINE_Regression(nn.Module):\n",
    "    \"\"\"\n",
    "    A GNN for regression using GINEConv layers + edge attributes,\n",
    "    where all layers have the same hidden_dim (no dimension mismatch).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dim: int = 128,\n",
    "                 num_layers: int = 3,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encode edges from edge_in_dim to hidden_dim\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Encode nodes from node_in_dim to hidden_dim\n",
    "        self.node_encoder = nn.Linear(node_in_dim, hidden_dim)\n",
    "        \n",
    "        # Multiple GINEConv layers & corresponding BatchNorm\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Process external factors\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Encode\n",
    "        x = self.node_encoder(x)\n",
    "        edge_emb = self.edge_encoder(edge_attr)\n",
    "        \n",
    "        # Pass through GINEConv layers\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Global pooling\n",
    "        graph_emb = global_mean_pool(x, batch)\n",
    "\n",
    "        # Process external factors\n",
    "        ext_emb = self.externals_mlp(data.externals)\n",
    "\n",
    "        # Combine & regress\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)\n",
    "        out = self.final_regressor(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                   TRAIN/VALID/EVALUATION UTILS\n",
    "# =============================================================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_data)\n",
    "        y = batch_data.y.to(device).view(-1)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "        count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            preds = model(batch_data)\n",
    "            y = batch_data.y.to(device).view(-1)\n",
    "            loss = criterion(preds, y)\n",
    "            total_loss += loss.item() * batch_data.num_graphs\n",
    "            count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset loader and compute R² and RMSE.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            preds = model(batch)\n",
    "            y_true.append(batch.y.cpu())\n",
    "            y_pred.append(preds.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true).numpy().squeeze()\n",
    "    y_pred = torch.cat(y_pred).numpy().squeeze()\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return r2, rmse\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                             DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "env_file = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\dataset02.xlsx\"\n",
    "data = pd.read_excel(env_file, engine='openpyxl').dropna(subset=['degradation_rate'])\n",
    "data['seawater'] = data['seawater'].map({'art': 1, 'sea': 0})\n",
    "\n",
    "folder_path = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\"\n",
    "graph_pickles = [f for f in os.listdir(folder_path) if f.endswith(\".pkl\")]\n",
    "\n",
    "base_dir = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\"\n",
    "if os.path.exists(base_dir):\n",
    "    print(\"Directory exists:\", base_dir)\n",
    "    print(\"Files in directory:\", os.listdir(base_dir))\n",
    "else:\n",
    "    print(f\"Error: Directory {base_dir} does not exist!\")\n",
    "\n",
    "compounds = data.component.unique()\n",
    "graphs_dict = {}\n",
    "for compound, graph_pickle in zip(compounds, graph_pickles):\n",
    "    with open(os.path.join(base_dir, graph_pickle), 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "        graphs_dict[compound] = networkx_to_pyg(graph)\n",
    "\n",
    "dataset = MolDataset(\n",
    "    raw_dataframe=data,\n",
    "    nx_graph_dict=graphs_dict,\n",
    "    component_col=\"component\",\n",
    "    global_state_cols=[\"temperature\", \"concentration\", \"time\", \"seawater\"],\n",
    "    label_col=\"degradation_rate\",\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "#                      CROSS-VALIDATION (FIXED MODEL)\n",
    "# =============================================================================\n",
    "from torch_geometric.data import DataLoader as PyGDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 5  # number of folds\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"\\n--- Fold {fold + 1} ---\")\n",
    "\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "    val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = GINE_Regression(\n",
    "        node_in_dim=1,\n",
    "        edge_in_dim=2,\n",
    "        external_in_dim=4,\n",
    "        hidden_dim=16,  # Example hidden_dim\n",
    "        num_layers=5,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    num_epochs = 500\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[Fold {fold + 1} Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    print(f\"Evaluating fold {fold + 1} ...\")\n",
    "    r2, rmse = evaluate_model(model, val_loader, device)\n",
    "    fold_results.append({\"fold\": fold + 1, \"r2\": r2, \"rmse\": rmse})\n",
    "\n",
    "r2_scores = [res[\"r2\"] for res in fold_results]\n",
    "rmse_scores = [res[\"rmse\"] for res in fold_results]\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "for res in fold_results:\n",
    "    print(f\"Fold {res['fold']}: R² = {res['r2']:.4f}, RMSE = {res['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#             IMPROVED MODEL WITH TRAPEZOID DIMENSIONS & PROJECTIONS\n",
    "# =============================================================================\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "\n",
    "class GINE_RegressionTrapezoid(nn.Module):\n",
    "    \"\"\"\n",
    "    A GINEConv-based regression model that uses a list of hidden dimensions\n",
    "    to build layers with decreasing size (trapezoid architecture),\n",
    "    ensuring dimension consistency with projection layers between convs.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dims: list,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # For the first layer, encode edges to hidden_dims[0], and encode nodes as well\n",
    "        self.initial_edge_encoder = nn.Linear(edge_in_dim, hidden_dims[0])\n",
    "        self.initial_node_encoder = nn.Linear(node_in_dim, hidden_dims[0])\n",
    "\n",
    "        # We'll build each GINEConv to transform dimension: hidden_dims[i] -> hidden_dims[i].\n",
    "        # After each conv i, if i < len(hidden_dims)-1, we project to hidden_dims[i+1].\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.projections = nn.ModuleList()  # for node features\n",
    "        self.edge_projections = nn.ModuleList()  # for edge features\n",
    "\n",
    "        for i in range(len(hidden_dims)):\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i])\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dims[i]))\n",
    "\n",
    "            if i < len(hidden_dims) - 1:\n",
    "                self.projections.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "                self.edge_projections.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            else:\n",
    "                self.projections.append(None)\n",
    "                self.edge_projections.append(None)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "\n",
    "        final_dim = hidden_dims[-1]\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, final_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(final_dim, final_dim)\n",
    "        )\n",
    "\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(final_dim + final_dim, final_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(final_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        x = self.initial_node_encoder(x)\n",
    "        edge_emb = self.initial_edge_encoder(edge_attr)\n",
    "\n",
    "        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout_layer(x)\n",
    "\n",
    "            if i < len(self.projections) - 1 and self.projections[i] is not None:\n",
    "                x = self.projections[i](x)\n",
    "                edge_emb = self.edge_projections[i](edge_emb)\n",
    "\n",
    "        graph_emb = global_mean_pool(x, batch)\n",
    "        ext_emb = self.externals_mlp(data.externals)\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)\n",
    "        out = self.final_regressor(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna.\n",
    "    We do k-fold cross validation using a new GNN model with a non-increasing\n",
    "    (trapezoidal) architecture. The hyperparameters include learning rate, dropout,\n",
    "    number of layers, and for each layer, a hidden dimension chosen from a fixed\n",
    "    candidate set so that each subsequent layer's dimension is <= the previous layer's.\n",
    "    We return the average RMSE (lower = better). R² is stored in user_attrs.\n",
    "    Intermediate validation losses are reported for early stopping & dashboard visualization.\n",
    "    \"\"\"\n",
    "    # Hyperparameter search space\n",
    "    lr = trial.suggest_categorical(\"learning_rate\", [1e-3, 3e-3, 1e-4, 3e-4])\n",
    "    dropout = trial.suggest_categorical(\"dropout\", [0.1, 0.5])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 6)  # 层数限制\n",
    "\n",
    "    # 固定候选集：所有层均使用相同的候选集\n",
    "    candidate_values = [200, 128, 64, 32, 16]\n",
    "\n",
    "    # 固定定义6个隐藏层参数\n",
    "    hd0 = trial.suggest_categorical(\"NEW_hidden_dim_0\", candidate_values)\n",
    "    hd1 = trial.suggest_categorical(\"NEW_hidden_dim_1\", candidate_values)\n",
    "    hd2 = trial.suggest_categorical(\"NEW_hidden_dim_2\", candidate_values)\n",
    "    hd3 = trial.suggest_categorical(\"NEW_hidden_dim_3\", candidate_values)\n",
    "    hd4 = trial.suggest_categorical(\"NEW_hidden_dim_4\", candidate_values)\n",
    "    hd5 = trial.suggest_categorical(\"NEW_hidden_dim_5\", candidate_values)\n",
    "    hidden_dims_all = [hd0, hd1, hd2, hd3, hd4, hd5]\n",
    "\n",
    "    # 只使用前 num_layers 个隐藏层\n",
    "    hidden_dims = hidden_dims_all[:num_layers]\n",
    "\n",
    "    # 检查是否满足非递增（梯形）结构\n",
    "    for i in range(1, num_layers):\n",
    "        if hidden_dims[i] > hidden_dims[i-1]:\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    # Early stopping 参数\n",
    "    max_epochs = 200\n",
    "    patience = 10\n",
    "    min_delta = 1e-5\n",
    "\n",
    "    kf_local = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf_local.split(dataset)):\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "        val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "        # 构建使用选定梯形隐藏维度的 GNN 模型\n",
    "        model = GINE_RegressionTrapezoid(\n",
    "            node_in_dim=1,\n",
    "            edge_in_dim=2,\n",
    "            external_in_dim=4,\n",
    "            hidden_dims=hidden_dims,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "            val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "            trial.report(val_loss, step=epoch)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "            if val_loss < (best_val_loss - min_delta):\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    break\n",
    "\n",
    "        r2_fold, rmse_fold = evaluate_model(model, val_loader, device)\n",
    "        rmse_scores.append(rmse_fold)\n",
    "        r2_scores.append(r2_fold)\n",
    "\n",
    "    avg_rmse = float(np.mean(rmse_scores))\n",
    "    avg_r2 = float(np.mean(r2_scores))\n",
    "\n",
    "    trial.set_user_attr(\"avg_r2\", avg_r2)\n",
    "    return avg_rmse\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                           OPTUNA STUDY & DASHBOARD\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 使用 load_if_exists=False 以确保使用全新 study，避免旧数据冲突\n",
    "    study = optuna.create_study(\n",
    "        storage=\"sqlite:///gnn_mix_op09.sqlite3\",\n",
    "        study_name=\"GNN-mixed model different layers02\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=False\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "    print(\"\\n================= Optuna Study Results =================\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Best Trial Value (RMSE): {best_trial.value}\")\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for key, val in best_trial.params.items():\n",
    "        print(f\"  {key}: {val}\")\n",
    "    print(f\"User Attrs (R², etc.): {best_trial.user_attrs}\")\n",
    "\n",
    "    try:\n",
    "        fig1 = vis.plot_optimization_history(study)\n",
    "        fig1.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate optimization history plot: {e}\")\n",
    "\n",
    "    try:\n",
    "        fig2 = vis.plot_param_importances(study)\n",
    "        fig2.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate hyperparameter importance plot: {e}\")\n",
    "\n",
    "    try:\n",
    "        fig3 = vis.plot_intermediate_values(study)\n",
    "        fig3.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate intermediate values plot: {e}\")\n",
    "\n",
    "    print(\"\\n================= End of Optuna Tuning =================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfabet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
