{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\json_utils.py:187: The name tf.Dimension is deprecated. Please use tf.compat.v1.Dimension instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:178: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:178: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from alfabet.drawing import draw_mol_outlier\n",
    "from alfabet.fragment import canonicalize_smiles\n",
    "from alfabet.neighbors import find_neighbor_bonds\n",
    "from alfabet.prediction import predict_bdes, check_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import alfabet\n",
    "alfabet.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024.03.5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdkit.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def create_bde_graph_selective_hs(smiles: str, bde_df) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Build a NetworkX graph from the *original (heavy-atom)* RDKit Mol:\n",
    "      - Keep all heavy-atom ring & skeleton bonds from the SMILES.\n",
    "      - Add new H-X bonds (i.e., only the hydrogens needed) when a row in bde_df indicates\n",
    "        a predicted bond that doesn't already exist in the heavy-atom Mol.\n",
    "    \n",
    "    bde_df is expected to have columns:\n",
    "       - start_atom, end_atom: integer indexes or placeholders\n",
    "       - bde_pred, bdfe_pred, etc.: predicted data for each bond\n",
    "       - possibly bond_index (optional)\n",
    "    \n",
    "    Steps:\n",
    "       1) Parse the SMILES without adding Hs (just once).\n",
    "       2) Build a base Nx graph with all heavy-atom nodes & edges.\n",
    "       3) Iterate over bde_df. If the row corresponds to an existing heavy–heavy bond,\n",
    "          update the Nx edge with predicted data. If the row corresponds to an H–X bond,\n",
    "          add the H node + edge and store the predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Parse the SMILES into an RDKit Mol (no AddHs)\n",
    "    base_mol = Chem.MolFromSmiles(smiles)\n",
    "    if base_mol is None:\n",
    "        # Handle parse error, e.g. return empty graph\n",
    "        return nx.Graph()\n",
    "\n",
    "    # 2. Create an Nx graph, optionally store the RDKit Mol for reference\n",
    "    G = nx.Graph(mol=base_mol)\n",
    "\n",
    "    # 3. Add heavy-atom nodes\n",
    "    #    We'll store:\n",
    "    #      - 'symbol': e.g. 'C', 'O', 'N', etc.\n",
    "    #      - 'rdkit_idx': the integer index assigned by RDKit\n",
    "    #    Feel free to store other attributes as well.\n",
    "    for atom in base_mol.GetAtoms():\n",
    "        atom_idx = atom.GetIdx()\n",
    "        G.add_node(atom_idx, \n",
    "                   symbol=atom.GetSymbol(),\n",
    "                   rdkit_idx=atom_idx)\n",
    "\n",
    "    # 4. Add edges for all heavy-atom bonds in the original (no-H) Mol\n",
    "    #    We won't attach any BDE predictions yet (set them to None).\n",
    "    #    We'll also store a default bond_index=None if desired.\n",
    "    for bond in base_mol.GetBonds():\n",
    "        a1 = bond.GetBeginAtomIdx()\n",
    "        a2 = bond.GetEndAtomIdx()\n",
    "        G.add_edge(a1, a2,\n",
    "                   bond_index=None,\n",
    "                   bde_pred=None,\n",
    "                   bdfe_pred=None)\n",
    "\n",
    "    # 5. Iterate over bde_df.  We'll assume the columns are something like:\n",
    "    #     start_atom, end_atom, bde_pred, bdfe_pred, bond_index, etc.\n",
    "    #    - For heavy–heavy predictions, update the existing edge with predicted data.\n",
    "    #    - For H–X predictions, add the new hydrogen node & edge if not present.\n",
    "    #    - This approach assumes that for an H–X bond, either start_atom or end_atom\n",
    "    #      is a placeholder for hydrogen or an integer representing \"H\" in your dataset.\n",
    "    for _, row in bde_df.iterrows():\n",
    "        s = row['start_atom']\n",
    "        e = row['end_atom']\n",
    "        \n",
    "        # Attempt to interpret s and e in the context of the base mol\n",
    "        # We'll use a simple rule:\n",
    "        #  - If the index is >= base_mol.GetNumAtoms(), treat it as \"this is a hydrogen\"\n",
    "        #  - Or you could have a special marker like -1 for hydrogen\n",
    "        #    (depends on how your data is structured)\n",
    "        \n",
    "        # We also store predicted data\n",
    "        bde_pred_value = row.get('bde_pred', None)\n",
    "        bdfe_pred_value = row.get('bdfe_pred', None)\n",
    "        bond_index_value = row.get('bond_index', None)\n",
    "        \n",
    "        # Convert them to integers if needed\n",
    "        # (In practice, you may need to handle missing or invalid indexes carefully)\n",
    "        \n",
    "        # We'll define a helper function to check if an index is \"heavy\" or \"hydrogen\"\n",
    "        def is_heavy(idx):\n",
    "            return (0 <= idx < base_mol.GetNumAtoms())\n",
    "        \n",
    "        # Determine the \"types\" of s and e\n",
    "        s_is_heavy = is_heavy(s)\n",
    "        e_is_heavy = is_heavy(e)\n",
    "\n",
    "        if s_is_heavy and e_is_heavy:\n",
    "            # This is a heavy–heavy bond.\n",
    "            # If it already exists in G, update attributes.\n",
    "            if G.has_edge(s, e):\n",
    "                # Just update the existing edge\n",
    "                G[s][e]['bde_pred'] = bde_pred_value\n",
    "                G[s][e]['bdfe_pred'] = bdfe_pred_value\n",
    "                G[s][e]['bond_index'] = bond_index_value\n",
    "            else:\n",
    "                # Possibly -?> no, not possible the bond doesn't exist in the original skeleton \n",
    "                # (this can happen if the SMILES didn't have it).\n",
    "                # Add it as a new edge. This is unusual, but let's handle it anyway.\n",
    "                G.add_edge(s, e,\n",
    "                           bond_index=bond_index_value,\n",
    "                           bde_pred=bde_pred_value,\n",
    "                           bdfe_pred=bdfe_pred_value)\n",
    "\n",
    "        else:\n",
    "            # At least one of them is a \"hydrogen\" or out-of-range index\n",
    "            # We'll figure out which one is the heavy atom and which is the hydrogen.\n",
    "            if s_is_heavy and not e_is_heavy:\n",
    "                heavy_idx, hydrogen_idx = s, e\n",
    "            elif e_is_heavy and not s_is_heavy:\n",
    "                heavy_idx, hydrogen_idx = e, s\n",
    "            else:\n",
    "                # Both are hydrogens or out-of-range, which might be invalid.\n",
    "                # For safety, just skip or handle error.\n",
    "                # Could print a warning, raise an exception, etc.\n",
    "                continue\n",
    "\n",
    "            # Step 1: ensure the hydrogen node is present in G\n",
    "            # We'll generate a unique node key for the H, e.g. \"H_{hydrogen_idx}\"\n",
    "            # or something that won't collide with integer-based heavy nodes.\n",
    "            # You could also store the actual integer if your system allows it.\n",
    "            h_node = f\"H_{hydrogen_idx}\"\n",
    "            if not G.has_node(h_node):\n",
    "                # Add the hydrogen node with minimal attributes\n",
    "                G.add_node(h_node,\n",
    "                           symbol='H',\n",
    "                           rdkit_idx=None)  # or some other placeholder\n",
    "\n",
    "            # Step 2: add the H–X bond or update if it already exists\n",
    "            # The heavy_idx is the integer from RDKit.\n",
    "            if not G.has_edge(heavy_idx, h_node):\n",
    "                G.add_edge(heavy_idx, h_node,\n",
    "                           bond_index=bond_index_value,\n",
    "                           bde_pred=bde_pred_value,\n",
    "                           bdfe_pred=bdfe_pred_value)\n",
    "            else:\n",
    "                # If it somehow exists, just update attributes\n",
    "                G[heavy_idx][h_node]['bde_pred'] = bde_pred_value\n",
    "                G[heavy_idx][h_node]['bdfe_pred'] = bdfe_pred_value\n",
    "                G[heavy_idx][h_node]['bond_index'] = bond_index_value\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_to_df(bde_graph: nx.Graph) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the edges of bde_graph into a DataFrame with columns:\n",
    "      ['u', 'v', 'bond_index', 'graph_bde_pred', 'graph_bdfe_pred'].\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for u, v, data in bde_graph.edges(data=True):\n",
    "        rows.append({\n",
    "            'u': u,\n",
    "            'v': v,\n",
    "            'bond_index': data['bond_index'],\n",
    "            'graph_bde_pred': data.get('bde_pred', None),\n",
    "            'graph_bdfe_pred': data.get('bdfe_pred', None)\n",
    "        })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list = ['C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CCCC)(C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CCC(C)C)(C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CC[C@@H](C)CC)(C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CC[C@H](CCC)C)(C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@]2(C)CC3)CC[C@H](C)CCCCC)(C)C',\n",
    "       'C(CCC)C[C@H](C)CC[C@@H]1[C@H](CC[C@H]2[C@]1(CC[C@@H]3[C@@]2(CCCC3(C)C)C)C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CC[C@@H](CCCC(C)C)C)(C)C',\n",
    "       'C(C[C@@H](CC[C@H]1[C@]3([C@H](CC[C@@H]1C)[C@]2(CCCC(C)(C)[C@@H]2CC3)C)C)C)CC(C)C',\n",
    "       '[C@]23(CC[C@@H]1[C@@](CCCC1(C)C)(C)[C@H]2CC[C@H]4[C@]3(CC[C@]5([C@@H]4CCC5)C)C)C',\n",
    "       '[C@]12(CC[C@@H]5[C@@]([C@H]1CC[C@H]3[C@@]2(C)CC[C@H]4[C@@]3(CCC4)C)(CCCC5(C)C)C)C',\n",
    "       'CC[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCC[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCC(C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CC[C@@H](C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCC(C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCC[C@@H](C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCCC(C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCC[C@@H](C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCCCC(C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCCC[C@@H](C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "def quote(x):\n",
    "    return urllib.parse.quote(x, safe='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from rdkit import Chem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Molecule CC[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCC[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCC(C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CC[C@@H](C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCC(C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCC[C@@H](C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCCC(C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCC[C@@H](C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCCCC(C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCCC[C@@H](C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "graphs = []  # Optionally keep a list of graphs if you want them separately\n",
    "\n",
    "for smiles in smiles_list:\n",
    "    # 1) Canonicalize and sanity-check input\n",
    "    can_smiles = canonicalize_smiles(smiles)\n",
    "    is_outlier, missing_atom, missing_bond = check_input(can_smiles)\n",
    "\n",
    "    # 2) Get DataFrame of predicted BDE/BDFE for each bond\n",
    "    bde_df = predict_bdes(can_smiles, draw=True)\n",
    "    bde_df['raw_smiles'] = smiles\n",
    "\n",
    "    # 3) Deduplicate and store any extra columns you like\n",
    "    bde_df = bde_df.drop_duplicates(['fragment1', 'fragment2']).reset_index(drop=True)\n",
    "    bde_df['smiles_link'] = bde_df.molecule.apply(quote)\n",
    "\n",
    "    # 4) Build a NetworkX graph containing predicted BDE/BDFE\n",
    "    bde_graph = create_bde_graph_selective_hs(can_smiles, bde_df)\n",
    "\n",
    "    # 5) (Optional) store the graph in the DataFrame if you want\n",
    "    #    the same graph for all rows (one per entire molecule)\n",
    "    bde_df['nx_graph'] = [bde_graph] * len(bde_df)\n",
    "\n",
    "    # 6) Append to your results\n",
    "    dfs.append(bde_df)\n",
    "    graphs.append(bde_graph)   # In case you want them in parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all DataFrame results\n",
    "alfabet_results_022 = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>bond_index</th>\n",
       "      <th>graph_bde_pred</th>\n",
       "      <th>graph_bdfe_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.382645</td>\n",
       "      <td>75.711853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>H_23</td>\n",
       "      <td>25.0</td>\n",
       "      <td>100.077187</td>\n",
       "      <td>91.049133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.872467</td>\n",
       "      <td>71.412849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>H_27</td>\n",
       "      <td>29.0</td>\n",
       "      <td>97.163109</td>\n",
       "      <td>87.689636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>85.041306</td>\n",
       "      <td>70.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>H_28</td>\n",
       "      <td>30.0</td>\n",
       "      <td>95.392189</td>\n",
       "      <td>86.257256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>83.115479</td>\n",
       "      <td>66.995270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>H_30</td>\n",
       "      <td>32.0</td>\n",
       "      <td>94.518456</td>\n",
       "      <td>84.748627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>H_32</td>\n",
       "      <td>34.0</td>\n",
       "      <td>93.767822</td>\n",
       "      <td>84.237808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>86.171143</td>\n",
       "      <td>71.943581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>H_33</td>\n",
       "      <td>35.0</td>\n",
       "      <td>93.715736</td>\n",
       "      <td>84.072701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6</td>\n",
       "      <td>H_36</td>\n",
       "      <td>38.0</td>\n",
       "      <td>98.747505</td>\n",
       "      <td>89.695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>H_37</td>\n",
       "      <td>39.0</td>\n",
       "      <td>95.855804</td>\n",
       "      <td>86.941345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>H_40</td>\n",
       "      <td>42.0</td>\n",
       "      <td>94.971001</td>\n",
       "      <td>85.988342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9</td>\n",
       "      <td>H_41</td>\n",
       "      <td>43.0</td>\n",
       "      <td>86.859306</td>\n",
       "      <td>75.766243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>79.460541</td>\n",
       "      <td>64.253860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11</td>\n",
       "      <td>H_44</td>\n",
       "      <td>46.0</td>\n",
       "      <td>97.184883</td>\n",
       "      <td>88.282654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12</td>\n",
       "      <td>H_46</td>\n",
       "      <td>48.0</td>\n",
       "      <td>95.408737</td>\n",
       "      <td>86.714401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13</td>\n",
       "      <td>H_47</td>\n",
       "      <td>49.0</td>\n",
       "      <td>92.920944</td>\n",
       "      <td>84.154495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>14</td>\n",
       "      <td>H_49</td>\n",
       "      <td>51.0</td>\n",
       "      <td>89.283226</td>\n",
       "      <td>79.903214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>15.0</td>\n",
       "      <td>82.408630</td>\n",
       "      <td>67.338509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>16</td>\n",
       "      <td>H_52</td>\n",
       "      <td>54.0</td>\n",
       "      <td>98.126953</td>\n",
       "      <td>89.117233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>18</td>\n",
       "      <td>H_56</td>\n",
       "      <td>58.0</td>\n",
       "      <td>96.827782</td>\n",
       "      <td>87.840210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>19</td>\n",
       "      <td>H_59</td>\n",
       "      <td>61.0</td>\n",
       "      <td>95.608147</td>\n",
       "      <td>86.592361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20</td>\n",
       "      <td>H_61</td>\n",
       "      <td>63.0</td>\n",
       "      <td>96.082130</td>\n",
       "      <td>87.054794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>21.0</td>\n",
       "      <td>79.644073</td>\n",
       "      <td>64.361122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>22</td>\n",
       "      <td>H_62</td>\n",
       "      <td>64.0</td>\n",
       "      <td>97.762428</td>\n",
       "      <td>88.862091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     u     v  bond_index  graph_bde_pred  graph_bdfe_pred\n",
       "0    0     1         0.0       89.382645        75.711853\n",
       "1    0  H_23        25.0      100.077187        91.049133\n",
       "2    1     2         1.0       85.872467        71.412849\n",
       "3    1  H_27        29.0       97.163109        87.689636\n",
       "4    2     3         2.0       85.041306        70.000275\n",
       "5    2  H_28        30.0       95.392189        86.257256\n",
       "6    3     4         3.0       83.115479        66.995270\n",
       "7    3  H_30        32.0       94.518456        84.748627\n",
       "8    4     5         NaN             NaN              NaN\n",
       "9    4    10         NaN             NaN              NaN\n",
       "10   4  H_32        34.0       93.767822        84.237808\n",
       "11   5     6         5.0       86.171143        71.943581\n",
       "12   5     7         NaN             NaN              NaN\n",
       "13   5  H_33        35.0       93.715736        84.072701\n",
       "14   6  H_36        38.0       98.747505        89.695900\n",
       "15   7     8         NaN             NaN              NaN\n",
       "16   7  H_37        39.0       95.855804        86.941345\n",
       "17   8     9         NaN             NaN              NaN\n",
       "18   8  H_40        42.0       94.971001        85.988342\n",
       "19   9    10         NaN             NaN              NaN\n",
       "20   9    21         NaN             NaN              NaN\n",
       "21   9  H_41        43.0       86.859306        75.766243\n",
       "22  10    11        10.0       79.460541        64.253860\n",
       "23  10    12         NaN             NaN              NaN\n",
       "24  11  H_44        46.0       97.184883        88.282654\n",
       "25  12    13         NaN             NaN              NaN\n",
       "26  12  H_46        48.0       95.408737        86.714401\n",
       "27  13    14         NaN             NaN              NaN\n",
       "28  13  H_47        49.0       92.920944        84.154495\n",
       "29  14    15         NaN             NaN              NaN\n",
       "30  14    21         NaN             NaN              NaN\n",
       "31  14  H_49        51.0       89.283226        79.903214\n",
       "32  15    16        15.0       82.408630        67.338509\n",
       "33  15    17         NaN             NaN              NaN\n",
       "34  15    18         NaN             NaN              NaN\n",
       "35  16  H_52        54.0       98.126953        89.117233\n",
       "36  18    19         NaN             NaN              NaN\n",
       "37  18  H_56        58.0       96.827782        87.840210\n",
       "38  19    20         NaN             NaN              NaN\n",
       "39  19  H_59        61.0       95.608147        86.592361\n",
       "40  20    21         NaN             NaN              NaN\n",
       "41  20  H_61        63.0       96.082130        87.054794\n",
       "42  21    22        21.0       79.644073        64.361122\n",
       "43  22  H_62        64.0       97.762428        88.862091"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_to_df(graphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MoleculeEnvDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A simple Dataset that yields (graph_data, env_features, target).\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          df: a pandas DataFrame with columns:\n",
    "              [ 'nx_graph', 'temperature', 'Seawater', 'Concentration', 'Time', 'degradation_rate', ...]\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        \n",
    "        # Optionally do some numerical encoding of 'Seawater' if it is categorical\n",
    "        # or handle arbitrary env variables. Here we assume they are numeric or\n",
    "        # can be turned numeric. For example:\n",
    "        #   'Seawater' -> 1, 'freshwater' -> 0\n",
    "        # or keep them as real values if numeric.\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 1) The graph is a networkx Graph or something we can convert to PyG's Data\n",
    "        nx_graph = row['nx_graph']  # e.g. from your create_bde_graph_selective_hs()\n",
    "        # Convert it if needed to PyG Data: you might have a helper function like:\n",
    "        #   data = convert_nx_to_pyg(nx_graph)\n",
    "        # For simplicity here, we’ll just return nx_graph directly.\n",
    "        \n",
    "        # 2) Env features: \n",
    "        # Suppose we stack them in a tensor [temp, concentration, time, ...].\n",
    "        # If \"Seawater\" is categorical, encode that too.\n",
    "        # We'll do a minimal example, but adapt to your actual columns.\n",
    "        env_features = torch.tensor([\n",
    "            row['temperature'],\n",
    "            row['concentration'],\n",
    "            row['time']\n",
    "            # Possibly also handle 'Seawater' (0 or 1)\n",
    "        ], dtype=torch.float)\n",
    "        \n",
    "        # 3) The target:\n",
    "        target = torch.tensor(row['degradation_rate'], dtype=torch.float)\n",
    "        \n",
    "        return nx_graph, env_features, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EnvPositionalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Maps environment features (temperature, concentration, etc.) into a\n",
    "    learned positional embedding. Instead of sinusoidal, this is an MLP.\n",
    "    \"\"\"\n",
    "    def __init__(self, env_input_dim, d_model, hidden_dim=64):  # MLP初始化为64层\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(env_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, env_features):  \n",
    "        \"\"\"\n",
    "        env_features: (batch_size, env_input_dim)\n",
    "        Returns: (batch_size, d_model)\n",
    "        \"\"\"\n",
    "        return self.mlp(env_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class SimpleGraphModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_node_features,   # e.g. dimension of (atomic symbol) embeddings\n",
    "                 env_input_dim,       # dimension of environment features\n",
    "                 hidden_dim=128,      # 现在每个环境条件都表示为128维矢量，在进出变压器中之前，可以将其添加到分子嵌入中\n",
    "                 output_dim=1):       # for regression (1 = predicted degradation rate)\n",
    "        super().__init__()\n",
    "        \n",
    "        self.env_encoder = EnvPositionalEncoder(env_input_dim, d_model=hidden_dim)\n",
    "        \n",
    "        # Example: We embed node features up to hidden_dim\n",
    "        self.node_embedding = nn.Linear(num_node_features, hidden_dim)\n",
    "        \n",
    "        # A couple of GCNConv layers for demonstration 2层\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Final MLP to produce a single scalar\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data, env_features):\n",
    "        \"\"\"\n",
    "        data: a PyG Data object or something with:\n",
    "              data.x (node_features) shape = [num_nodes, num_node_features]\n",
    "              data.edge_index\n",
    "              data.batch if using a batch of graphs\n",
    "        env_features: (batch_size, env_input_dim) -> we map to (batch_size, hidden_dim)\n",
    "        \n",
    "        Return: predicted degradation rate shape (batch_size,)\n",
    "        \"\"\"\n",
    "        # 1) Encode environment -> positional embedding\n",
    "        batch_size = env_features.shape[0]\n",
    "        env_pos_emb = self.env_encoder(env_features)  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # 2) Node embedding\n",
    "        # data.x shape = (total_num_nodes_in_batch, num_node_features)\n",
    "        x = self.node_embedding(data.x)\n",
    "        \n",
    "        # 3) For each graph in the batch, we add the environment embedding:\n",
    "        #    We need to figure out which of the `batch_size` each node belongs to.\n",
    "        #    PyG uses data.batch to indicate the graph index for each node.\n",
    "        #    So we broadcast-add env_pos_emb to x for each node belonging to the same graph.\n",
    "        # shape of data.batch = (total_num_nodes_in_batch,)\n",
    "        \n",
    "        # Expand env_pos_emb for each node. For example:\n",
    "        x = x + env_pos_emb[data.batch]  # broadcast by indexing the correct row in env_pos_emb\n",
    "        \n",
    "        # 4) Pass through GCN layers\n",
    "        x = self.conv1(x, data.edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, data.edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # 5) Global pooling to get a single graph-level vector\n",
    "        #    shape = (batch_size, hidden_dim)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        \n",
    "        # 6) Final MLP to get predicted scalar\n",
    "        out = self.fc_out(x)  # shape (batch_size, 1)\n",
    "        \n",
    "        return out.squeeze(-1)  # shape (batch_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def train_model(model, dataloader, epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for graphs_batch, envs_batch, targets_batch in dataloader:\n",
    "            graphs_batch = graphs_batch.to(device)\n",
    "            envs_batch = envs_batch.to(device)\n",
    "            targets_batch = targets_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(graphs_batch, envs_batch)\n",
    "\n",
    "            # Compute MSE Loss\n",
    "            loss = criterion(outputs, targets_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * targets_batch.size(0)\n",
    "            total_samples += targets_batch.size(0)\n",
    "\n",
    "        avg_loss = total_loss / total_samples  # Compute average MSE loss\n",
    "\n",
    "        # === Evaluation Step ===\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds, gts = [], []\n",
    "            \n",
    "            for graphs_batch, envs_batch, targets_batch in dataloader:\n",
    "                graphs_batch = graphs_batch.to(device)\n",
    "                envs_batch = envs_batch.to(device)\n",
    "                targets_batch = targets_batch.to(device)\n",
    "\n",
    "                out = model(graphs_batch, envs_batch)\n",
    "                preds.append(out.cpu())\n",
    "                gts.append(targets_batch.cpu())\n",
    "\n",
    "            preds = torch.cat(preds).numpy()\n",
    "            gts = torch.cat(gts).numpy()\n",
    "\n",
    "            mse_val = F.mse_loss(torch.tensor(preds), torch.tensor(gts)).item()\n",
    "            rmse_val = math.sqrt(mse_val)\n",
    "            r2_val = r2_score(gts, preds)\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "def train_model(model, dataloader, device='cpu', epochs=10):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for graphs_batch, envs_batch, targets_batch in dataloader:\n",
    "            # Convert your graphs_batch into PyG data if needed\n",
    "            # e.g. data = batchify_nx_to_pyg(graphs_batch)\n",
    "            # For demonstration:\n",
    "            data = graphs_batch  # placeholder function\n",
    "            data = data.to(device)\n",
    "            \n",
    "            envs_batch = envs_batch.to(device)\n",
    "            targets_batch = targets_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data, envs_batch)  # shape = (batch_size,)\n",
    "            \n",
    "            # MSE Loss\n",
    "            loss = criterion(outputs, targets_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * targets_batch.size(0)\n",
    "            total_samples += targets_batch.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / total_samples\n",
    "        \n",
    "        # Evaluate RMSE on the same or a val set. For brevity we show same data:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = []\n",
    "            gts = []\n",
    "            for graphs_batch, envs_batch, targets_batch in dataloader:\n",
    "                data = graphs_batch\n",
    "                data = data.to(device)\n",
    "                envs_batch = envs_batch.to(device)\n",
    "                targets_batch = targets_batch.to(device)\n",
    "                \n",
    "                out = model(data, envs_batch)\n",
    "                preds.append(out.cpu())\n",
    "                gts.append(targets_batch.cpu())\n",
    "\n",
    "            preds = torch.cat(preds)\n",
    "            gts = torch.cat(gts)\n",
    "            mse_val = F.mse_loss(preds, gts)\n",
    "            rmse_val = math.sqrt(mse_val.item())\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} - Train MSE: {avg_loss:.4f}, Val RMSE: {rmse_val:.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} - Train MSE: {avg_loss:.4f}, Val RMSE: {rmse_val:.4f}, R²: {r2_val:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1023 environment rows\n",
      "   temperature  seawater  time component  concentration  degradation_rate\n",
      "0         35.6         1    30       C23             70          0.670914\n",
      "1         35.6         1    30       C24             70          0.680071\n",
      "2         35.6         1    30       C25             70          0.655230\n",
      "3         35.6         1    30       C26             70          0.625193\n",
      "4         35.6         1    30      C28a             70          0.605853\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the environmental data from Excel\n",
    "env_file = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\dataset02.xlsx\"\n",
    "env_df = pd.read_excel(env_file, engine='openpyxl')\n",
    "\n",
    "# Select only the relevant columns for the environment\n",
    "env_columns = [\"temperature\", \"seawater\", \"time\", \"component\",\"concentration\", \"degradation_rate\"]\n",
    "\n",
    "# Ensure all columns exist in the dataset\n",
    "env_var = env_df[env_columns].copy()\n",
    "\n",
    "# Convert categorical \"seawater\" to numerical (if needed)\n",
    "env_var[\"seawater\"] = env_var[\"seawater\"].map({\"sea\": 1, \"art\": 0})  # Map \"sea\" → 1, \"art\" → 0\n",
    "\n",
    "# Drop rows with missing values\n",
    "env_var = env_var.dropna().reset_index(drop=True)\n",
    "\n",
    "# Check if it matches the number of graphs\n",
    "print(f\"Loaded {len(env_var)} environment rows\")\n",
    "print(env_var.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule</th>\n",
       "      <th>bond_index</th>\n",
       "      <th>bond_type</th>\n",
       "      <th>start_atom</th>\n",
       "      <th>end_atom</th>\n",
       "      <th>fragment1</th>\n",
       "      <th>fragment2</th>\n",
       "      <th>is_valid_stereo</th>\n",
       "      <th>bde_pred</th>\n",
       "      <th>bdfe_pred</th>\n",
       "      <th>bde</th>\n",
       "      <th>bdfe</th>\n",
       "      <th>set</th>\n",
       "      <th>svg</th>\n",
       "      <th>has_dft_bde</th>\n",
       "      <th>raw_smiles</th>\n",
       "      <th>smiles_link</th>\n",
       "      <th>nx_graph</th>\n",
       "      <th>temperature</th>\n",
       "      <th>Concentration</th>\n",
       "      <th>Time</th>\n",
       "      <th>Seawater</th>\n",
       "      <th>degradation_rate</th>\n",
       "      <th>concentration</th>\n",
       "      <th>time</th>\n",
       "      <th>seawater</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...</td>\n",
       "      <td>10</td>\n",
       "      <td>C-C</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>CCCC[C@H]1[C]2CC[C@H]3C(C)(C)CCC[C@]3(C)[C@H]2...</td>\n",
       "      <td>[CH3]</td>\n",
       "      <td>True</td>\n",
       "      <td>79.460541</td>\n",
       "      <td>64.253860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;?xml version='1.0' encoding='iso-8859-1'?&gt;\\n&lt;...</td>\n",
       "      <td>False</td>\n",
       "      <td>C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...</td>\n",
       "      <td>CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...</td>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>16.893020</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125758</td>\n",
       "      <td>38.339115</td>\n",
       "      <td>98.792154</td>\n",
       "      <td>sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...</td>\n",
       "      <td>21</td>\n",
       "      <td>C-C</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>CCCC[C@@H]1[C@@H](C)CC[C@@H]2[C]3CCCC(C)(C)[C@...</td>\n",
       "      <td>[CH3]</td>\n",
       "      <td>True</td>\n",
       "      <td>79.644073</td>\n",
       "      <td>64.361122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;?xml version='1.0' encoding='iso-8859-1'?&gt;\\n&lt;...</td>\n",
       "      <td>False</td>\n",
       "      <td>C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...</td>\n",
       "      <td>CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...</td>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>37.111687</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.727786</td>\n",
       "      <td>52.868431</td>\n",
       "      <td>116.513431</td>\n",
       "      <td>sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...</td>\n",
       "      <td>15</td>\n",
       "      <td>C-C</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1[...</td>\n",
       "      <td>[CH3]</td>\n",
       "      <td>True</td>\n",
       "      <td>82.408630</td>\n",
       "      <td>67.338509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;?xml version='1.0' encoding='iso-8859-1'?&gt;\\n&lt;...</td>\n",
       "      <td>False</td>\n",
       "      <td>C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...</td>\n",
       "      <td>CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...</td>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>31.240800</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.851969</td>\n",
       "      <td>18.325284</td>\n",
       "      <td>113.183123</td>\n",
       "      <td>sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...</td>\n",
       "      <td>3</td>\n",
       "      <td>C-C</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[CH2]CCC</td>\n",
       "      <td>C[C@@H]1[CH][C@]2(C)CC[C@H]3C(C)(C)CCC[C@]3(C)...</td>\n",
       "      <td>True</td>\n",
       "      <td>83.115479</td>\n",
       "      <td>66.995270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;?xml version='1.0' encoding='iso-8859-1'?&gt;\\n&lt;...</td>\n",
       "      <td>False</td>\n",
       "      <td>C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...</td>\n",
       "      <td>CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...</td>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>14.878858</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.111461</td>\n",
       "      <td>35.933629</td>\n",
       "      <td>79.226441</td>\n",
       "      <td>sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...</td>\n",
       "      <td>2</td>\n",
       "      <td>C-C</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[CH2]CC</td>\n",
       "      <td>[CH2][C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1...</td>\n",
       "      <td>True</td>\n",
       "      <td>85.041306</td>\n",
       "      <td>70.000275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;?xml version='1.0' encoding='iso-8859-1'?&gt;\\n&lt;...</td>\n",
       "      <td>False</td>\n",
       "      <td>C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...</td>\n",
       "      <td>CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...</td>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>20.748270</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.101440</td>\n",
       "      <td>51.223863</td>\n",
       "      <td>50.751491</td>\n",
       "      <td>sea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            molecule  bond_index bond_type  \\\n",
       "0  CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...          10       C-C   \n",
       "1  CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...          21       C-C   \n",
       "2  CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...          15       C-C   \n",
       "3  CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...           3       C-C   \n",
       "4  CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...           2       C-C   \n",
       "\n",
       "   start_atom  end_atom                                          fragment1  \\\n",
       "0          10        11  CCCC[C@H]1[C]2CC[C@H]3C(C)(C)CCC[C@]3(C)[C@H]2...   \n",
       "1          21        22  CCCC[C@@H]1[C@@H](C)CC[C@@H]2[C]3CCCC(C)(C)[C@...   \n",
       "2          15        16  CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1[...   \n",
       "3           3         4                                           [CH2]CCC   \n",
       "4           2         3                                            [CH2]CC   \n",
       "\n",
       "                                           fragment2  is_valid_stereo  \\\n",
       "0                                              [CH3]             True   \n",
       "1                                              [CH3]             True   \n",
       "2                                              [CH3]             True   \n",
       "3  C[C@@H]1[CH][C@]2(C)CC[C@H]3C(C)(C)CCC[C@]3(C)...             True   \n",
       "4  [CH2][C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1...             True   \n",
       "\n",
       "    bde_pred  bdfe_pred  bde  bdfe  set  \\\n",
       "0  79.460541  64.253860  NaN   NaN  NaN   \n",
       "1  79.644073  64.361122  NaN   NaN  NaN   \n",
       "2  82.408630  67.338509  NaN   NaN  NaN   \n",
       "3  83.115479  66.995270  NaN   NaN  NaN   \n",
       "4  85.041306  70.000275  NaN   NaN  NaN   \n",
       "\n",
       "                                                 svg  has_dft_bde  \\\n",
       "0  <?xml version='1.0' encoding='iso-8859-1'?>\\n<...        False   \n",
       "1  <?xml version='1.0' encoding='iso-8859-1'?>\\n<...        False   \n",
       "2  <?xml version='1.0' encoding='iso-8859-1'?>\\n<...        False   \n",
       "3  <?xml version='1.0' encoding='iso-8859-1'?>\\n<...        False   \n",
       "4  <?xml version='1.0' encoding='iso-8859-1'?>\\n<...        False   \n",
       "\n",
       "                                          raw_smiles  \\\n",
       "0  C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...   \n",
       "1  C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...   \n",
       "2  C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...   \n",
       "3  C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...   \n",
       "4  C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...   \n",
       "\n",
       "                                         smiles_link  \\\n",
       "0  CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...   \n",
       "1  CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...   \n",
       "2  CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...   \n",
       "3  CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...   \n",
       "4  CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...   \n",
       "\n",
       "                                            nx_graph  temperature  \\\n",
       "0  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...    16.893020   \n",
       "1  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...    37.111687   \n",
       "2  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...    31.240800   \n",
       "3  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...    14.878858   \n",
       "4  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...    20.748270   \n",
       "\n",
       "   Concentration  Time  Seawater  degradation_rate  concentration        time  \\\n",
       "0             70    30         1          0.125758      38.339115   98.792154   \n",
       "1             70    30         1          0.727786      52.868431  116.513431   \n",
       "2             70    30         1          0.851969      18.325284  113.183123   \n",
       "3             70    30         1          0.111461      35.933629   79.226441   \n",
       "4             70    30         1          0.101440      51.223863   50.751491   \n",
       "\n",
       "  seawater  \n",
       "0      sea  \n",
       "1      sea  \n",
       "2      sea  \n",
       "3      sea  \n",
       "4      sea  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Assuming the environment variable\n",
    "\n",
    "# 1) Add random environment columns for demonstration\n",
    "num_rows = len(alfabet_results_022)\n",
    "\n",
    "# Temperatures between 10°C and 40°C\n",
    "alfabet_results_022['temperature'] = np.random.uniform(10, 40, size=num_rows)\n",
    "\n",
    "# Concentration in mg/L, random 1–100\n",
    "alfabet_results_022['concentration'] = np.random.uniform(1, 100, size=num_rows)\n",
    "\n",
    "# Time in hours, random 0–120\n",
    "alfabet_results_022['time'] = np.random.uniform(0, 120, size=num_rows)\n",
    "\n",
    "# Categorical 'Seawater' vs 'fresh' environment\n",
    "alfabet_results_022['seawater'] = np.random.choice(['sea', 'fresh'], size=num_rows)\n",
    "\n",
    "# And a random target: 'degradation_rate' (arbitrary range)\n",
    "alfabet_results_022['degradation_rate'] = np.random.uniform(0.1, 1.0, size=num_rows)\n",
    "\n",
    "# 2) Inspect the updated DataFrame\n",
    "alfabet_results_022.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit alfabet_results_022\n",
    "num_rows = len(alfabet_results_022)\n",
    "if len(env_var) < num_rows:\n",
    "    raise ValueError(\"error\")\n",
    "\n",
    "# instead \n",
    "alfabet_results_022[\"temperature\"] = env_var[\"temperature\"][:num_rows].values\n",
    "alfabet_results_022[\"seawater\"] = env_var[\"seawater\"][:num_rows].values\n",
    "alfabet_results_022[\"time\"] = env_var[\"time\"][:num_rows].values\n",
    "alfabet_results_022[\"concentration\"] = env_var[\"concentration\"][:num_rows].values\n",
    "alfabet_results_022[\"degradation_rate\"] = env_var[\"degradation_rate\"][:num_rows].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MoleculeEnvDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.features = [\"temperature\", \"seawater\", \"time\", \"concentration\"]\n",
    "        self.target = \"degradation_rate\"\n",
    "\n",
    "        # 确保列是数值类型\n",
    "        self.data[self.features] = self.data[self.features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "        self.data[self.target] = pd.to_numeric(self.data[self.target], errors=\"coerce\")\n",
    "\n",
    "        # 检查 NaN 之前的数据情况\n",
    "        print(\"Before dropna:\", self.data.shape)\n",
    "\n",
    "        # 删除包含 NaN 的行\n",
    "        self.data = self.data.dropna().reset_index(drop=True)\n",
    "\n",
    "        # 确保数据集不是空的\n",
    "        if len(self.data) == 0:\n",
    "            raise ValueError(\"❌ 所有数据都被 dropna() 删除了，请检查数据格式！\")\n",
    "\n",
    "        print(\"After dropna:\", self.data.shape)  # 重新检查数据\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        x = torch.tensor(row[self.features].astype(float).values, dtype=torch.float32)\n",
    "        y = torch.tensor(row[self.target].astype(float), dtype=torch.float32)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['molecule', 'bond_index', 'bond_type', 'start_atom', 'end_atom', 'fragment1', 'fragment2', 'is_valid_stereo', 'bde_pred', 'bdfe_pred', 'bde', 'bdfe', 'set', 'svg', 'has_dft_bde', 'raw_smiles', 'smiles_link', 'nx_graph', 'temperature', 'Concentration', 'Time', 'Seawater', 'degradation_rate', 'concentration', 'time', 'seawater']\n"
     ]
    }
   ],
   "source": [
    "print(alfabet_results_022.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MoleculeEnvDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.reset_index(drop=True)  # 确保索引连续\n",
    "        self.features = [\"temperature\", \"seawater\", \"time\", \"concentration\"]  # 需要的特征列\n",
    "        self.target = \"degradation_rate\"  # 目标变量\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # 这个方法必须实现，DataLoader 需要它\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        x = torch.tensor(row[self.features].values, dtype=torch.float32)  # 转换为 Tensor\n",
    "        y = torch.tensor(row[self.target], dtype=torch.float32)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MoleculeEnvDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.reset_index(drop=True)  # 确保索引连续\n",
    "        self.features = [\"temperature\", \"seawater\", \"time\", \"concentration\"]  # 需要的特征列\n",
    "        self.target = \"degradation_rate\"\n",
    "\n",
    "        # **强制转换列为数值类型**\n",
    "        self.data[self.features] = self.data[self.features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "        self.data[self.target] = pd.to_numeric(self.data[self.target], errors=\"coerce\")\n",
    "\n",
    "        # **删除包含 NaN 的行**\n",
    "        self.data = self.data.dropna().reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        x = torch.tensor(row[self.features].astype(float).values, dtype=torch.float32)  # 转换为 float\n",
    "        y = torch.tensor(row[self.target].astype(float), dtype=torch.float32)  # 目标变量\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def my_collate(batch):\n",
    "    nx_list   = [b[0] for b in batch]  # 取 `nx_graph`\n",
    "    env_list  = [b[1] for b in batch]  # 取 `env_features`\n",
    "    tgt_list  = [b[2] for b in batch]  # 取 `target`\n",
    "\n",
    "    return nx_list, default_collate(env_list), default_collate(tgt_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    row = self.df.iloc[idx]\n",
    "    \n",
    "    nx_graph = row['nx_graph']\n",
    "    print(f\"Graph {idx}: {nx_graph}\")  # Debug 检查图数据\n",
    "\n",
    "    env_features = torch.tensor([\n",
    "        row['temperature'],\n",
    "        row['concentration'],\n",
    "        row['time']\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "    target = torch.tensor(row['degradation_rate'], dtype=torch.float)\n",
    "\n",
    "    return nx_graph, env_features, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 测试 DataLoader\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch_x\u001b[38;5;241m.\u001b[39mshape, batch_y\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# 确保数据格式正确\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[97], line 20\u001b[0m, in \u001b[0;36mMoleculeEnvDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     19\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[1;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(row[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     22\u001b[0m     env \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(row[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_features]\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# 额外的环境变量\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "dataset = MoleculeEnvDataset(alfabet_results_022)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# 测试 DataLoader\n",
    "for batch_x, batch_y in dataloader:\n",
    "    print(batch_x.shape, batch_y.shape)  # 确保数据格式正确\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeEnvDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.features = [\"temperature\", \"seawater\", \"time\", \"concentration\"]  # 只用小写\n",
    "        self.target = \"degradation_rate\"\n",
    "\n",
    "        # 确保所有列是数值类型\n",
    "        self.data[self.features] = self.data[self.features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "        self.data[self.target] = pd.to_numeric(self.data[self.target], errors=\"coerce\")\n",
    "\n",
    "        # 处理 NaN\n",
    "        self.data = self.data.dropna(subset=self.features + [self.target]).reset_index(drop=True)\n",
    "\n",
    "        # 确保数据集不是空的\n",
    "        if len(self.data) == 0:\n",
    "            raise ValueError(\"❌ 所有数据都被 dropna() 删除了，请检查数据！\")\n",
    "\n",
    "        print(\"\\n数据清理后大小:\", self.data.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        x = torch.tensor(row[self.features].astype(float).values, dtype=torch.float32)\n",
    "        y = torch.tensor(row[self.target].astype(float), dtype=torch.float32)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数据清理后大小: (676, 26)\n",
      "torch.Size([8, 4]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "dataset = MoleculeEnvDataset(alfabet_results_022)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "for batch_x, batch_y in dataloader:\n",
    "    print(batch_x.shape, batch_y.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[86.0000,  1.0000, 60.0000, 70.0000],\n",
      "        [86.0000,  0.0000, 30.0000, 70.0000],\n",
      "        [35.6000,  1.0000, 30.0000, 70.0000],\n",
      "        [35.6000,  1.0000, 30.0000, 70.0000],\n",
      "        [86.0000,  1.0000, 60.0000, 70.0000],\n",
      "        [35.6000,  1.0000, 60.0000, 70.0000],\n",
      "        [86.0000,  1.0000, 30.0000, 70.0000],\n",
      "        [86.0000,  1.0000, 60.0000, 70.0000]]) tensor([0.9552, 0.7592, 0.4557, 0.5921, 1.3140, 0.8380, 0.8556, 0.8295])\n"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in dataloader:\n",
    "    print(batch_x, batch_y)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeEnvDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.features = [\"temperature\", \"seawater\", \"time\", \"concentration\"]\n",
    "        self.target = \"degradation_rate\"\n",
    "\n",
    "        # 确保所有列是数值类型\n",
    "        for col in self.features + [self.target]:\n",
    "            self.data[col] = pd.to_numeric(self.data[col], errors=\"coerce\")  # 强制转换为数值\n",
    "\n",
    "        # 处理 NaN\n",
    "        self.data = self.data.dropna(subset=self.features + [self.target]).reset_index(drop=True)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        x = torch.tensor(row[self.features].astype(float).values, dtype=torch.float32)\n",
    "        y = torch.tensor(row[self.target].astype(float), dtype=torch.float32)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    row = self.data.iloc[idx]\n",
    "    print(\"Row data:\", row[self.features])  # 检查数据格式\n",
    "    print(\"Data types:\", row[self.features].dtypes)\n",
    "\n",
    "    x = torch.tensor(row[self.features].astype(float).values, dtype=torch.float32)\n",
    "    y = torch.tensor(row[self.target].astype(float), dtype=torch.float32)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      2\u001b[0m     batch_x, batch_y \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device), batch_y\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# 如果使用 GPU\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(batch_x)  \u001b[38;5;66;03m# 运行你的模型\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[97], line 20\u001b[0m, in \u001b[0;36mMoleculeEnvDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     19\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[1;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(row[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     22\u001b[0m     env \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(row[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_features]\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# 额外的环境变量\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in dataloader:\n",
    "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # 如果使用 GPU\n",
    "    output = model(batch_x)  # 运行你的模型\n",
    "    loss = loss_fn(output, batch_y)  # 计算损失\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1)  # 假设是一个简单的全连接层\n",
    "\n",
    "    def forward(self, x):  # 只接受 x\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      2\u001b[0m     batch_x, batch_y \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device), batch_y\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# 送入 GPU\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(batch_x)  \u001b[38;5;66;03m# 只传 batch_x\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[88], line 59\u001b[0m, in \u001b[0;36mmy_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     57\u001b[0m nx_list   \u001b[38;5;241m=\u001b[39m [b[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m     58\u001b[0m env_list  \u001b[38;5;241m=\u001b[39m [b[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m---> 59\u001b[0m tgt_list  \u001b[38;5;241m=\u001b[39m [b[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Convert Nx -> PyG\u001b[39;00m\n\u001b[0;32m     62\u001b[0m pyg_list \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[88], line 59\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     57\u001b[0m nx_list   \u001b[38;5;241m=\u001b[39m [b[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m     58\u001b[0m env_list  \u001b[38;5;241m=\u001b[39m [b[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m---> 59\u001b[0m tgt_list  \u001b[38;5;241m=\u001b[39m [\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Convert Nx -> PyG\u001b[39;00m\n\u001b[0;32m     62\u001b[0m pyg_list \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in dataloader:\n",
    "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # 送入 GPU\n",
    "    output = model(batch_x)  # 只传 batch_x\n",
    "    loss = loss_fn(output, batch_y)  # 计算损失\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def my_collate(batch):\n",
    "    batch_x = [item[0] for item in batch]  # 提取 X\n",
    "    batch_y = [item[1] for item in batch]  # 提取 Y\n",
    "    return default_collate(batch_x), default_collate(batch_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数据清理后大小: (676, 26)\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "dataset = MoleculeEnvDataset(alfabet_results_022)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=my_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 11\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleGraphModel(num_node_features\u001b[38;5;241m=\u001b[39mnum_node_features,\n\u001b[0;32m      6\u001b[0m                          env_input_dim\u001b[38;5;241m=\u001b[39menv_input_dim,\n\u001b[0;32m      7\u001b[0m                          hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m      8\u001b[0m                          output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 4) Train\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[66], line 70\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, device, epochs)\u001b[0m\n\u001b[0;32m     67\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     68\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m graphs_batch, envs_batch, targets_batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# Convert your graphs_batch into PyG data if needed\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# e.g. data = batchify_nx_to_pyg(graphs_batch)\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# For demonstration:\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     data \u001b[38;5;241m=\u001b[39m graphs_batch  \u001b[38;5;66;03m# placeholder function\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the model\n",
    "#    For demonstration, suppose node features = 1 (maybe a 1-hot or embedding dimension)\n",
    "num_node_features = 1\n",
    "env_input_dim = 3  # e.g. [temp, concentration, time], or more if you encode \"Seawater\" etc\n",
    "model = SimpleGraphModel(num_node_features=num_node_features,\n",
    "                         env_input_dim=env_input_dim,\n",
    "                         hidden_dim=128,\n",
    "                         output_dim=1)\n",
    "\n",
    "# 4) Train\n",
    "train_model(model, dataloader, device='cpu', epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, MSE: 0.7475, RMSE: 0.8646\n",
      "Epoch 2/5, MSE: 0.0570, RMSE: 0.2387\n",
      "Epoch 3/5, MSE: 0.0602, RMSE: 0.2454\n",
      "Epoch 4/5, MSE: 0.0897, RMSE: 0.2995\n",
      "Epoch 5/5, MSE: 0.0475, RMSE: 0.2180\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "dummy_g = nx.Graph()\n",
    "dummy_g.add_nodes_from([0, 1, 2])  # trivial 3-node graph\n",
    "dummy_g.add_edges_from([(0,1), (1,2)])  # 2 edges\n",
    "\n",
    "alfabet_results_022['nx_graph'] = [dummy_g]*num_rows\n",
    "\n",
    "########################################\n",
    "# 3) Define the Dataset class\n",
    "########################################\n",
    "class MoleculeEnvDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Yields (nx_graph, env_tensor, target). In practice, you’d convert the Nx graph\n",
    "    to a PyG Data object in the collate function or here.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        # For example, encode 'Seawater' as 1.0 for 'sea' and 0.0 for 'fresh'\n",
    "        self.water_map = {'sea': 1.0, 'fresh': 0.0}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Nx graph for this row (toy example: they’re all the same)\n",
    "        nx_graph = row['nx_graph']\n",
    "\n",
    "        # Build environment tensor. For demonstration, we use [temp, conc, time]\n",
    "        # If you want to include 'Seawater' too, just do so below.\n",
    "        env_features = torch.tensor([\n",
    "            row['temperature'],\n",
    "            row['Concentration'],\n",
    "            row['Time']\n",
    "            # or self.water_map.get(row['Seawater'], 0.0) if you want\n",
    "        ], dtype=torch.float)\n",
    "\n",
    "        # Regression target\n",
    "        target = torch.tensor(row['degradation_rate'], dtype=torch.float)\n",
    "\n",
    "        return nx_graph, env_features, target\n",
    "\n",
    "########################################\n",
    "# 4) Define a collate_fn to handle Nx -> PyG\n",
    "########################################\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (nx_graph, env_tensor, target)\n",
    "    \"\"\"\n",
    "    nx_list   = [b[0] for b in batch]\n",
    "    env_list  = [b[1] for b in batch]\n",
    "    tgt_list  = [b[2] for b in batch]\n",
    "\n",
    "    # Convert Nx -> PyG\n",
    "    pyg_list = []\n",
    "    for g in nx_list:\n",
    "        pyg_data = from_networkx(g)\n",
    "        # Minimal node feature: each node just has 1 feature = node index\n",
    "        # (In reality, you'd embed atomic symbols, etc.)\n",
    "        x = []\n",
    "        for node_idx in range(pyg_data.num_nodes):\n",
    "            x.append([float(node_idx)])  # toy\n",
    "        pyg_data.x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        pyg_list.append(pyg_data)\n",
    "\n",
    "    # Combine into a single batch\n",
    "    pyg_batch = Batch.from_data_list(pyg_list)\n",
    "\n",
    "    # Stack environment + targets\n",
    "    env_batch = torch.stack(env_list, dim=0)\n",
    "    tgt_batch = torch.stack(tgt_list, dim=0)\n",
    "\n",
    "    return pyg_batch, env_batch, tgt_batch\n",
    "\n",
    "########################################\n",
    "# 5) Instantiate the dataset / dataloader\n",
    "########################################\n",
    "dataset = MoleculeEnvDataset(alfabet_results_022)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=my_collate)\n",
    "\n",
    "########################################\n",
    "# 6) Build a simple GNN model\n",
    "########################################\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# MLP that maps env_features -> embedding\n",
    "class EnvPositionalEncoder(nn.Module):\n",
    "    def __init__(self, env_input_dim, d_model, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(env_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, d_model)\n",
    "        )\n",
    "    def forward(self, env):\n",
    "        return self.mlp(env)\n",
    "\n",
    "# A small GCN-based model\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class SimpleGraphModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_node_features,\n",
    "                 env_input_dim,\n",
    "                 hidden_dim=128,\n",
    "                 output_dim=1):\n",
    "        super().__init__()\n",
    "        self.env_encoder = EnvPositionalEncoder(env_input_dim, d_model=hidden_dim)\n",
    "        self.node_encoder = nn.Linear(num_node_features, hidden_dim)\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, data, env):\n",
    "        # data: PyG batch\n",
    "        # env: (batch_size, env_input_dim)\n",
    "        env_emb = self.env_encoder(env)  # => (B, hidden_dim)\n",
    "\n",
    "        x = self.node_encoder(data.x)    # => (num_nodes, hidden_dim)\n",
    "        x = x + env_emb[data.batch]      # broadcast env to each node\n",
    "\n",
    "        x = F.relu(self.conv1(x, data.edge_index))\n",
    "        x = F.relu(self.conv2(x, data.edge_index))\n",
    "\n",
    "        # Global average pooling => (batch_size, hidden_dim)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "\n",
    "        # Output => (batch_size, 1)\n",
    "        return self.fc_out(x).squeeze(-1)  # => (batch_size,)\n",
    "\n",
    "########################################\n",
    "# 7) Instantiate + Train\n",
    "########################################\n",
    "model = SimpleGraphModel(\n",
    "    num_node_features=1,  # we only gave each node a single feature in my_collate\n",
    "    env_input_dim=3,      # [temp, concentration, time]\n",
    "    hidden_dim=64,\n",
    "    output_dim=1\n",
    ")\n",
    "\n",
    "device = 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for pyg_batch, env_batch, tgt_batch in dataloader:\n",
    "        pyg_batch, env_batch, tgt_batch = (\n",
    "            pyg_batch.to(device),\n",
    "            env_batch.to(device),\n",
    "            tgt_batch.to(device)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(pyg_batch, env_batch)\n",
    "        loss = criterion(preds, tgt_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * tgt_batch.size(0)\n",
    "        total_samples += tgt_batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "\n",
    "    # For a toy example, we'll just print training loss each epoch\n",
    "    print(f\"Epoch {epoch}/{epochs}, MSE: {avg_loss:.4f}, RMSE: {math.sqrt(avg_loss):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfabet-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
