{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/NREL/alfabet.git@0.2.2\n",
      "  Cloning https://github.com/NREL/alfabet.git (to revision 0.2.2) to c:\\users\\80710\\appdata\\local\\temp\\pip-req-build-w3k8arnm\n",
      "  Resolved https://github.com/NREL/alfabet.git to commit 9942cbd6fceeed549e8126692b15bb135e103f5a\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from alfabet==0.2.2) (2.2.3)\n",
      "Requirement already satisfied: nfp>=0.3.6 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from alfabet==0.2.2) (0.3.12)\n",
      "Requirement already satisfied: tqdm in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from alfabet==0.2.2) (4.66.5)\n",
      "Requirement already satisfied: pooch in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from alfabet==0.2.2) (1.8.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from alfabet==0.2.2) (1.4.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from alfabet==0.2.2) (0.24.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from nfp>=0.3.6->alfabet==0.2.2) (1.26.4)\n",
      "Requirement already satisfied: networkx>2.0 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from nfp>=0.3.6->alfabet==0.2.2) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from pandas->alfabet==0.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from pandas->alfabet==0.2.2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from pandas->alfabet==0.2.2) (2024.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from pooch->alfabet==0.2.2) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from pooch->alfabet==0.2.2) (24.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from pooch->alfabet==0.2.2) (2.32.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from scikit-learn->alfabet==0.2.2) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from scikit-learn->alfabet==0.2.2) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from tqdm->alfabet==0.2.2) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->alfabet==0.2.2) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from requests>=2.19.0->pooch->alfabet==0.2.2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from requests>=2.19.0->pooch->alfabet==0.2.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from requests>=2.19.0->pooch->alfabet==0.2.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages (from requests>=2.19.0->pooch->alfabet==0.2.2) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/NREL/alfabet.git 'C:\\Users\\80710\\AppData\\Local\\Temp\\pip-req-build-w3k8arnm'\n",
      "  Running command git checkout -q 9942cbd6fceeed549e8126692b15bb135e103f5a\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/NREL/alfabet.git@0.2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alfabet.drawing import draw_mol_outlier\n",
    "from alfabet.fragment import canonicalize_smiles\n",
    "from alfabet.neighbors import find_neighbor_bonds\n",
    "from alfabet.prediction import predict_bdes, check_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import alfabet\n",
    "alfabet.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024.03.5'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdkit.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "\n",
    "def create_bde_graph_selective_hs(smiles: str, bde_df) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Build a NetworkX graph from the *original (heavy-atom)* RDKit Mol:\n",
    "      - Keep all heavy-atom ring & skeleton bonds from the SMILES.\n",
    "      - Add new H-X bonds (i.e., only the hydrogens needed) when a row in bde_df indicates\n",
    "        a predicted bond that doesn't already exist in the heavy-atom Mol.\n",
    "    \n",
    "    bde_df is expected to have columns:\n",
    "       - start_atom, end_atom: integer indexes or placeholders\n",
    "       - bde_pred, bdfe_pred, etc.: predicted data for each bond\n",
    "       - possibly bond_index (optional)\n",
    "    \n",
    "    Steps:\n",
    "       1) Parse the SMILES without adding Hs (just once).\n",
    "       2) Build a base Nx graph with all heavy-atom nodes & edges.\n",
    "       3) Iterate over bde_df. If the row corresponds to an existing heavy–heavy bond,\n",
    "          update the Nx edge with predicted data. If the row corresponds to an H–X bond,\n",
    "          add the H node + edge and store the predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Parse the SMILES into an RDKit Mol (no AddHs)\n",
    "    base_mol = Chem.MolFromSmiles(smiles)\n",
    "    if base_mol is None:\n",
    "        # Handle parse error, e.g. return empty graph\n",
    "        return nx.Graph()\n",
    "\n",
    "    # 2. Create an Nx graph, optionally store the RDKit Mol for reference\n",
    "    G = nx.Graph(mol=base_mol)\n",
    "\n",
    "    # 3. Add heavy-atom nodes\n",
    "    #    We'll store:\n",
    "    #      - 'symbol': e.g. 'C', 'O', 'N', etc.\n",
    "    #      - 'rdkit_idx': the integer index assigned by RDKit\n",
    "    #    Feel free to store other attributes as well.\n",
    "    for atom in base_mol.GetAtoms():\n",
    "        atom_idx = atom.GetIdx()\n",
    "        G.add_node(atom_idx, \n",
    "                   symbol=atom.GetSymbol(),\n",
    "                   rdkit_idx=atom_idx)\n",
    "\n",
    "    # 4. Add edges for all heavy-atom bonds in the original (no-H) Mol\n",
    "    #    We won't attach any BDE predictions yet (set them to None).\n",
    "    #    We'll also store a default bond_index=None if desired.\n",
    "    for bond in base_mol.GetBonds():\n",
    "        a1 = bond.GetBeginAtomIdx()\n",
    "        a2 = bond.GetEndAtomIdx()\n",
    "        G.add_edge(a1, a2,\n",
    "                   bond_index=None,\n",
    "                   bde_pred=None,\n",
    "                   bdfe_pred=None)\n",
    "\n",
    "    # 5. Iterate over bde_df.  We'll assume the columns are something like:\n",
    "    #     start_atom, end_atom, bde_pred, bdfe_pred, bond_index, etc.\n",
    "    #    - For heavy–heavy predictions, update the existing edge with predicted data.\n",
    "    #    - For H–X predictions, add the new hydrogen node & edge if not present.\n",
    "    #    - This approach assumes that for an H–X bond, either start_atom or end_atom\n",
    "    #      is a placeholder for hydrogen or an integer representing \"H\" in your dataset.\n",
    "    for _, row in bde_df.iterrows():\n",
    "        s = row['start_atom']\n",
    "        e = row['end_atom']\n",
    "        \n",
    "        # Attempt to interpret s and e in the context of the base mol\n",
    "        # We'll use a simple rule:\n",
    "        #  - If the index is >= base_mol.GetNumAtoms(), treat it as \"this is a hydrogen\"\n",
    "        #  - Or you could have a special marker like -1 for hydrogen\n",
    "        #    (depends on how your data is structured)\n",
    "        \n",
    "        # We also store predicted data\n",
    "        bde_pred_value = row.get('bde_pred', None)\n",
    "        bdfe_pred_value = row.get('bdfe_pred', None)\n",
    "        bond_index_value = row.get('bond_index', None)\n",
    "        \n",
    "        # Convert them to integers if needed\n",
    "        # (In practice, you may need to handle missing or invalid indexes carefully)\n",
    "        \n",
    "        # We'll define a helper function to check if an index is \"heavy\" or \"hydrogen\"\n",
    "        def is_heavy(idx):\n",
    "            return (0 <= idx < base_mol.GetNumAtoms())\n",
    "        \n",
    "        # Determine the \"types\" of s and e\n",
    "        s_is_heavy = is_heavy(s)\n",
    "        e_is_heavy = is_heavy(e)\n",
    "\n",
    "        if s_is_heavy and e_is_heavy:\n",
    "            # This is a heavy–heavy bond.\n",
    "            # If it already exists in G, update attributes.\n",
    "            if G.has_edge(s, e):\n",
    "                # Just update the existing edge\n",
    "                G[s][e]['bde_pred'] = bde_pred_value\n",
    "                G[s][e]['bdfe_pred'] = bdfe_pred_value\n",
    "                G[s][e]['bond_index'] = bond_index_value\n",
    "            else:\n",
    "                # Possibly -?> no, not possible the bond doesn't exist in the original skeleton \n",
    "                # (this can happen if the SMILES didn't have it).\n",
    "                # Add it as a new edge. This is unusual, but let's handle it anyway.\n",
    "                G.add_edge(s, e,\n",
    "                           bond_index=bond_index_value,\n",
    "                           bde_pred=bde_pred_value,\n",
    "                           bdfe_pred=bdfe_pred_value)\n",
    "\n",
    "        else:\n",
    "            # At least one of them is a \"hydrogen\" or out-of-range index\n",
    "            # We'll figure out which one is the heavy atom and which is the hydrogen.\n",
    "            if s_is_heavy and not e_is_heavy:\n",
    "                heavy_idx, hydrogen_idx = s, e\n",
    "            elif e_is_heavy and not s_is_heavy:\n",
    "                heavy_idx, hydrogen_idx = e, s\n",
    "            else:\n",
    "                # Both are hydrogens or out-of-range, which might be invalid.\n",
    "                # For safety, just skip or handle error.\n",
    "                # Could print a warning, raise an exception, etc.\n",
    "                continue\n",
    "\n",
    "            # Step 1: ensure the hydrogen node is present in G\n",
    "            # We'll generate a unique node key for the H, e.g. \"H_{hydrogen_idx}\"\n",
    "            # or something that won't collide with integer-based heavy nodes.\n",
    "            # You could also store the actual integer if your system allows it.\n",
    "            h_node = f\"H_{hydrogen_idx}\"\n",
    "            if not G.has_node(h_node):\n",
    "                # Add the hydrogen node with minimal attributes\n",
    "                G.add_node(h_node,\n",
    "                           symbol='H',\n",
    "                           rdkit_idx=None)  # or some other placeholder\n",
    "\n",
    "            # Step 2: add the H–X bond or update if it already exists\n",
    "            # The heavy_idx is the integer from RDKit.\n",
    "            if not G.has_edge(heavy_idx, h_node):\n",
    "                G.add_edge(heavy_idx, h_node,\n",
    "                           bond_index=bond_index_value,\n",
    "                           bde_pred=bde_pred_value,\n",
    "                           bdfe_pred=bdfe_pred_value)\n",
    "            else:\n",
    "                # If it somehow exists, just update attributes\n",
    "                G[heavy_idx][h_node]['bde_pred'] = bde_pred_value\n",
    "                G[heavy_idx][h_node]['bdfe_pred'] = bdfe_pred_value\n",
    "                G[heavy_idx][h_node]['bond_index'] = bond_index_value\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_to_df(bde_graph: nx.Graph) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the edges of bde_graph into a DataFrame with columns:\n",
    "      ['u', 'v', 'bond_index', 'graph_bde_pred', 'graph_bdfe_pred'].\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for u, v, data in bde_graph.edges(data=True):\n",
    "        rows.append({\n",
    "            'u': u,\n",
    "            'v': v,\n",
    "            'bond_index': data['bond_index'],\n",
    "            'graph_bde_pred': data.get('bde_pred', None),\n",
    "            'graph_bdfe_pred': data.get('bdfe_pred', None)\n",
    "        })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list = ['C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CCCC)(C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CCC(C)C)(C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CC[C@@H](C)CC)(C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CC[C@H](CCC)C)(C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@]2(C)CC3)CC[C@H](C)CCCCC)(C)C',\n",
    "       'C(CCC)C[C@H](C)CC[C@@H]1[C@H](CC[C@H]2[C@]1(CC[C@@H]3[C@@]2(CCCC3(C)C)C)C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CC[C@@H](CCCC(C)C)C)(C)C',\n",
    "       'C(C[C@@H](CC[C@H]1[C@]3([C@H](CC[C@@H]1C)[C@]2(CCCC(C)(C)[C@@H]2CC3)C)C)C)CC(C)C',\n",
    "       '[C@]23(CC[C@@H]1[C@@](CCCC1(C)C)(C)[C@H]2CC[C@H]4[C@]3(CC[C@]5([C@@H]4CCC5)C)C)C',\n",
    "       '[C@]12(CC[C@@H]5[C@@]([C@H]1CC[C@H]3[C@@]2(C)CC[C@H]4[C@@]3(CCC4)C)(CCCC5(C)C)C)C',\n",
    "       'CC[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCC[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCC(C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CC[C@@H](C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCC(C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCC[C@@H](C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCCC(C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCC[C@@H](C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCCCC(C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCCC[C@@H](C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "def quote(x):\n",
    "    return urllib.parse.quote(x, safe='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Molecule CC[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCC[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCC(C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CC[C@@H](C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCC(C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCC[C@@H](C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCCC(C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCC[C@@H](C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCCCC(C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCCC[C@@H](C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "graphs = []  # Optionally keep a list of graphs if you want them separately\n",
    "\n",
    "for smiles in smiles_list:\n",
    "    # 1) Canonicalize and sanity-check input\n",
    "    can_smiles = canonicalize_smiles(smiles)\n",
    "    is_outlier, missing_atom, missing_bond = check_input(can_smiles)\n",
    "\n",
    "    # 2) Get DataFrame of predicted BDE/BDFE for each bond\n",
    "    bde_df = predict_bdes(can_smiles, draw=True)\n",
    "    bde_df['raw_smiles'] = smiles\n",
    "\n",
    "    # 3) Deduplicate and store any extra columns you like\n",
    "    bde_df = bde_df.drop_duplicates(['fragment1', 'fragment2']).reset_index(drop=True)\n",
    "    bde_df['smiles_link'] = bde_df.molecule.apply(quote)\n",
    "\n",
    "    # 4) Build a NetworkX graph containing predicted BDE/BDFE\n",
    "    bde_graph = create_bde_graph_selective_hs(can_smiles, bde_df)\n",
    "\n",
    "    # 5) (Optional) store the graph in the DataFrame if you want\n",
    "    #    the same graph for all rows (one per entire molecule)\n",
    "    bde_df['nx_graph'] = [bde_graph] * len(bde_df)\n",
    "\n",
    "    # 6) Append to your results\n",
    "    dfs.append(bde_df)\n",
    "    graphs.append(bde_graph)   # In case you want them in parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all DataFrame results\n",
    "alfabet_results_022 = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>bond_index</th>\n",
       "      <th>graph_bde_pred</th>\n",
       "      <th>graph_bdfe_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.382645</td>\n",
       "      <td>75.711853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>H_23</td>\n",
       "      <td>25.0</td>\n",
       "      <td>100.077187</td>\n",
       "      <td>91.049133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.872467</td>\n",
       "      <td>71.412849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>H_27</td>\n",
       "      <td>29.0</td>\n",
       "      <td>97.163109</td>\n",
       "      <td>87.689636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>85.041306</td>\n",
       "      <td>70.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>H_28</td>\n",
       "      <td>30.0</td>\n",
       "      <td>95.392189</td>\n",
       "      <td>86.257256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>83.115479</td>\n",
       "      <td>66.995270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>H_30</td>\n",
       "      <td>32.0</td>\n",
       "      <td>94.518456</td>\n",
       "      <td>84.748627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>H_32</td>\n",
       "      <td>34.0</td>\n",
       "      <td>93.767822</td>\n",
       "      <td>84.237808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>86.171143</td>\n",
       "      <td>71.943581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>H_33</td>\n",
       "      <td>35.0</td>\n",
       "      <td>93.715736</td>\n",
       "      <td>84.072701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6</td>\n",
       "      <td>H_36</td>\n",
       "      <td>38.0</td>\n",
       "      <td>98.747505</td>\n",
       "      <td>89.695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>H_37</td>\n",
       "      <td>39.0</td>\n",
       "      <td>95.855804</td>\n",
       "      <td>86.941345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>H_40</td>\n",
       "      <td>42.0</td>\n",
       "      <td>94.971001</td>\n",
       "      <td>85.988342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9</td>\n",
       "      <td>H_41</td>\n",
       "      <td>43.0</td>\n",
       "      <td>86.859306</td>\n",
       "      <td>75.766243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>79.460541</td>\n",
       "      <td>64.253860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11</td>\n",
       "      <td>H_44</td>\n",
       "      <td>46.0</td>\n",
       "      <td>97.184883</td>\n",
       "      <td>88.282654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12</td>\n",
       "      <td>H_46</td>\n",
       "      <td>48.0</td>\n",
       "      <td>95.408737</td>\n",
       "      <td>86.714401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13</td>\n",
       "      <td>H_47</td>\n",
       "      <td>49.0</td>\n",
       "      <td>92.920944</td>\n",
       "      <td>84.154495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>14</td>\n",
       "      <td>H_49</td>\n",
       "      <td>51.0</td>\n",
       "      <td>89.283226</td>\n",
       "      <td>79.903214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>15.0</td>\n",
       "      <td>82.408630</td>\n",
       "      <td>67.338509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>16</td>\n",
       "      <td>H_52</td>\n",
       "      <td>54.0</td>\n",
       "      <td>98.126953</td>\n",
       "      <td>89.117233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>18</td>\n",
       "      <td>H_56</td>\n",
       "      <td>58.0</td>\n",
       "      <td>96.827782</td>\n",
       "      <td>87.840210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>19</td>\n",
       "      <td>H_59</td>\n",
       "      <td>61.0</td>\n",
       "      <td>95.608147</td>\n",
       "      <td>86.592361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20</td>\n",
       "      <td>H_61</td>\n",
       "      <td>63.0</td>\n",
       "      <td>96.082130</td>\n",
       "      <td>87.054794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>21.0</td>\n",
       "      <td>79.644073</td>\n",
       "      <td>64.361122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>22</td>\n",
       "      <td>H_62</td>\n",
       "      <td>64.0</td>\n",
       "      <td>97.762428</td>\n",
       "      <td>88.862091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     u     v  bond_index  graph_bde_pred  graph_bdfe_pred\n",
       "0    0     1         0.0       89.382645        75.711853\n",
       "1    0  H_23        25.0      100.077187        91.049133\n",
       "2    1     2         1.0       85.872467        71.412849\n",
       "3    1  H_27        29.0       97.163109        87.689636\n",
       "4    2     3         2.0       85.041306        70.000275\n",
       "5    2  H_28        30.0       95.392189        86.257256\n",
       "6    3     4         3.0       83.115479        66.995270\n",
       "7    3  H_30        32.0       94.518456        84.748627\n",
       "8    4     5         NaN             NaN              NaN\n",
       "9    4    10         NaN             NaN              NaN\n",
       "10   4  H_32        34.0       93.767822        84.237808\n",
       "11   5     6         5.0       86.171143        71.943581\n",
       "12   5     7         NaN             NaN              NaN\n",
       "13   5  H_33        35.0       93.715736        84.072701\n",
       "14   6  H_36        38.0       98.747505        89.695900\n",
       "15   7     8         NaN             NaN              NaN\n",
       "16   7  H_37        39.0       95.855804        86.941345\n",
       "17   8     9         NaN             NaN              NaN\n",
       "18   8  H_40        42.0       94.971001        85.988342\n",
       "19   9    10         NaN             NaN              NaN\n",
       "20   9    21         NaN             NaN              NaN\n",
       "21   9  H_41        43.0       86.859306        75.766243\n",
       "22  10    11        10.0       79.460541        64.253860\n",
       "23  10    12         NaN             NaN              NaN\n",
       "24  11  H_44        46.0       97.184883        88.282654\n",
       "25  12    13         NaN             NaN              NaN\n",
       "26  12  H_46        48.0       95.408737        86.714401\n",
       "27  13    14         NaN             NaN              NaN\n",
       "28  13  H_47        49.0       92.920944        84.154495\n",
       "29  14    15         NaN             NaN              NaN\n",
       "30  14    21         NaN             NaN              NaN\n",
       "31  14  H_49        51.0       89.283226        79.903214\n",
       "32  15    16        15.0       82.408630        67.338509\n",
       "33  15    17         NaN             NaN              NaN\n",
       "34  15    18         NaN             NaN              NaN\n",
       "35  16  H_52        54.0       98.126953        89.117233\n",
       "36  18    19         NaN             NaN              NaN\n",
       "37  18  H_56        58.0       96.827782        87.840210\n",
       "38  19    20         NaN             NaN              NaN\n",
       "39  19  H_59        61.0       95.608147        86.592361\n",
       "40  20    21         NaN             NaN              NaN\n",
       "41  20  H_61        63.0       96.082130        87.054794\n",
       "42  21    22        21.0       79.644073        64.361122\n",
       "43  22  H_62        64.0       97.762428        88.862091"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_to_df(graphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MoleculeEnvDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A simple Dataset that yields (graph_data, env_features, target).\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          df: a pandas DataFrame with columns:\n",
    "              [ 'nx_graph', 'temperature', 'Seawater', 'Concentration', 'Time', 'degradation_rate', ...]\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        \n",
    "        # Optionally do some numerical encoding of 'Seawater' if it is categorical\n",
    "        # or handle arbitrary env variables. Here we assume they are numeric or\n",
    "        # can be turned numeric. For example:\n",
    "        #   'Seawater' -> 1, 'freshwater' -> 0\n",
    "        # or keep them as real values if numeric.\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 1) The graph is a networkx Graph or something we can convert to PyG's Data\n",
    "        nx_graph = row['nx_graph']  # e.g. from your create_bde_graph_selective_hs()\n",
    "        # Convert it if needed to PyG Data: you might have a helper function like:\n",
    "        #   data = convert_nx_to_pyg(nx_graph)\n",
    "        # For simplicity here, we’ll just return nx_graph directly.\n",
    "        \n",
    "        # 2) Env features: \n",
    "        # Suppose we stack them in a tensor [temp, concentration, time, ...].\n",
    "        # If \"Seawater\" is categorical, encode that too.\n",
    "        # We'll do a minimal example, but adapt to your actual columns.\n",
    "        env_features = torch.tensor([\n",
    "            row['temperature'],\n",
    "            row['Concentration'],\n",
    "            row['Time']\n",
    "            # Possibly also handle 'Seawater' (0 or 1)\n",
    "        ], dtype=torch.float)\n",
    "        \n",
    "        # 3) The target:\n",
    "        target = torch.tensor(row['degradation_rate'], dtype=torch.float)\n",
    "        \n",
    "        return nx_graph, env_features, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EnvPositionalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Maps environment features (temperature, concentration, etc.) into a\n",
    "    learned positional embedding. Instead of sinusoidal, this is an MLP.\n",
    "    \"\"\"\n",
    "    def __init__(self, env_input_dim, d_model, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(env_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, env_features):\n",
    "        \"\"\"\n",
    "        env_features: (batch_size, env_input_dim)\n",
    "        Returns: (batch_size, d_model)\n",
    "        \"\"\"\n",
    "        return self.mlp(env_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "# or any Graph Transformer block you'd like\n",
    "\n",
    "class SimpleGraphModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_node_features,   # e.g. dimension of (atomic symbol) embeddings\n",
    "                 env_input_dim,       # dimension of environment features\n",
    "                 hidden_dim=128, \n",
    "                 output_dim=1):       # for regression (1 = predicted degradation rate)\n",
    "        super().__init__()\n",
    "        \n",
    "        self.env_encoder = EnvPositionalEncoder(env_input_dim, d_model=hidden_dim)\n",
    "        \n",
    "        # Example: We embed node features up to hidden_dim\n",
    "        self.node_embedding = nn.Linear(num_node_features, hidden_dim)\n",
    "        \n",
    "        # A couple of GCNConv layers for demonstration\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Final MLP to produce a single scalar\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data, env_features):\n",
    "        \"\"\"\n",
    "        data: a PyG Data object or something with:\n",
    "              data.x (node_features) shape = [num_nodes, num_node_features]\n",
    "              data.edge_index\n",
    "              data.batch if using a batch of graphs\n",
    "        env_features: (batch_size, env_input_dim) -> we map to (batch_size, hidden_dim)\n",
    "        \n",
    "        Return: predicted degradation rate shape (batch_size,)\n",
    "        \"\"\"\n",
    "        # 1) Encode environment -> positional embedding\n",
    "        batch_size = env_features.shape[0]\n",
    "        env_pos_emb = self.env_encoder(env_features)  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # 2) Node embedding\n",
    "        # data.x shape = (total_num_nodes_in_batch, num_node_features)\n",
    "        x = self.node_embedding(data.x)\n",
    "        \n",
    "        # 3) For each graph in the batch, we add the environment embedding:\n",
    "        #    We need to figure out which of the `batch_size` each node belongs to.\n",
    "        #    PyG uses data.batch to indicate the graph index for each node.\n",
    "        #    So we broadcast-add env_pos_emb to x for each node belonging to the same graph.\n",
    "        # shape of data.batch = (total_num_nodes_in_batch,)\n",
    "        \n",
    "        # Expand env_pos_emb for each node. For example:\n",
    "        x = x + env_pos_emb[data.batch]  # broadcast by indexing the correct row in env_pos_emb\n",
    "        \n",
    "        # 4) Pass through GCN layers\n",
    "        x = self.conv1(x, data.edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, data.edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # 5) Global pooling to get a single graph-level vector\n",
    "        #    shape = (batch_size, hidden_dim)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        \n",
    "        # 6) Final MLP to get predicted scalar\n",
    "        out = self.fc_out(x)  # shape (batch_size, 1)\n",
    "        \n",
    "        return out.squeeze(-1)  # shape (batch_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def train_model(model, dataloader, epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for graphs_batch, envs_batch, targets_batch in dataloader:\n",
    "            graphs_batch = graphs_batch.to(device)\n",
    "            envs_batch = envs_batch.to(device)\n",
    "            targets_batch = targets_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(graphs_batch, envs_batch)\n",
    "\n",
    "            # Compute MSE Loss\n",
    "            loss = criterion(outputs, targets_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * targets_batch.size(0)\n",
    "            total_samples += targets_batch.size(0)\n",
    "\n",
    "        avg_loss = total_loss / total_samples  # Compute average MSE loss\n",
    "\n",
    "        # === Evaluation Step ===\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds, gts = [], []\n",
    "            \n",
    "            for graphs_batch, envs_batch, targets_batch in dataloader:\n",
    "                graphs_batch = graphs_batch.to(device)\n",
    "                envs_batch = envs_batch.to(device)\n",
    "                targets_batch = targets_batch.to(device)\n",
    "\n",
    "                out = model(graphs_batch, envs_batch)\n",
    "                preds.append(out.cpu())\n",
    "                gts.append(targets_batch.cpu())\n",
    "\n",
    "            preds = torch.cat(preds).numpy()\n",
    "            gts = torch.cat(gts).numpy()\n",
    "\n",
    "            mse_val = F.mse_loss(torch.tensor(preds), torch.tensor(gts)).item()\n",
    "            rmse_val = math.sqrt(mse_val)\n",
    "            r2_val = r2_score(gts, preds)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} - Train MSE: {avg_loss:.4f}, Val RMSE: {rmse_val:.4f}, R²: {r2_val:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule</th>\n",
       "      <th>bond_index</th>\n",
       "      <th>bond_type</th>\n",
       "      <th>start_atom</th>\n",
       "      <th>end_atom</th>\n",
       "      <th>fragment1</th>\n",
       "      <th>fragment2</th>\n",
       "      <th>is_valid_stereo</th>\n",
       "      <th>bde_pred</th>\n",
       "      <th>bdfe_pred</th>\n",
       "      <th>bde</th>\n",
       "      <th>bdfe</th>\n",
       "      <th>set</th>\n",
       "      <th>svg</th>\n",
       "      <th>has_dft_bde</th>\n",
       "      <th>raw_smiles</th>\n",
       "      <th>smiles_link</th>\n",
       "      <th>nx_graph</th>\n",
       "      <th>temperature</th>\n",
       "      <th>Concentration</th>\n",
       "      <th>Time</th>\n",
       "      <th>Seawater</th>\n",
       "      <th>degradation_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...</td>\n",
       "      <td>10</td>\n",
       "      <td>C-C</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>CCCC[C@H]1[C]2CC[C@H]3C(C)(C)CCC[C@]3(C)[C@H]2...</td>\n",
       "      <td>[CH3]</td>\n",
       "      <td>True</td>\n",
       "      <td>79.460541</td>\n",
       "      <td>64.253860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;?xml version='1.0' encoding='iso-8859-1'?&gt;\\n&lt;...</td>\n",
       "      <td>False</td>\n",
       "      <td>C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...</td>\n",
       "      <td>CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...</td>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>32.238788</td>\n",
       "      <td>43.460098</td>\n",
       "      <td>4.486329</td>\n",
       "      <td>sea</td>\n",
       "      <td>0.812391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...</td>\n",
       "      <td>21</td>\n",
       "      <td>C-C</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>CCCC[C@@H]1[C@@H](C)CC[C@@H]2[C]3CCCC(C)(C)[C@...</td>\n",
       "      <td>[CH3]</td>\n",
       "      <td>True</td>\n",
       "      <td>79.644073</td>\n",
       "      <td>64.361122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;?xml version='1.0' encoding='iso-8859-1'?&gt;\\n&lt;...</td>\n",
       "      <td>False</td>\n",
       "      <td>C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...</td>\n",
       "      <td>CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...</td>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>39.681957</td>\n",
       "      <td>32.678321</td>\n",
       "      <td>3.274464</td>\n",
       "      <td>fresh</td>\n",
       "      <td>0.584275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...</td>\n",
       "      <td>15</td>\n",
       "      <td>C-C</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1[...</td>\n",
       "      <td>[CH3]</td>\n",
       "      <td>True</td>\n",
       "      <td>82.408630</td>\n",
       "      <td>67.338509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;?xml version='1.0' encoding='iso-8859-1'?&gt;\\n&lt;...</td>\n",
       "      <td>False</td>\n",
       "      <td>C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...</td>\n",
       "      <td>CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...</td>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>21.003736</td>\n",
       "      <td>97.309748</td>\n",
       "      <td>7.638673</td>\n",
       "      <td>fresh</td>\n",
       "      <td>0.400473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...</td>\n",
       "      <td>3</td>\n",
       "      <td>C-C</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[CH2]CCC</td>\n",
       "      <td>C[C@@H]1[CH][C@]2(C)CC[C@H]3C(C)(C)CCC[C@]3(C)...</td>\n",
       "      <td>True</td>\n",
       "      <td>83.115479</td>\n",
       "      <td>66.995270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;?xml version='1.0' encoding='iso-8859-1'?&gt;\\n&lt;...</td>\n",
       "      <td>False</td>\n",
       "      <td>C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...</td>\n",
       "      <td>CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...</td>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>32.229716</td>\n",
       "      <td>47.585323</td>\n",
       "      <td>105.864370</td>\n",
       "      <td>fresh</td>\n",
       "      <td>0.281733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...</td>\n",
       "      <td>2</td>\n",
       "      <td>C-C</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[CH2]CC</td>\n",
       "      <td>[CH2][C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1...</td>\n",
       "      <td>True</td>\n",
       "      <td>85.041306</td>\n",
       "      <td>70.000275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;?xml version='1.0' encoding='iso-8859-1'?&gt;\\n&lt;...</td>\n",
       "      <td>False</td>\n",
       "      <td>C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...</td>\n",
       "      <td>CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...</td>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>12.298953</td>\n",
       "      <td>35.873991</td>\n",
       "      <td>70.581110</td>\n",
       "      <td>fresh</td>\n",
       "      <td>0.562167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            molecule  bond_index bond_type  \\\n",
       "0  CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...          10       C-C   \n",
       "1  CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...          21       C-C   \n",
       "2  CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...          15       C-C   \n",
       "3  CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...           3       C-C   \n",
       "4  CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1C...           2       C-C   \n",
       "\n",
       "   start_atom  end_atom                                          fragment1  \\\n",
       "0          10        11  CCCC[C@H]1[C]2CC[C@H]3C(C)(C)CCC[C@]3(C)[C@H]2...   \n",
       "1          21        22  CCCC[C@@H]1[C@@H](C)CC[C@@H]2[C]3CCCC(C)(C)[C@...   \n",
       "2          15        16  CCCC[C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1[...   \n",
       "3           3         4                                           [CH2]CCC   \n",
       "4           2         3                                            [CH2]CC   \n",
       "\n",
       "                                           fragment2  is_valid_stereo  \\\n",
       "0                                              [CH3]             True   \n",
       "1                                              [CH3]             True   \n",
       "2                                              [CH3]             True   \n",
       "3  C[C@@H]1[CH][C@]2(C)CC[C@H]3C(C)(C)CCC[C@]3(C)...             True   \n",
       "4  [CH2][C@@H]1[C@@H](C)CC[C@H]2[C@@]1(C)CC[C@H]1...             True   \n",
       "\n",
       "    bde_pred  bdfe_pred  bde  bdfe  set  \\\n",
       "0  79.460541  64.253860  NaN   NaN  NaN   \n",
       "1  79.644073  64.361122  NaN   NaN  NaN   \n",
       "2  82.408630  67.338509  NaN   NaN  NaN   \n",
       "3  83.115479  66.995270  NaN   NaN  NaN   \n",
       "4  85.041306  70.000275  NaN   NaN  NaN   \n",
       "\n",
       "                                                 svg  has_dft_bde  \\\n",
       "0  <?xml version='1.0' encoding='iso-8859-1'?>\\n<...        False   \n",
       "1  <?xml version='1.0' encoding='iso-8859-1'?>\\n<...        False   \n",
       "2  <?xml version='1.0' encoding='iso-8859-1'?>\\n<...        False   \n",
       "3  <?xml version='1.0' encoding='iso-8859-1'?>\\n<...        False   \n",
       "4  <?xml version='1.0' encoding='iso-8859-1'?>\\n<...        False   \n",
       "\n",
       "                                          raw_smiles  \\\n",
       "0  C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...   \n",
       "1  C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...   \n",
       "2  C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...   \n",
       "3  C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...   \n",
       "4  C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([...   \n",
       "\n",
       "                                         smiles_link  \\\n",
       "0  CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...   \n",
       "1  CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...   \n",
       "2  CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...   \n",
       "3  CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...   \n",
       "4  CCCC%5BC%40%40H%5D1%5BC%40%40H%5D%28C%29CC%5BC...   \n",
       "\n",
       "                                            nx_graph  temperature  \\\n",
       "0  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...    32.238788   \n",
       "1  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...    39.681957   \n",
       "2  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...    21.003736   \n",
       "3  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...    32.229716   \n",
       "4  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...    12.298953   \n",
       "\n",
       "   Concentration        Time Seawater  degradation_rate  \n",
       "0      43.460098    4.486329      sea          0.812391  \n",
       "1      32.678321    3.274464    fresh          0.584275  \n",
       "2      97.309748    7.638673    fresh          0.400473  \n",
       "3      47.585323  105.864370    fresh          0.281733  \n",
       "4      35.873991   70.581110    fresh          0.562167  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming the environment variable\n",
    "\n",
    "# 1) Add random environment columns for demonstration\n",
    "num_rows = len(alfabet_results_022)\n",
    "\n",
    "# Temperatures between 10°C and 40°C\n",
    "alfabet_results_022['temperature'] = np.random.uniform(10, 40, size=num_rows)\n",
    "\n",
    "# Concentration in mg/L, random 1–100\n",
    "alfabet_results_022['Concentration'] = np.random.uniform(1, 100, size=num_rows)\n",
    "\n",
    "# Time in hours, random 0–120\n",
    "alfabet_results_022['Time'] = np.random.uniform(0, 120, size=num_rows)\n",
    "\n",
    "# Categorical 'Seawater' vs 'fresh' environment\n",
    "alfabet_results_022['Seawater'] = np.random.choice(['sea', 'fresh'], size=num_rows)\n",
    "\n",
    "# And a random target: 'degradation_rate' (arbitrary range)\n",
    "alfabet_results_022['degradation_rate'] = np.random.uniform(0.1, 1.0, size=num_rows)\n",
    "\n",
    "# 2) Inspect the updated DataFrame\n",
    "alfabet_results_022.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for molecular graphs with environmental variables.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, list_of_graphs, env_columns, target_column='degradation_rate'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: pandas DataFrame with environment variables and degradation rate.\n",
    "            list_of_graphs: list of NetworkX graphs (one per molecule).\n",
    "            env_columns: list of environment variable column names.\n",
    "            target_column: the name of the target variable in df.\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.graphs = list_of_graphs  # 20 molecules\n",
    "        self.env_columns = env_columns\n",
    "        self.target_column = target_column\n",
    "        \n",
    "        # Ensure the number of molecules match across environments\n",
    "        self.num_molecules = len(self.graphs)\n",
    "        self.num_env_conditions = len(self.df) // self.num_molecules  # 16 environments\n",
    "        assert len(self.df) == self.num_molecules * self.num_env_conditions, \\\n",
    "            \"Mismatch between molecules and environment conditions.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)  # Total = 20 molecules * 16 conditions = 320\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the graph index (ensures each molecule repeats across environments)\n",
    "        graph_idx = idx % self.num_molecules\n",
    "        G = self.graphs[graph_idx]\n",
    "        \n",
    "        # Extract node features (e.g., atom types mapped to indices)\n",
    "        node_features = [G.nodes[n]['symbol'] for n in G.nodes()]\n",
    "        node_features = torch.tensor([self.atom_symbol_to_index(atom) for atom in node_features], dtype=torch.long)\n",
    "\n",
    "        # Extract environmental variables\n",
    "        env_data = torch.tensor(self.df.loc[idx, self.env_columns].values.astype(np.float32), dtype=torch.float32)\n",
    "\n",
    "        # Extract target degradation rate\n",
    "        y = torch.tensor(self.df.loc[idx, self.target_column], dtype=torch.float32)\n",
    "        \n",
    "        return node_features, env_data, y\n",
    "\n",
    "    @staticmethod\n",
    "    def atom_symbol_to_index(symbol):\n",
    "        \"\"\"Map atom symbols (C, O, H, etc.) to unique indices.\"\"\"\n",
    "        symbol_to_idx = {'C': 0, 'O': 1, 'H': 2, 'N': 3}  # Extend as needed\n",
    "        return symbol_to_idx.get(symbol, len(symbol_to_idx))  # Default to unknown index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dataset\n",
    "dataset = MoleculeEnvDataset(alfabet_results_022)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=my_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_model() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 11\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleGraphModel(num_node_features\u001b[38;5;241m=\u001b[39mnum_node_features,\n\u001b[0;32m      6\u001b[0m                          env_input_dim\u001b[38;5;241m=\u001b[39menv_input_dim,\n\u001b[0;32m      7\u001b[0m                          hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m      8\u001b[0m                          output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 4) Train\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: train_model() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the model\n",
    "#    For demonstration, suppose node features = 1 (maybe a 1-hot or embedding dimension)\n",
    "num_node_features = 1\n",
    "env_input_dim = 3  # e.g. [temp, concentration, time], or more if you encode \"Seawater\" etc\n",
    "model = SimpleGraphModel(num_node_features=num_node_features,\n",
    "                         env_input_dim=env_input_dim,\n",
    "                         hidden_dim=128,\n",
    "                         output_dim=1)\n",
    "\n",
    "# 4) Train\n",
    "train_model(model, dataloader, device='cpu', epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, MSE: 0.2216, RMSE: 0.4707\n",
      "Epoch 1/20 - Train MSE: 0.2216, Val RMSE: 0.3000, R²: -0.2757\n",
      "Epoch 2/20, MSE: 0.1088, RMSE: 0.3298\n",
      "Epoch 2/20 - Train MSE: 0.1088, Val RMSE: 0.3035, R²: -0.3056\n",
      "Epoch 3/20, MSE: 0.0962, RMSE: 0.3101\n",
      "Epoch 3/20 - Train MSE: 0.0962, Val RMSE: 0.2953, R²: -0.2361\n",
      "Epoch 4/20, MSE: 0.0950, RMSE: 0.3082\n",
      "Epoch 4/20 - Train MSE: 0.0950, Val RMSE: 0.2965, R²: -0.2463\n",
      "Epoch 5/20, MSE: 0.0963, RMSE: 0.3103\n",
      "Epoch 5/20 - Train MSE: 0.0963, Val RMSE: 0.2855, R²: -0.1555\n",
      "Epoch 6/20, MSE: 0.0849, RMSE: 0.2915\n",
      "Epoch 6/20 - Train MSE: 0.0849, Val RMSE: 0.2792, R²: -0.1047\n",
      "Epoch 7/20, MSE: 0.0853, RMSE: 0.2921\n",
      "Epoch 7/20 - Train MSE: 0.0853, Val RMSE: 0.2886, R²: -0.1803\n",
      "Epoch 8/20, MSE: 0.0836, RMSE: 0.2891\n",
      "Epoch 8/20 - Train MSE: 0.0836, Val RMSE: 0.2749, R²: -0.0709\n",
      "Epoch 9/20, MSE: 0.0803, RMSE: 0.2834\n",
      "Epoch 9/20 - Train MSE: 0.0803, Val RMSE: 0.2788, R²: -0.1018\n",
      "Epoch 10/20, MSE: 0.0795, RMSE: 0.2819\n",
      "Epoch 10/20 - Train MSE: 0.0795, Val RMSE: 0.2806, R²: -0.1158\n",
      "Epoch 11/20, MSE: 0.0809, RMSE: 0.2844\n",
      "Epoch 11/20 - Train MSE: 0.0809, Val RMSE: 0.2742, R²: -0.0659\n",
      "Epoch 12/20, MSE: 0.0784, RMSE: 0.2799\n",
      "Epoch 12/20 - Train MSE: 0.0784, Val RMSE: 0.3039, R²: -0.3094\n",
      "Epoch 13/20, MSE: 0.0799, RMSE: 0.2826\n",
      "Epoch 13/20 - Train MSE: 0.0799, Val RMSE: 0.2706, R²: -0.0379\n",
      "Epoch 14/20, MSE: 0.0759, RMSE: 0.2755\n",
      "Epoch 14/20 - Train MSE: 0.0759, Val RMSE: 0.2686, R²: -0.0223\n",
      "Epoch 15/20, MSE: 0.0769, RMSE: 0.2774\n",
      "Epoch 15/20 - Train MSE: 0.0769, Val RMSE: 0.2763, R²: -0.0819\n",
      "Epoch 16/20, MSE: 0.0743, RMSE: 0.2727\n",
      "Epoch 16/20 - Train MSE: 0.0743, Val RMSE: 0.2699, R²: -0.0328\n",
      "Epoch 17/20, MSE: 0.0738, RMSE: 0.2716\n",
      "Epoch 17/20 - Train MSE: 0.0738, Val RMSE: 0.2750, R²: -0.0717\n",
      "Epoch 18/20, MSE: 0.0734, RMSE: 0.2709\n",
      "Epoch 18/20 - Train MSE: 0.0734, Val RMSE: 0.2677, R²: -0.0159\n",
      "Epoch 19/20, MSE: 0.0719, RMSE: 0.2681\n",
      "Epoch 19/20 - Train MSE: 0.0719, Val RMSE: 0.2668, R²: -0.0086\n",
      "Epoch 20/20, MSE: 0.0730, RMSE: 0.2701\n",
      "Epoch 20/20 - Train MSE: 0.0730, Val RMSE: 0.2764, R²: -0.0827\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "dummy_g = nx.Graph()\n",
    "dummy_g.add_nodes_from([0, 1, 2])  # trivial 3-node graph\n",
    "dummy_g.add_edges_from([(0,1), (1,2)])  # 2 edges\n",
    "\n",
    "alfabet_results_022['nx_graph'] = [dummy_g]*num_rows\n",
    "\n",
    "########################################\n",
    "# 3) Define the Dataset class\n",
    "########################################\n",
    "class MoleculeEnvDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Yields (nx_graph, env_tensor, target). In practice, you’d convert the Nx graph\n",
    "    to a PyG Data object in the collate function or here.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        # For example, encode 'Seawater' as 1.0 for 'sea' and 0.0 for 'fresh'\n",
    "        self.water_map = {'sea': 1.0, 'fresh': 0.0}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Nx graph for this row (toy example: they’re all the same)\n",
    "        nx_graph = row['nx_graph']\n",
    "\n",
    "        # Build environment tensor. For demonstration, we use [temp, conc, time]\n",
    "        # If you want to include 'Seawater' too, just do so below.\n",
    "        env_features = torch.tensor([\n",
    "            row['temperature'],\n",
    "            row['Concentration'],\n",
    "            row['Time']\n",
    "            # or self.water_map.get(row['Seawater'], 0.0) if you want\n",
    "        ], dtype=torch.float)\n",
    "\n",
    "        # Regression target\n",
    "        target = torch.tensor(row['degradation_rate'], dtype=torch.float)\n",
    "\n",
    "        return nx_graph, env_features, target\n",
    "\n",
    "########################################\n",
    "# 4) Define a collate_fn to handle Nx -> PyG\n",
    "########################################\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (nx_graph, env_tensor, target)\n",
    "    \"\"\"\n",
    "    nx_list   = [b[0] for b in batch]\n",
    "    env_list  = [b[1] for b in batch]\n",
    "    tgt_list  = [b[2] for b in batch]\n",
    "\n",
    "    # Convert Nx -> PyG\n",
    "    pyg_list = []\n",
    "    for g in nx_list:\n",
    "        pyg_data = from_networkx(g)\n",
    "        # Minimal node feature: each node just has 1 feature = node index\n",
    "        # (In reality, you'd embed atomic symbols, etc.)\n",
    "        x = []\n",
    "        for node_idx in range(pyg_data.num_nodes):\n",
    "            x.append([float(node_idx)])  # toy\n",
    "        pyg_data.x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        pyg_list.append(pyg_data)\n",
    "\n",
    "    # Combine into a single batch\n",
    "    pyg_batch = Batch.from_data_list(pyg_list)\n",
    "\n",
    "    # Stack environment + targets\n",
    "    env_batch = torch.stack(env_list, dim=0)\n",
    "    tgt_batch = torch.stack(tgt_list, dim=0)\n",
    "\n",
    "    return pyg_batch, env_batch, tgt_batch\n",
    "\n",
    "########################################\n",
    "# 5) Instantiate the dataset / dataloader\n",
    "########################################\n",
    "dataset = MoleculeEnvDataset(alfabet_results_022)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=my_collate)\n",
    "\n",
    "########################################\n",
    "# 6) Build a simple GNN model\n",
    "########################################\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# MLP that maps env_features -> embedding\n",
    "class EnvPositionalEncoder(nn.Module):\n",
    "    def __init__(self, env_input_dim, d_model, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(env_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, d_model)\n",
    "        )\n",
    "    def forward(self, env):\n",
    "        return self.mlp(env)\n",
    "\n",
    "# A small GCN-based model\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class SimpleGraphModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_node_features,\n",
    "                 env_input_dim,\n",
    "                 hidden_dim=128,\n",
    "                 output_dim=1):\n",
    "        super().__init__()\n",
    "        self.env_encoder = EnvPositionalEncoder(env_input_dim, d_model=hidden_dim)\n",
    "        self.node_encoder = nn.Linear(num_node_features, hidden_dim)\n",
    "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, data, env):\n",
    "        # data: PyG batch\n",
    "        # env: (batch_size, env_input_dim)\n",
    "        env_emb = self.env_encoder(env)  # => (B, hidden_dim)\n",
    "\n",
    "        x = self.node_encoder(data.x)    # => (num_nodes, hidden_dim)\n",
    "        x = x + env_emb[data.batch]      # broadcast env to each node\n",
    "\n",
    "        x = F.relu(self.conv1(x, data.edge_index))\n",
    "        x = F.relu(self.conv2(x, data.edge_index))\n",
    "\n",
    "        # Global average pooling => (batch_size, hidden_dim)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "\n",
    "        # Output => (batch_size, 1)\n",
    "        return self.fc_out(x).squeeze(-1)  # => (batch_size,)\n",
    "\n",
    "########################################\n",
    "# 7) Instantiate + Train\n",
    "########################################\n",
    "model = SimpleGraphModel(\n",
    "    num_node_features=1,  # we only gave each node a single feature in my_collate\n",
    "    env_input_dim=3,      # [temp, concentration, time]\n",
    "    hidden_dim=64,\n",
    "    output_dim=1\n",
    ")\n",
    "\n",
    "device = 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for pyg_batch, env_batch, tgt_batch in dataloader:\n",
    "        pyg_batch, env_batch, tgt_batch = (\n",
    "            pyg_batch.to(device),\n",
    "            env_batch.to(device),\n",
    "            tgt_batch.to(device)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(pyg_batch, env_batch)\n",
    "        loss = criterion(preds, tgt_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * tgt_batch.size(0)\n",
    "        total_samples += tgt_batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "\n",
    "\n",
    "\n",
    "    # For a toy example, we'll just print training loss each epoch\n",
    "    print(f\"Epoch {epoch}/{epochs}, MSE: {avg_loss:.4f}, RMSE: {math.sqrt(avg_loss):.4f}\")\n",
    "\n",
    "        # === Evaluation Step ===\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds, gts = [], []\n",
    "            \n",
    "        for graphs_batch, envs_batch, targets_batch in dataloader:\n",
    "            graphs_batch = graphs_batch.to(device)\n",
    "            envs_batch = envs_batch.to(device)\n",
    "            targets_batch = targets_batch.to(device)\n",
    "\n",
    "            out = model(graphs_batch, envs_batch)\n",
    "            preds.append(out.cpu())\n",
    "            gts.append(targets_batch.cpu())\n",
    "\n",
    "        preds = torch.cat(preds).numpy()\n",
    "        gts = torch.cat(gts).numpy()\n",
    "\n",
    "        mse_val = F.mse_loss(torch.tensor(preds), torch.tensor(gts)).item()\n",
    "        rmse_val = math.sqrt(mse_val)\n",
    "        r2_val = r2_score(gts, preds)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} - Train MSE: {avg_loss:.4f}, Val RMSE: {rmse_val:.4f}, R²: {r2_val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfabet-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
