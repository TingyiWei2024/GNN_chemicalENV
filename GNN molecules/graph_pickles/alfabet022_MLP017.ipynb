{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the environmental variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1023 environment rows\n",
      "   temperature  seawater  time component  concentration  degradation_rate\n",
      "0         35.6         1    30       C23             70          0.670914\n",
      "1         35.6         1    30       C24             70          0.680071\n",
      "2         35.6         1    30       C25             70          0.655230\n",
      "3         35.6         1    30       C26             70          0.625193\n",
      "4         35.6         1    30      C28a             70          0.605853\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the environmental data from Excel\n",
    "env_file = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\dataset02.xlsx\"\n",
    "env_df = pd.read_excel(env_file, engine='openpyxl')\n",
    "\n",
    "# Select only the relevant columns for the environment\n",
    "env_columns = [\"temperature\", \"seawater\", \"time\", \"component\",\"concentration\", \"degradation_rate\"]\n",
    "\n",
    "# Ensure all columns exist in the dataset\n",
    "env_var = env_df[env_columns].copy()\n",
    "\n",
    "# Convert categorical \"seawater\" to numerical (if needed)\n",
    "env_var[\"seawater\"] = env_var[\"seawater\"].map({\"sea\": 1, \"art\": 0})  # Map \"sea\" → 1, \"art\" → 0\n",
    "\n",
    "# Drop rows with missing values\n",
    "env_var = env_var.dropna().reset_index(drop=True)\n",
    "\n",
    "# Check if it matches the number of graphs\n",
    "print(f\"Loaded {len(env_var)} environment rows\")\n",
    "print(env_var.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compounds names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C23' 'C24' 'C25' 'C26' 'C28a' 'C28b' 'C29a' 'C29b' 'Ts' 'Tm' 'C29' 'C30'\n",
      " 'H31S' 'H31R' 'H32S' 'H32R' 'H33S' 'H33R' 'H34S' 'H34R']\n"
     ]
    }
   ],
   "source": [
    "# Get unique compounds (should be 20 total)\n",
    "compounds = env_var[\"component\"].unique()\n",
    "print(compounds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load molecules graph datas from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded graphs for compounds: ['C23', 'C24', 'C25', 'C26', 'C28a', 'C28b', 'C29', 'C29a', 'C29b', 'C30', 'H31R', 'H31S', 'H32R', 'H32S', 'H33R', 'H33S', 'H34R', 'H34S', 'Tm', 'Ts']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "folder_path = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\"\n",
    "pkl_files = [f for f in os.listdir(folder_path) if f.endswith(\".pkl\")]\n",
    "\n",
    "graphs_dict = {}\n",
    "\n",
    "for filename in pkl_files:\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    # Extract the compound name from the filename, e.g., \"C23.pkl\" -> \"C23\"\n",
    "    compound_name = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Load the pickle file\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        graph = pickle.load(file)\n",
    "    \n",
    "    # Store the graph in the dictionary under the compound name\n",
    "    graphs_dict[compound_name] = graph\n",
    "\n",
    "\n",
    "print(\"Loaded graphs for compounds:\", list(graphs_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换为PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted graphs: dict_keys(['C23', 'C24', 'C25', 'C26', 'C28a', 'C28b', 'C29', 'C29a', 'C29b', 'C30', 'H31R', 'H31S', 'H32R', 'H32S', 'H33R', 'H33S', 'H34R', 'H34S', 'Tm', 'Ts'])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "pyg_graphs_dict = {}\n",
    "printprintpp\n",
    "for compound_name, nx_graph in graphs_dict.items():\n",
    "    # Convert the NetworkX graph to a PyG Data object\n",
    "    data = from_networkx(nx_graph)\n",
    "    pyg_graphs_dict[compound_name] = data\n",
    "\n",
    "print(\"Converted graphs:\", pyg_graphs_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 88], symbol=[42], rdkit_idx=[42], bond_index=[88], bde_pred=[88], bdfe_pred=[88], mol=<rdkit.Chem.rdchem.Mol object at 0x000001F83B641DD0>, num_nodes=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyg_graphs_dict[\"C23\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "1040\n"
     ]
    }
   ],
   "source": [
    "print(len(pyg_graphs_dict))\n",
    "print(len(env_df)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use environmental variables as Transformer Positional Encoding connect\n",
    "\n",
    "simply use nx graph is enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境变量的PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (715384983.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[25], line 14\u001b[1;36m\u001b[0m\n\u001b[1;33m    base_data = pyg_graphs_dict[]\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Specify which environment columns you want as features\n",
    "env_columns = [\"temperature\", \"seawater\", \"time\", \"concentration\"]\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for idx, row in env_df.iterrows():\n",
    "    compound_N = row[\"component\"]\n",
    "\n",
    "    if compound_N in pyg_graphs_dict:\n",
    "        # Get the base PyG graph\n",
    "        base_data = pyg_graphs_dict[compound_N] #应该是用graph去匹配compounds吧？晕了已经\n",
    "\n",
    "        # 这出问题了\n",
    "        data = Data(\n",
    "            x=base_data.x.clone(),\n",
    "            edge_index=base_data.edge_index.clone(),\n",
    "            edge_attr=base_data.edge_attr.clone() if base_data.edge_attr is not None else None\n",
    "        )\n",
    "\n",
    "        # Create a tensor for environment features\n",
    "        env_feats = torch.tensor([row[col] for col in env_columns], dtype=torch.float) #提取一整行，转换tensor\n",
    "        data.env = env_feats  # Attach env features\n",
    "\n",
    "        # Create a tensor for the target (degradation rate)\n",
    "        y = torch.tensor([row[\"degradation_rate\"]], dtype=torch.float)\n",
    "        data.y = y\n",
    "\n",
    "        data_list.append(data)\n",
    "    else:\n",
    "        # If a compound isn't found in pyg_graphs_dict, you can skip or log a warning\n",
    "        print(f\"Warning: Compound {compound_name} not in pyg_graphs_dict.\")\n",
    "\n",
    "print(f\"Constructed {len(data_list)} (graph + env) samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1  建立Build DataLoader\n",
    "PYG的Data对象代表图形，图形在节点和边缘的数量上有所不同。默认的DataLoader试图像张量一样堆叠它们，是不行滴。\n",
    " Custom Collate？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 建立 env + molecules graph层 → 建立GNN\n",
    "由于PYG将多个图批量分为一个大图结构，因此我们需要data.batch来确保每个节点都会收到其相应图的正确环境嵌入\n",
    "1.PYG将多个图形组合在一起→来自不同图形的节点被连接\n",
    "2.data.batch tracks nodes属于哪个图。\n",
    "3.我们使用env_pos_emb[data.batch]添加每个节点的正确环境效果\n",
    "4.这确保每个节点都会获取其相应的环境编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv, global_mean_pool\n",
    "\n",
    "class EnvGraphTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_node_dim,      # dimension of node features\n",
    "                 in_env_dim,       # dimension of environment features\n",
    "                 hidden_dim=64, \n",
    "                 num_layers=2, \n",
    "                 out_dim=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Project environment to same dimension as node embeddings for addition\n",
    "        self.env_proj = nn.Linear(in_env_dim, hidden_dim)\n",
    "        \n",
    "        # If your node features are not 'hidden_dim', project them first\n",
    "        self.node_proj = nn.Linear(in_node_dim, hidden_dim)\n",
    "\n",
    "        # Build a stack of TransformerConv layers\n",
    "        self.transformer_convs = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = TransformerConv(\n",
    "                hidden_dim, hidden_dim,\n",
    "                heads=4,   # multi-head\n",
    "                dropout=0.1,\n",
    "                edge_dim=None  # if you have edge features, set dimension here\n",
    "            )\n",
    "            self.transformer_convs.append(conv)\n",
    "        \n",
    "        # Final linear to map from hidden_dim -> out_dim (e.g. 1 for regression)\n",
    "        self.final_fc = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # data.x: node features, shape [num_nodes_in_batch, in_node_dim] \n",
    "        # data.env: environment, shape [batch_size, in_env_dim] \n",
    "        # data.edge_index: shape [2, E]\n",
    "        # data.batch: shape [num_nodes_in_batch], which graph each node belongs to\n",
    "\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # 1) environment projection -> shape [batch_size, hidden_dim]\n",
    "        #    We'll broadcast to nodes in each graph\n",
    "        env_emb = self.env_proj(data.env)  # [batch_size, hidden_dim]\n",
    "\n",
    "        # 2) node projection -> shape [num_nodes_in_batch, hidden_dim]\n",
    "        x = self.node_proj(x)\n",
    "\n",
    "        # 3) add environment embedding to each node\n",
    "        #    broadcast env_emb -> shape [num_nodes_in_batch, hidden_dim]\n",
    "        x = x + env_emb[batch]  # adds environment positional encoding to each node\n",
    "\n",
    "        # 4) pass through TransformerConv layers\n",
    "        for conv in self.transformer_convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        # 5) global pooling to get a single graph-level embedding\n",
    "        graph_emb = global_mean_pool(x, batch)\n",
    "\n",
    "        # 6) final regression\n",
    "        out = self.final_fc(graph_emb)  # shape [batch_size, 1]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 MSE/RMSE/R2 检验 → 测试model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Suppose your data_list has each item with data.x, data.env, data.y\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# We do a random shuffle + batch them\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# The dimension of node features:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m in_node_dim \u001b[38;5;241m=\u001b[39m data_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:87\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, follow_batch, exclude_keys, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_batch \u001b[38;5;241m=\u001b[39m follow_batch\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude_keys \u001b[38;5;241m=\u001b[39m exclude_keys\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     88\u001b[0m     dataset,\n\u001b[0;32m     89\u001b[0m     batch_size,\n\u001b[0;32m     90\u001b[0m     shuffle,\n\u001b[0;32m     91\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mCollater(dataset, follow_batch, exclude_keys),\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     93\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:376\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 376\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\torch\\utils\\data\\sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Suppose your data_list has each item with data.x, data.env, data.y\n",
    "# We do a random shuffle + batch them\n",
    "loader = DataLoader(data_list, batch_size=16, shuffle=True)\n",
    "\n",
    "# The dimension of node features:\n",
    "in_node_dim = data_list[0].x.shape[1]\n",
    "# The dimension of environment features, e.g. \"env_columns = 4\"\n",
    "in_env_dim = len(env_columns)\n",
    "\n",
    "model = EnvGraphTransformer(in_node_dim, in_env_dim, hidden_dim=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# If you have a GPU:\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_data in loader:\n",
    "        # batch_data = batch_data.to(device)  # if using GPU\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(batch_data).squeeze(dim=-1)  # shape [batch_size]\n",
    "        target = batch_data.y.squeeze(dim=-1)     # shape [batch_size]\n",
    "\n",
    "        loss = loss_fn(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1:02d}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. fixed code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfabet-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
