{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   # 文件系统操作\n",
    "import pickle  # 模块 Python 标准库\n",
    "import pandas as pd  #  表格数据 处理\n",
    "import numpy as np  # 模块 科学计算库   数组计算\n",
    "\n",
    "import networkx as nx # 复杂网络分析库（图数据）\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F # PyTorch 的函数（如激活函数）\n",
    "from torch import nn           # PyTorch 的神经网络层（类）\n",
    "from torch.utils.data import Dataset, DataLoader, random_split # 数据加载类\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader as PyGDataLoader # PyTorch Geometric（图神经网络库），用于 GNN 计算\n",
    "from torch_geometric.utils import from_networkx # 函数，用于从 networkx 转换为 torch_geometric 格式\n",
    "from torch_geometric.nn import GCNConv, GINEConv, global_mean_pool, BatchNorm # 图神经网络层的类 \n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm # 函数，用于对 GCN 进行归一化处理\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常见特殊方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 实例方法（Instance Method）：定义在类中的函数，属于某个对象（实例）\n",
    "\n",
    "1.必须定义在类中\n",
    "\n",
    "2.必须接受 self（指向当前实例）\n",
    "\n",
    "3.用于实例的索引访问\n",
    "\n",
    "特殊方法\t作用\n",
    "- __init__\t构造函数，创建对象时调用\n",
    "- __str__\t定义 print(obj) 时的字符串表示\n",
    "- __repr__\t定义 repr(obj) 时的字符串表示\n",
    "\n",
    "__len__\t让对象支持 len(obj)\n",
    "\n",
    "__getitem__\t让对象支持索引访问 obj[key]\n",
    "\n",
    "__setitem__\t让对象支持索引赋值 obj[key] = value\n",
    "\n",
    "__delitem__\t让对象支持 del obj[key]\n",
    "\n",
    "__iter__\t让对象支持 for item in obj 迭代\n",
    "\n",
    "__contains__\t让对象支持 in 关键字，如 item in obj\n",
    "\n",
    "__call__\t让对象像函数一样调用，如 obj()\n",
    "\n",
    "__eq__\t定义 == 比较\n",
    "\n",
    "__add__\t让对象支持 + 运算符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset that:\n",
    "      - Reads external factors from CSV\n",
    "      - Loads the corresponding pickle for the molecule's graph\n",
    "      - Converts it into a PyG Data object\n",
    "    \"\"\"\n",
    "    def __init__(self,                      # \n",
    "                 raw_dataframe: pd.DataFrame,\n",
    "                 nx_graph_dict: dict,\n",
    "                 *,                         # 表示后面的参数必须以关键字参数传递\n",
    "                 component_col: str,\n",
    "                 global_state_cols: list[str],\n",
    "                 label_col: str,\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        \"\"\" \n",
    "        # 数据完整性检查\n",
    "        self.raw_dataframe = raw_dataframe\n",
    "        self.nx_graph_dict = nx_graph_dict\n",
    "        self.component_col = [component_col] if type(component_col) is str else component_col\n",
    "        self.global_state_cols = global_state_cols\n",
    "        self.label_col = [label_col] if type(label_col) is str else label_col\n",
    "        self.transform = transform\n",
    "\n",
    "        ## 优化code\n",
    "        #self.component_col = [component_col] if isinstance(component_col, str) else component_col\n",
    "        #self.label_col = [label_col] if isinstance(label_col, str) else label_col\n",
    "        \n",
    "        # 合并所有必需的列为set\n",
    "        required_cols = set(self.global_state_cols + self.label_col + self.component_col)\n",
    "        for col in required_cols:\n",
    "            if col not in self.raw_dataframe.columns:\n",
    "                raise ValueError(f\"Missing column in DataFrame: '{col}'\") #检查缺失(可删)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataframe) # 样本数量有多少行\n",
    "\n",
    "    def __getitem__(self, idx): # 特殊方法，用于索引数据集（如 dataset[idx]）  self: 实例对象，指向当前类的实例\n",
    "        row = self.raw_dataframe.iloc[idx] # 通过索引 idx 获取 CSV 中的一行数据\n",
    "        \n",
    "        # 1. Load the molecule graph\n",
    "        component_name = row[self.component_col[0]]  # e.g. \"C23\"\n",
    "        pyg_data = self.nx_graph_dict[component_name]\n",
    "\n",
    "        # 3. Prepare the external factors\n",
    "        #    Convert selected columns into a float tensor\n",
    "        externals = torch.tensor(row[self.global_state_cols].values.astype(float), dtype=torch.float)\n",
    "        externals = externals.unsqueeze(0)\n",
    "\n",
    "        # 4. Prepare the label (regression target)\n",
    "        label = torch.tensor([row[self.label_col][0]], dtype=torch.float)\n",
    "\n",
    "        # 5. Attach externals & label to the Data object for use in the model\n",
    "        #    (We can store them in Data object attributes if you like)\n",
    "        pyg_data.externals = externals  # 1D vector of external factors\n",
    "        pyg_data.y = label  # shape [1]\n",
    "\n",
    "        if self.transform:\n",
    "            pyg_data = self.transform(pyg_data)\n",
    "\n",
    "        return pyg_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Standard training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MolDataset(Dataset):\n",
    "    def __init__(self, raw_dataframe, nx_graph_dict, *, component_col: str, global_state_cols: list[str], label_col: str, transform=None):\n",
    "        self.raw_dataframe = raw_dataframe\n",
    "        self.nx_graph_dict = nx_graph_dict\n",
    "        self.component_col = [component_col] if isinstance(component_col, str) else component_col\n",
    "        self.global_state_cols = global_state_cols\n",
    "        self.label_col = [label_col] if isinstance(label_col, str) else label_col\n",
    "        self.transform = transform\n",
    "        \n",
    "        required_cols = set(self.global_state_cols + self.label_col + self.component_col)\n",
    "        for col in required_cols:\n",
    "            if col not in self.raw_dataframe.columns:\n",
    "                raise ValueError(f\"Missing column in DataFrame: '{col}'\")\n",
    "\n",
    "        # Create uninitialized scalers\n",
    "        self.node_scaler = StandardScaler()\n",
    "        self.edge_scaler = StandardScaler()\n",
    "        self.env_scaler = StandardScaler()\n",
    "\n",
    "    def fit_standardizers(self, train_indices):\n",
    "        \"\"\" Fit scalers only using training data \"\"\"\n",
    "        node_features, edge_features, env_features = [], [], []\n",
    "\n",
    "        for idx in train_indices:\n",
    "            row = self.raw_dataframe.iloc[idx]\n",
    "            component_name = row[self.component_col[0]]\n",
    "            pyg_data = self.nx_graph_dict[component_name]\n",
    "\n",
    "            if pyg_data.x is not None:\n",
    "                node_features.append(pyg_data.x.numpy())\n",
    "            if pyg_data.edge_attr is not None:\n",
    "                edge_features.append(pyg_data.edge_attr.numpy())\n",
    "            env_features.append(row[self.global_state_cols].values.astype(float))\n",
    "\n",
    "        # Fit scalers only on training data\n",
    "        if node_features:\n",
    "            all_node_features = np.vstack(node_features)\n",
    "            self.node_scaler.fit(all_node_features)\n",
    "\n",
    "        if edge_features:\n",
    "            all_edge_features = np.vstack(edge_features)\n",
    "            self.edge_scaler.fit(all_edge_features)\n",
    "\n",
    "        all_env_features = np.vstack(env_features)\n",
    "        self.env_scaler.fit(all_env_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.raw_dataframe.iloc[idx]\n",
    "        component_name = row[self.component_col[0]]\n",
    "        pyg_data = self.nx_graph_dict[component_name]\n",
    "\n",
    "        # Standardize node features\n",
    "        if pyg_data.x is not None:\n",
    "            pyg_data.x = torch.tensor(self.node_scaler.transform(pyg_data.x.numpy()), dtype=torch.float)\n",
    "\n",
    "        # Standardize edge features\n",
    "        if pyg_data.edge_attr is not None:\n",
    "            pyg_data.edge_attr = torch.tensor(self.edge_scaler.transform(pyg_data.edge_attr.numpy()), dtype=torch.float)\n",
    "\n",
    "        # Standardize environmental data\n",
    "        externals = row[self.global_state_cols].values.astype(float)\n",
    "        externals = torch.tensor(self.env_scaler.transform([externals])[0], dtype=torch.float).unsqueeze(0)\n",
    "        pyg_data.externals = externals  \n",
    "\n",
    "        # Prepare label\n",
    "        label = torch.tensor([row[self.label_col][0]], dtype=torch.float)\n",
    "        pyg_data.y = label  \n",
    "\n",
    "        if self.transform:\n",
    "            pyg_data = self.transform(pyg_data)\n",
    "\n",
    "        return pyg_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def networkx_to_pyg(nx_graph):\n",
    "    \"\"\"\n",
    "    Convert a networkx graph to a torch_geometric.data.Data object.\n",
    "    This is a basic template; adjust for your actual node/edge features.\n",
    "    \"\"\"\n",
    "    # Sort nodes to ensure consistent ordering\n",
    "    # e.g. node 0, node 1, ...\n",
    "    # In some networkx graphs, node labels might be strings. We’ll map them to integers.\n",
    "    node_mapping = {node: i for i, node in enumerate(nx_graph.nodes())}\n",
    "\n",
    "    # Build lists for PyG\n",
    "    x_list = []\n",
    "    edge_index_list = []\n",
    "    edge_attr_list = []\n",
    "\n",
    "    for node in nx_graph.nodes(data=True):\n",
    "        original_id = node[0]\n",
    "        attrs = node[1]\n",
    "        # Example: 'symbol' might be in attrs, etc.\n",
    "        # For demonstration, let's store only \"symbol\" as a simple categorical embedding\n",
    "        # You might do something more sophisticated (e.g., one-hot) for real usage\n",
    "        symbol = attrs.get(\"symbol\", \"C\")\n",
    "        # Convert symbol to a simple ID (C=0, H=1, etc.) or some vector\n",
    "        # We'll do a naive approach here:\n",
    "        symbol_id = 0 if symbol == \"C\" else 1 if symbol == \"H\" else 2\n",
    "        \n",
    "        x_list.append([symbol_id])\n",
    "\n",
    "    for u, v, edge_attrs in nx_graph.edges(data=True):\n",
    "        u_idx = node_mapping[u]\n",
    "        v_idx = node_mapping[v]\n",
    "        edge_index_list.append((u_idx, v_idx))\n",
    "        # Possibly store bond features: \"bond_index\", \"bde_pred\", etc.\n",
    "        bde_pred = edge_attrs.get(\"bde_pred\", 0.0)\n",
    "        if bde_pred is None:\n",
    "            bde_pred = 0.0\n",
    "        bdfe_pred = edge_attrs.get(\"bdfe_pred\", 0.0)\n",
    "        if bdfe_pred is None:\n",
    "            bdfe_pred = 0.0\n",
    "        edge_attr_list.append([bde_pred, bdfe_pred])\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    x = torch.tensor(x_list, dtype=torch.float)  # shape [num_nodes, num_node_features]\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()  # shape [2, num_edges]\n",
    "    edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)  # shape [num_edges, edge_feat_dim]\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINE_Regression(nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dim: int = 128,\n",
    "                 num_layers: int = 3,\n",
    "                 dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        A more 'realistic' GNN for regression, using GINEConv layers + edge attributes.\n",
    "        \n",
    "        Args:\n",
    "            node_in_dim (int): Dim of node features (e.g. 1 or 3).\n",
    "            edge_in_dim (int): Dim of edge features (e.g. 2 for [bde_pred, bdfe_pred]).\n",
    "            external_in_dim (int): Dim of external factor features (e.g. 6).\n",
    "            hidden_dim (int): Hidden embedding size for GNN layers.\n",
    "            num_layers (int): Number of GNN layers.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # A learnable linear transform for edge features (required by GINEConv's \"nn\" argument):\n",
    "        # Typically GINEConv uses a small MLP to incorporate edge_attr into the message.\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # A learnable linear transform for node features:\n",
    "        self.node_encoder = nn.Linear(node_in_dim, hidden_dim)\n",
    "        \n",
    "        # Create multiple GINEConv layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            # GINEConv requires an MLP for node update:\n",
    "            # We'll use a simple 2-layer MLP\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dim))  # batch norm for stability\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # An MLP to process external factors\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression MLP after pooling + external embedding\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: PyG Data object, expected fields:\n",
    "                - x: Node features [num_nodes, node_in_dim]\n",
    "                - edge_index: [2, num_edges]\n",
    "                - edge_attr: [num_edges, edge_in_dim]\n",
    "                - batch: [num_nodes] mapping each node to a graph ID\n",
    "                - externals: [batch_size, external_in_dim]\n",
    "        Returns:\n",
    "            A tensor of shape [batch_size], the predicted regression value.\n",
    "        \"\"\"\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # 1) Encode node features and edge features\n",
    "        x = self.node_encoder(x)                 # [num_nodes, hidden_dim]\n",
    "        edge_emb = self.edge_encoder(edge_attr)  # [num_edges, hidden_dim]\n",
    "        \n",
    "        # 2) Pass through multiple GINEConv layers\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # 3) Global pooling to get graph embedding\n",
    "        graph_emb = global_mean_pool(x, batch)  # [batch_size, hidden_dim]\n",
    "\n",
    "        # 4) Process external factors\n",
    "        ext_emb = self.externals_mlp(data.externals)  # [batch_size, hidden_dim]\n",
    "\n",
    "        # 5) Combine + final regression\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)  # [batch_size, hidden_dim * 2]\n",
    "        out = self.final_regressor(combined).squeeze(-1)    # [batch_size]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_data)               # [batch_size]\n",
    "        y = batch_data.y.to(device).view(-1)    # [batch_size]\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "        count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            preds = model(batch_data)\n",
    "            y = batch_data.y.to(device).view(-1)\n",
    "            loss = criterion(preds, y)\n",
    "            total_loss += loss.item() * batch_data.num_graphs\n",
    "            count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset loader and compute R² and RMSE.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained GNN model.\n",
    "        loader (DataLoader): The PyG DataLoader for the evaluation dataset.\n",
    "        device (torch.device): The device to run on.\n",
    "    \n",
    "    Returns:\n",
    "        r2 (float): Coefficient of determination.\n",
    "        rmse (float): Root Mean Squared Error.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            preds = model(batch)\n",
    "            y_true.append(batch.y.cpu())\n",
    "            y_pred.append(preds.cpu())\n",
    "\n",
    "    # If your labels are stored as tensors with an extra dimension, use .squeeze() if needed.\n",
    "    y_true = torch.cat(y_true).numpy().squeeze()\n",
    "    y_pred = torch.cat(y_pred).numpy().squeeze()\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return r2, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_file = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\dataset02.xlsx\"\n",
    "\n",
    "data = pd.read_excel(env_file, engine='openpyxl').dropna(subset=['degradation_rate'])\n",
    "data['seawater'] = data['seawater'].map({'art': 1, 'sea': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1023 entries, 0 to 1039\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   data number       1023 non-null   float64\n",
      " 1   temperature       1023 non-null   float64\n",
      " 2   seawater          1023 non-null   int64  \n",
      " 3   concentration     1023 non-null   int64  \n",
      " 4   time              1023 non-null   int64  \n",
      " 5   component         1023 non-null   object \n",
      " 6   BDE               1023 non-null   float64\n",
      " 7   BDFE              1023 non-null   float64\n",
      " 8   energy            1023 non-null   float64\n",
      " 9   degradation_rate  1023 non-null   float64\n",
      "dtypes: float64(6), int64(3), object(1)\n",
      "memory usage: 87.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\molecules\"\n",
    "graph_pickles = [f for f in os.listdir(folder_path) if f.endswith(\".pkl\")]\n",
    "\n",
    "#graph_pickles = [f for f in os.listdir('./molecules') if f.endswith('.pkl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\molecules\n",
      "Files in directory: ['gpickle_graph_0.pkl', 'gpickle_graph_1.pkl', 'gpickle_graph_10.pkl', 'gpickle_graph_11.pkl', 'gpickle_graph_12.pkl', 'gpickle_graph_13.pkl', 'gpickle_graph_14.pkl', 'gpickle_graph_15.pkl', 'gpickle_graph_16.pkl', 'gpickle_graph_17.pkl', 'gpickle_graph_18.pkl', 'gpickle_graph_19.pkl', 'gpickle_graph_2.pkl', 'gpickle_graph_3.pkl', 'gpickle_graph_4.pkl', 'gpickle_graph_5.pkl', 'gpickle_graph_6.pkl', 'gpickle_graph_7.pkl', 'gpickle_graph_8.pkl', 'gpickle_graph_9.pkl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\molecules\"\n",
    "\n",
    "if os.path.exists(base_dir):\n",
    "    print(\"Directory exists:\", base_dir)\n",
    "    print(\"Files in directory:\", os.listdir(base_dir))\n",
    "else:\n",
    "    print(f\"Error: Directory {base_dir} does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = data.component.unique()\n",
    "graphs_dict = {}\n",
    "\n",
    "for compound, graph_pickle in zip(compounds, graph_pickles):\n",
    "    #with open(f'./molecules/{graph_pickle}', 'rb') as f:\n",
    "    with open(os.path.join(base_dir, graph_pickle), 'rb') as f:\n",
    "\n",
    "        graph = pickle.load(f)\n",
    "        graphs_dict[compound] = networkx_to_pyg(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "# train set st.\n",
    "dataset = MolDataset(raw_dataframe=data, nx_graph_dict=graphs_dict, component_col=\"component\", global_state_cols=[\"temperature\", \"concentration\", \"time\", \"seawater\"], label_col=\"degradation_rate\")\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "indices = list(range(len(data)))\n",
    "train_indices, test_indices = random_split(indices, [train_size, test_size])\n",
    "# random_split() 需要的数据类型是 Dataset 对象\n",
    "\n",
    "dataset.fit_standardizers(train_indices)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = PyGDataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader   = PyGDataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 2) Instantiate model + optimizer\n",
    "# -----------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GINE_Regression(\n",
    "    node_in_dim=1,\n",
    "    edge_in_dim=2,\n",
    "    external_in_dim=4,\n",
    "    hidden_dim=16,\n",
    "    num_layers=5,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] train_loss: 0.0377, val_loss: 0.0313\n",
      "[Epoch 20] train_loss: 0.0423, val_loss: 0.0963\n",
      "[Epoch 30] train_loss: 0.0345, val_loss: 0.0393\n",
      "[Epoch 40] train_loss: 0.0400, val_loss: 0.0307\n",
      "[Epoch 50] train_loss: 0.0426, val_loss: 0.0332\n",
      "[Epoch 60] train_loss: 0.0399, val_loss: 0.0307\n",
      "[Epoch 70] train_loss: 0.0410, val_loss: 0.0322\n",
      "[Epoch 80] train_loss: 0.0358, val_loss: 0.0306\n",
      "[Epoch 90] train_loss: 0.0400, val_loss: 0.0326\n",
      "[Epoch 100] train_loss: 0.0368, val_loss: 0.0305\n",
      "[Epoch 110] train_loss: 0.0370, val_loss: 0.0312\n",
      "[Epoch 120] train_loss: 0.0367, val_loss: 0.0307\n",
      "[Epoch 130] train_loss: 0.0427, val_loss: 0.0302\n",
      "[Epoch 140] train_loss: 0.0315, val_loss: 0.0311\n",
      "[Epoch 150] train_loss: 0.0471, val_loss: 0.0344\n",
      "[Epoch 160] train_loss: 0.0358, val_loss: 0.0312\n",
      "[Epoch 170] train_loss: 0.0428, val_loss: 0.0300\n",
      "[Epoch 180] train_loss: 0.0357, val_loss: 0.0304\n",
      "[Epoch 190] train_loss: 0.0355, val_loss: 0.0329\n",
      "[Epoch 200] train_loss: 0.0352, val_loss: 0.0303\n",
      "[Epoch 210] train_loss: 0.0371, val_loss: 0.0300\n",
      "[Epoch 220] train_loss: 0.0417, val_loss: 0.0303\n",
      "[Epoch 230] train_loss: 0.0298, val_loss: 0.0340\n",
      "[Epoch 240] train_loss: 0.0393, val_loss: 0.0304\n",
      "[Epoch 250] train_loss: 0.0373, val_loss: 0.0301\n",
      "[Epoch 260] train_loss: 0.0459, val_loss: 0.0304\n",
      "[Epoch 270] train_loss: 0.0404, val_loss: 0.0300\n",
      "[Epoch 280] train_loss: 0.0404, val_loss: 0.0311\n",
      "[Epoch 290] train_loss: 0.0445, val_loss: 0.0307\n",
      "[Epoch 300] train_loss: 0.0436, val_loss: 0.0296\n",
      "[Epoch 310] train_loss: 0.0341, val_loss: 0.0300\n",
      "[Epoch 320] train_loss: 0.0354, val_loss: 0.0309\n",
      "[Epoch 330] train_loss: 0.0386, val_loss: 0.0311\n",
      "[Epoch 340] train_loss: 0.0342, val_loss: 0.0304\n",
      "[Epoch 350] train_loss: 0.0332, val_loss: 0.0298\n",
      "[Epoch 360] train_loss: 0.0350, val_loss: 0.0305\n",
      "[Epoch 370] train_loss: 0.0385, val_loss: 0.0299\n",
      "[Epoch 380] train_loss: 0.0343, val_loss: 0.0299\n",
      "[Epoch 390] train_loss: 0.0455, val_loss: 0.0307\n",
      "[Epoch 400] train_loss: 0.0384, val_loss: 0.0422\n",
      "[Epoch 410] train_loss: 0.0370, val_loss: 0.0314\n",
      "[Epoch 420] train_loss: 0.0400, val_loss: 0.0300\n",
      "[Epoch 430] train_loss: 0.0336, val_loss: 0.0310\n",
      "[Epoch 440] train_loss: 0.0355, val_loss: 0.0300\n",
      "[Epoch 450] train_loss: 0.0379, val_loss: 0.0299\n",
      "[Epoch 460] train_loss: 0.0378, val_loss: 0.0301\n",
      "[Epoch 470] train_loss: 0.0376, val_loss: 0.0298\n",
      "[Epoch 480] train_loss: 0.0373, val_loss: 0.0298\n",
      "[Epoch 490] train_loss: 0.0401, val_loss: 0.0296\n",
      "[Epoch 500] train_loss: 0.0372, val_loss: 0.0299\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 3) Training Loop\n",
    "# -----------------------------------\n",
    "num_epochs = 500\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[Epoch {epoch}] train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}\")\n",
    "\n",
    "# Optionally, test or save the model\n",
    "# torch.save(model.state_dict(), \"trained_gine_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.5258\n",
      "RMSE: 0.1728\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage after training:\n",
    "r2, rmse = evaluate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfabet-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
