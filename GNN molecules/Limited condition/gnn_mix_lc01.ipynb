{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.要加三个约束条件让模型自己找规律\n",
    "A.bde/bdfe\n",
    "B.energy:空间复杂性\n",
    "C.C原子的 复杂性： 指标：周边原子数、平均bde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单添加三个条件的软约束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 12:47:17,633] A new study created in memory with name: no-name-d6eb2740-ca21-45f3-8d68-96c667e5e568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\n",
      "Files in directory: ['gpickle_graph_0.pkl', 'gpickle_graph_1.pkl', 'gpickle_graph_10.pkl', 'gpickle_graph_11.pkl', 'gpickle_graph_12.pkl', 'gpickle_graph_13.pkl', 'gpickle_graph_14.pkl', 'gpickle_graph_15.pkl', 'gpickle_graph_16.pkl', 'gpickle_graph_17.pkl', 'gpickle_graph_18.pkl', 'gpickle_graph_19.pkl', 'gpickle_graph_2.pkl', 'gpickle_graph_3.pkl', 'gpickle_graph_4.pkl', 'gpickle_graph_5.pkl', 'gpickle_graph_6.pkl', 'gpickle_graph_7.pkl', 'gpickle_graph_8.pkl', 'gpickle_graph_9.pkl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 13:25:50,279] Trial 0 finished with value: 0.6498154129034478 and parameters: {'hidden_dim': 8, 'num_layers': 3, 'dropout': 0.4, 'lr': 0.00022245941537414965, 'weight_decay': 3.247044171584706e-05, 'lambda_bde': 0.004616586913109393, 'lambda_energy': 0.0010743133480093142, 'lambda_c': 0.0002746876713893252}. Best is trial 0 with value: 0.6498154129034478.\n",
      "[I 2025-03-12 14:14:25,463] Trial 1 finished with value: 0.7361006607135934 and parameters: {'hidden_dim': 64, 'num_layers': 6, 'dropout': 0.4, 'lr': 0.004962370369068733, 'weight_decay': 0.0001343485824061221, 'lambda_bde': 0.010702283356320775, 'lambda_energy': 0.00022674793505244787, 'lambda_c': 0.007047858328958299}. Best is trial 0 with value: 0.6498154129034478.\n",
      "[I 2025-03-12 15:00:12,111] Trial 2 finished with value: 0.3035213933231613 and parameters: {'hidden_dim': 64, 'num_layers': 6, 'dropout': 0.5, 'lr': 0.0005408077716102977, 'weight_decay': 6.766651693564072e-05, 'lambda_bde': 0.00016876468231366942, 'lambda_energy': 0.0006878981403382621, 'lambda_c': 0.010363411103959986}. Best is trial 2 with value: 0.3035213933231613.\n",
      "[I 2025-03-12 15:40:08,461] Trial 3 finished with value: 0.30424684133728064 and parameters: {'hidden_dim': 56, 'num_layers': 3, 'dropout': 0.30000000000000004, 'lr': 0.0004866644996128066, 'weight_decay': 2.888234529697546e-06, 'lambda_bde': 0.00039661978001418716, 'lambda_energy': 0.00063069836106915, 'lambda_c': 0.011359322873505714}. Best is trial 2 with value: 0.3035213933231613.\n",
      "[I 2025-03-12 16:22:22,820] Trial 4 finished with value: -44.0877694600715 and parameters: {'hidden_dim': 48, 'num_layers': 5, 'dropout': 0.30000000000000004, 'lr': 0.007146062523938986, 'weight_decay': 5.797289001421735e-05, 'lambda_bde': 0.00018256637786846574, 'lambda_energy': 0.039991691601718574, 'lambda_c': 0.00022296916048617115}. Best is trial 4 with value: -44.0877694600715.\n",
      "[I 2025-03-12 17:02:12,951] Trial 5 finished with value: -28.984254278605317 and parameters: {'hidden_dim': 16, 'num_layers': 6, 'dropout': 0.1, 'lr': 0.0014994523362637637, 'weight_decay': 1.8767059389537228e-06, 'lambda_bde': 0.02269092947619175, 'lambda_energy': 0.02699891869000693, 'lambda_c': 0.05080167429870281}. Best is trial 4 with value: -44.0877694600715.\n",
      "[I 2025-03-12 17:40:44,620] Trial 6 finished with value: -73.24290909988352 and parameters: {'hidden_dim': 56, 'num_layers': 2, 'dropout': 0.2, 'lr': 0.0009751297308430615, 'weight_decay': 1.429309662126955e-05, 'lambda_bde': 0.002575243259503391, 'lambda_energy': 0.049456732104875105, 'lambda_c': 0.0006547632142607776}. Best is trial 6 with value: -73.24290909988352.\n",
      "[I 2025-03-12 18:18:21,829] Trial 7 finished with value: 0.3845022221149753 and parameters: {'hidden_dim': 8, 'num_layers': 2, 'dropout': 0.1, 'lr': 0.00017058101400309596, 'weight_decay': 7.756446263107785e-06, 'lambda_bde': 0.002792747387573866, 'lambda_energy': 0.00031837273325016304, 'lambda_c': 0.0007210333083680351}. Best is trial 6 with value: -73.24290909988352.\n",
      "[I 2025-03-12 19:00:43,717] Trial 8 finished with value: 0.7679769849298332 and parameters: {'hidden_dim': 64, 'num_layers': 6, 'dropout': 0.4, 'lr': 0.006092748245072442, 'weight_decay': 3.440345753028289e-06, 'lambda_bde': 0.0001967081115410136, 'lambda_energy': 0.005287616176679988, 'lambda_c': 0.0010082522113099596}. Best is trial 6 with value: -73.24290909988352.\n",
      "[I 2025-03-12 19:39:44,323] Trial 9 finished with value: 0.6893826378480719 and parameters: {'hidden_dim': 16, 'num_layers': 4, 'dropout': 0.0, 'lr': 0.0033376476771551176, 'weight_decay': 0.0001463647259283067, 'lambda_bde': 0.009335352033868883, 'lambda_energy': 0.00031176162857991346, 'lambda_c': 0.003038568570639232}. Best is trial 6 with value: -73.24290909988352.\n",
      "[I 2025-03-12 20:17:48,344] Trial 10 finished with value: -20.55970573002046 and parameters: {'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.2, 'lr': 0.0016239668071923923, 'weight_decay': 0.0004818418326163929, 'lambda_bde': 0.0722501731312029, 'lambda_energy': 0.0090541391617855, 'lambda_c': 0.00010173312509439983}. Best is trial 6 with value: -73.24290909988352.\n",
      "[I 2025-03-12 20:57:56,235] Trial 11 finished with value: -255.13986602330766 and parameters: {'hidden_dim': 48, 'num_layers': 4, 'dropout': 0.2, 'lr': 0.009221421079613663, 'weight_decay': 1.1601271399268611e-05, 'lambda_bde': 0.0009706453231494022, 'lambda_energy': 0.0891420174842775, 'lambda_c': 0.00040376047554385296}. Best is trial 11 with value: -255.13986602330766.\n",
      "[I 2025-03-12 21:37:38,960] Trial 12 finished with value: -216.3119882369988 and parameters: {'hidden_dim': 40, 'num_layers': 4, 'dropout': 0.2, 'lr': 0.0007850967807807969, 'weight_decay': 1.2098208974318114e-05, 'lambda_bde': 0.0010314532324193252, 'lambda_energy': 0.0824479523456953, 'lambda_c': 0.0012318302906290483}. Best is trial 11 with value: -255.13986602330766.\n",
      "[I 2025-03-12 22:17:16,033] Trial 13 finished with value: -298.63063032824357 and parameters: {'hidden_dim': 40, 'num_layers': 4, 'dropout': 0.1, 'lr': 0.002783889193902362, 'weight_decay': 1.0742179092685423e-05, 'lambda_bde': 0.0006842276824057646, 'lambda_energy': 0.0962419656747787, 'lambda_c': 0.001697598999869065}. Best is trial 13 with value: -298.63063032824357.\n",
      "[I 2025-03-12 22:57:16,361] Trial 14 finished with value: -7.048031998738781 and parameters: {'hidden_dim': 32, 'num_layers': 5, 'dropout': 0.0, 'lr': 0.002984812788888677, 'weight_decay': 7.540846470146902e-06, 'lambda_bde': 0.0007641461875292296, 'lambda_energy': 0.01911610832579261, 'lambda_c': 0.0032049134247966907}. Best is trial 13 with value: -298.63063032824357.\n",
      "[I 2025-03-12 23:36:30,800] Trial 15 finished with value: -215.23551583580624 and parameters: {'hidden_dim': 40, 'num_layers': 3, 'dropout': 0.1, 'lr': 0.009783765567647068, 'weight_decay': 2.357302094610991e-05, 'lambda_bde': 0.0009477598465597917, 'lambda_energy': 0.08237034213822998, 'lambda_c': 0.0003258174926273053}. Best is trial 13 with value: -298.63063032824357.\n",
      "[I 2025-03-13 00:16:58,723] Trial 16 finished with value: -2.1426348855699553 and parameters: {'hidden_dim': 48, 'num_layers': 5, 'dropout': 0.1, 'lr': 0.002500376917672802, 'weight_decay': 4.644703995955455e-06, 'lambda_bde': 0.00045347095950518904, 'lambda_energy': 0.013433358457524388, 'lambda_c': 0.002534586623152428}. Best is trial 13 with value: -298.63063032824357.\n",
      "[I 2025-03-13 00:56:54,201] Trial 17 finished with value: 0.7152278673449296 and parameters: {'hidden_dim': 48, 'num_layers': 4, 'dropout': 0.30000000000000004, 'lr': 0.003944631515899986, 'weight_decay': 1.265913544173789e-06, 'lambda_bde': 0.0016593094910271344, 'lambda_energy': 0.002393828251045417, 'lambda_c': 0.035543303052181514}. Best is trial 13 with value: -298.63063032824357.\n",
      "[I 2025-03-13 01:36:04,831] Trial 18 finished with value: 0.14218202070534827 and parameters: {'hidden_dim': 24, 'num_layers': 3, 'dropout': 0.0, 'lr': 0.00920065480232435, 'weight_decay': 0.000573288395028319, 'lambda_bde': 0.00043737529189880807, 'lambda_energy': 0.00010851972268407038, 'lambda_c': 0.0001048302943918859}. Best is trial 13 with value: -298.63063032824357.\n",
      "[I 2025-03-13 02:16:23,255] Trial 19 finished with value: -324.5517720481986 and parameters: {'hidden_dim': 40, 'num_layers': 5, 'dropout': 0.2, 'lr': 0.00196480286848805, 'weight_decay': 2.0267095851164693e-05, 'lambda_bde': 0.005450074237182427, 'lambda_energy': 0.09902377512485237, 'lambda_c': 0.0018969098937381469}. Best is trial 19 with value: -324.5517720481986.\n",
      "[I 2025-03-13 02:56:32,707] Trial 20 finished with value: -2.2208415936671164 and parameters: {'hidden_dim': 24, 'num_layers': 5, 'dropout': 0.1, 'lr': 0.0018413544266247274, 'weight_decay': 3.318883195089836e-05, 'lambda_bde': 0.03756974547537767, 'lambda_energy': 0.0033017356930993306, 'lambda_c': 0.0015793851229471028}. Best is trial 19 with value: -324.5517720481986.\n",
      "[I 2025-03-13 03:36:17,667] Trial 21 finished with value: -247.94517458594174 and parameters: {'hidden_dim': 40, 'num_layers': 4, 'dropout': 0.2, 'lr': 0.0022016904263828657, 'weight_decay': 1.5060889272238715e-05, 'lambda_bde': 0.007269712103949232, 'lambda_energy': 0.0863911883447414, 'lambda_c': 0.0004816578521197194}. Best is trial 19 with value: -324.5517720481986.\n",
      "[I 2025-03-13 04:16:25,942] Trial 22 finished with value: -45.3625884302651 and parameters: {'hidden_dim': 48, 'num_layers': 4, 'dropout': 0.2, 'lr': 0.004893168406476183, 'weight_decay': 8.117560498487432e-06, 'lambda_bde': 0.0017113894730482202, 'lambda_energy': 0.0400646407945583, 'lambda_c': 0.002058016689316518}. Best is trial 19 with value: -324.5517720481986.\n",
      "[I 2025-03-13 04:56:49,881] Trial 23 finished with value: -82.81307656928946 and parameters: {'hidden_dim': 32, 'num_layers': 5, 'dropout': 0.30000000000000004, 'lr': 0.0010266284384393386, 'weight_decay': 6.0507560754477434e-05, 'lambda_bde': 0.004815083657689111, 'lambda_energy': 0.0518081577338166, 'lambda_c': 0.004813365516843433}. Best is trial 19 with value: -324.5517720481986.\n",
      "[I 2025-03-13 05:37:52,284] Trial 24 finished with value: -10.923961524069451 and parameters: {'hidden_dim': 56, 'num_layers': 5, 'dropout': 0.1, 'lr': 0.0003429976588662148, 'weight_decay': 2.2186279934428618e-05, 'lambda_bde': 0.0015738575030802724, 'lambda_energy': 0.022134998776030354, 'lambda_c': 0.019742641707942148}. Best is trial 19 with value: -324.5517720481986.\n",
      "[I 2025-03-13 06:17:29,697] Trial 25 finished with value: -319.1598621673803 and parameters: {'hidden_dim': 40, 'num_layers': 4, 'dropout': 0.2, 'lr': 0.001414783928680162, 'weight_decay': 4.43458264075735e-06, 'lambda_bde': 0.0006110914260649461, 'lambda_energy': 0.09928523520799089, 'lambda_c': 0.0004532028957498596}. Best is trial 19 with value: -324.5517720481986.\n",
      "[I 2025-03-13 06:56:45,496] Trial 26 finished with value: -0.2663083464924882 and parameters: {'hidden_dim': 40, 'num_layers': 4, 'dropout': 0.0, 'lr': 0.0012106222471374418, 'weight_decay': 3.920741109715483e-06, 'lambda_bde': 0.00010152188886401826, 'lambda_energy': 0.009889553779365439, 'lambda_c': 0.001109596414275308}. Best is trial 19 with value: -324.5517720481986.\n",
      "[I 2025-03-13 07:35:16,968] Trial 27 finished with value: -37.33585385731679 and parameters: {'hidden_dim': 24, 'num_layers': 3, 'dropout': 0.1, 'lr': 0.0006144931066542714, 'weight_decay': 1.0139595883686733e-06, 'lambda_bde': 0.01531749636584243, 'lambda_energy': 0.03261301770375427, 'lambda_c': 0.09890292861538799}. Best is trial 19 with value: -324.5517720481986.\n",
      "[I 2025-03-13 08:15:28,837] Trial 28 finished with value: -106.59944493604355 and parameters: {'hidden_dim': 32, 'num_layers': 5, 'dropout': 0.2, 'lr': 0.0023450236756387415, 'weight_decay': 5.715433762082789e-06, 'lambda_bde': 0.0003159677183822213, 'lambda_energy': 0.05944060697607765, 'lambda_c': 0.00015829270908605602}. Best is trial 19 with value: -324.5517720481986.\n",
      "[I 2025-03-13 08:54:51,335] Trial 29 finished with value: -3.985000229109879 and parameters: {'hidden_dim': 40, 'num_layers': 4, 'dropout': 0.5, 'lr': 0.00010478960752379528, 'weight_decay': 1.879748424618966e-06, 'lambda_bde': 0.0006282471904911065, 'lambda_energy': 0.01627899932210922, 'lambda_c': 0.00435125355852627}. Best is trial 19 with value: -324.5517720481986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Optuna Study Results ===\n",
      "Best trial number: 19\n",
      "Best trial value (loss): -324.5518\n",
      "Best hyperparameters:\n",
      "  hidden_dim: 40\n",
      "  num_layers: 5\n",
      "  dropout: 0.2\n",
      "  lr: 0.00196480286848805\n",
      "  weight_decay: 2.0267095851164693e-05\n",
      "  lambda_bde: 0.005450074237182427\n",
      "  lambda_energy: 0.09902377512485237\n",
      "  lambda_c: 0.0018969098937381469\n",
      "\n",
      "--- Retrain Fold 1 with Best Hyperparams ---\n",
      "[Fold 1 Epoch 50] Train Loss: -330.9276 | Val Loss: -324.9432\n",
      "[Fold 1 Epoch 100] Train Loss: -330.6546 | Val Loss: -324.9934\n",
      "[Fold 1 Epoch 150] Train Loss: -332.2515 | Val Loss: -325.6322\n",
      "[Fold 1 Epoch 200] Train Loss: -331.6856 | Val Loss: -322.0673\n",
      "[Fold 1 Epoch 250] Train Loss: -332.2506 | Val Loss: -325.8296\n",
      "[Fold 1 Epoch 300] Train Loss: -332.4581 | Val Loss: -325.4078\n",
      "Evaluating fold 1 ...\n",
      "R²: -5785.0156\n",
      "RMSE: 18.0308\n",
      "\n",
      "--- Retrain Fold 2 with Best Hyperparams ---\n",
      "[Fold 2 Epoch 50] Train Loss: -330.6627 | Val Loss: -323.6939\n",
      "[Fold 2 Epoch 100] Train Loss: -330.7971 | Val Loss: -325.9724\n",
      "[Fold 2 Epoch 150] Train Loss: -330.8994 | Val Loss: -324.8595\n",
      "[Fold 2 Epoch 200] Train Loss: -331.8426 | Val Loss: -326.0700\n",
      "[Fold 2 Epoch 250] Train Loss: -331.7077 | Val Loss: -319.7154\n",
      "[Fold 2 Epoch 300] Train Loss: -331.2734 | Val Loss: -326.0648\n",
      "Evaluating fold 2 ...\n",
      "R²: -3231.6572\n",
      "RMSE: 18.3734\n",
      "\n",
      "--- Retrain Fold 3 with Best Hyperparams ---\n",
      "[Fold 3 Epoch 50] Train Loss: -329.5562 | Val Loss: -329.3179\n",
      "[Fold 3 Epoch 100] Train Loss: -330.1323 | Val Loss: -330.2166\n",
      "[Fold 3 Epoch 150] Train Loss: -329.8269 | Val Loss: -329.0209\n",
      "[Fold 3 Epoch 200] Train Loss: -330.6592 | Val Loss: -329.0035\n",
      "[Fold 3 Epoch 250] Train Loss: -330.2791 | Val Loss: -328.8384\n",
      "[Fold 3 Epoch 300] Train Loss: -330.6890 | Val Loss: -330.5111\n",
      "Evaluating fold 3 ...\n",
      "R²: -5196.4453\n",
      "RMSE: 19.0485\n",
      "\n",
      "--- Retrain Fold 4 with Best Hyperparams ---\n",
      "[Fold 4 Epoch 50] Train Loss: -329.9835 | Val Loss: -329.8283\n",
      "[Fold 4 Epoch 100] Train Loss: -329.9129 | Val Loss: -328.9961\n",
      "[Fold 4 Epoch 150] Train Loss: -330.5187 | Val Loss: -328.0782\n",
      "[Fold 4 Epoch 200] Train Loss: -330.8286 | Val Loss: -329.9390\n",
      "[Fold 4 Epoch 250] Train Loss: -329.5957 | Val Loss: -329.4438\n",
      "[Fold 4 Epoch 300] Train Loss: -330.1258 | Val Loss: -329.8427\n",
      "Evaluating fold 4 ...\n",
      "R²: -3903.2141\n",
      "RMSE: 18.7213\n",
      "\n",
      "--- Retrain Fold 5 with Best Hyperparams ---\n",
      "[Fold 5 Epoch 50] Train Loss: -330.0553 | Val Loss: -321.4999\n",
      "[Fold 5 Epoch 100] Train Loss: -331.1713 | Val Loss: -321.2680\n",
      "[Fold 5 Epoch 150] Train Loss: -331.1454 | Val Loss: -320.5553\n",
      "[Fold 5 Epoch 200] Train Loss: -332.3558 | Val Loss: -320.8422\n",
      "[Fold 5 Epoch 250] Train Loss: -332.2708 | Val Loss: -321.3243\n",
      "[Fold 5 Epoch 300] Train Loss: -331.6246 | Val Loss: -320.8642\n",
      "Evaluating fold 5 ...\n",
      "R²: -4650.5024\n",
      "RMSE: 17.9542\n",
      "\n",
      "--- Final Cross-Validation Summary with Best Hyperparams ---\n",
      "Fold 1: R² = -5785.0156, RMSE = 18.0308\n",
      "Fold 2: R² = -3231.6572, RMSE = 18.3734\n",
      "Fold 3: R² = -5196.4453, RMSE = 19.0485\n",
      "Fold 4: R² = -3903.2141, RMSE = 18.7213\n",
      "Fold 5: R² = -4650.5024, RMSE = 17.9542\n",
      "\n",
      "Average R²: -4553.3669 ± 906.5935\n",
      "Average RMSE: 18.4257 ± 0.4138\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2c642d49dc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool, BatchNorm\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import optuna  # <-- For hyperparameter optimization\n",
    "\n",
    "# Dash imports for showing results in a dashboard\n",
    "import dash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "import dash_table\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "#                             DATASET & PREP\n",
    "# =============================================================================\n",
    "\n",
    "class MolDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset that:\n",
    "      - Reads external factors from CSV\n",
    "      - Loads the corresponding pickle for the molecule's graph\n",
    "      - Converts it into a PyG Data object\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        raw_dataframe: pd.DataFrame,\n",
    "        nx_graph_dict: dict,\n",
    "        *,\n",
    "        component_col: str,\n",
    "        global_state_cols: list[str],\n",
    "        label_col: str,\n",
    "        transform=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_dataframe: The input dataframe containing molecule info.\n",
    "            nx_graph_dict: Dictionary mapping component names to networkx graphs.\n",
    "            component_col: Column name for the component.\n",
    "            global_state_cols: List of columns representing external factors.\n",
    "            label_col: Column name for the regression target.\n",
    "            transform: Any transform to apply to each PyG Data object.\n",
    "        \"\"\"\n",
    "        self.raw_dataframe = raw_dataframe\n",
    "        self.nx_graph_dict = nx_graph_dict\n",
    "        self.component_col = [component_col] if isinstance(component_col, str) else component_col\n",
    "        self.global_state_cols = global_state_cols\n",
    "        self.label_col = [label_col] if isinstance(label_col, str) else label_col\n",
    "        self.transform = transform\n",
    "\n",
    "        required_cols = set(self.global_state_cols + self.label_col + self.component_col)\n",
    "        for col in required_cols:\n",
    "            if col not in self.raw_dataframe.columns:\n",
    "                raise ValueError(f\"Missing column in DataFrame: '{col}'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.raw_dataframe.iloc[idx]\n",
    "\n",
    "        # 1. Load the molecule graph\n",
    "        component_name = row[self.component_col[0]]  # e.g. \"C23\"\n",
    "        pyg_data = self.nx_graph_dict[component_name]\n",
    "\n",
    "        # 2. Prepare the external factors\n",
    "        externals = torch.tensor(row[self.global_state_cols].values.astype(float), dtype=torch.float)\n",
    "        externals = externals.unsqueeze(0)\n",
    "\n",
    "        # 3. Prepare the label (regression target)\n",
    "        label = torch.tensor([row[self.label_col][0]], dtype=torch.float)\n",
    "\n",
    "        # 4. Attach externals & label to the Data object\n",
    "        pyg_data.externals = externals  # shape [1, external_in_dim]\n",
    "        pyg_data.y = label  # shape [1]\n",
    "\n",
    "        if self.transform:\n",
    "            pyg_data = self.transform(pyg_data)\n",
    "\n",
    "        return pyg_data\n",
    "\n",
    "\n",
    "def networkx_to_pyg(nx_graph):\n",
    "    \"\"\"\n",
    "    Convert a networkx graph to a torch_geometric.data.Data object.\n",
    "    This is a basic template; adjust for your actual node/edge features.\n",
    "    \"\"\"\n",
    "    # Sort nodes to ensure consistent ordering\n",
    "    node_mapping = {node: i for i, node in enumerate(nx_graph.nodes())}\n",
    "\n",
    "    x_list = []\n",
    "    edge_index_list = []\n",
    "    edge_attr_list = []\n",
    "\n",
    "    # Node features\n",
    "    for node in nx_graph.nodes(data=True):\n",
    "        original_id = node[0]\n",
    "        attrs = node[1]\n",
    "        symbol = attrs.get(\"symbol\", \"C\")\n",
    "        symbol_id = 0 if symbol == \"C\" else 1 if symbol == \"H\" else 2\n",
    "        # Here, x_list could be expanded to include 'energy' if your data provides it\n",
    "        x_list.append([symbol_id])\n",
    "\n",
    "    # Edge features\n",
    "    for u, v, edge_attrs in nx_graph.edges(data=True):\n",
    "        u_idx = node_mapping[u]\n",
    "        v_idx = node_mapping[v]\n",
    "        edge_index_list.append((u_idx, v_idx))\n",
    "        bde_pred = edge_attrs.get(\"bde_pred\", 0.0) or 0.0\n",
    "        bdfe_pred = edge_attrs.get(\"bdfe_pred\", 0.0) or 0.0\n",
    "        edge_attr_list.append([bde_pred, bdfe_pred])\n",
    "\n",
    "    x = torch.tensor(x_list, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                     BASE GNN MODEL (WITH DIM MATCH)\n",
    "# =============================================================================\n",
    "\n",
    "class GINE_Regression(nn.Module):\n",
    "    \"\"\"\n",
    "    A GNN for regression using GINEConv layers + edge attributes,\n",
    "    where all layers have the same hidden_dim (no dimension mismatch).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_dim: int,\n",
    "        edge_in_dim: int,\n",
    "        external_in_dim: int,\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 3,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encode edges from edge_in_dim to hidden_dim\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Encode nodes from node_in_dim to hidden_dim\n",
    "        self.node_encoder = nn.Linear(node_in_dim, hidden_dim)\n",
    "\n",
    "        # Multiple GINEConv layers & corresponding BatchNorm\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Process external factors\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Encode\n",
    "        x = self.node_encoder(x)\n",
    "        edge_emb = self.edge_encoder(edge_attr)\n",
    "\n",
    "        # Pass through GINEConv layers\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Global pooling\n",
    "        graph_emb = global_mean_pool(x, batch)\n",
    "\n",
    "        # Process external factors\n",
    "        ext_emb = self.externals_mlp(data.externals)\n",
    "\n",
    "        # Combine & regress\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)\n",
    "        out = self.final_regressor(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                   SOFT CONSTRAINTS (HELPER FUNCTIONS)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_c_neighbors(batch_data):\n",
    "    \"\"\"\n",
    "    Compute the total (or average) number of carbon neighbors across all nodes in the batch.\n",
    "    We'll do a simple approach:\n",
    "      - For each node, check how many edges connect to a node with symbol_id == 0 (C).\n",
    "      - Then average or sum across the graph.\n",
    "    This is just an example \"soft\" aggregator.\n",
    "    \"\"\"\n",
    "    edge_index = batch_data.edge_index\n",
    "    symbol_ids = batch_data.x[:, 0]\n",
    "\n",
    "    c_neighbor_count = torch.zeros_like(symbol_ids)\n",
    "    num_edges = edge_index.size(1)\n",
    "\n",
    "    for e in range(num_edges):\n",
    "        src = edge_index[0, e]\n",
    "        tgt = edge_index[1, e]\n",
    "        # If the target is carbon\n",
    "        if symbol_ids[tgt] == 0:  # 0 => 'C'\n",
    "            c_neighbor_count[src] += 1\n",
    "        # If the source is carbon\n",
    "        if symbol_ids[src] == 0:\n",
    "            c_neighbor_count[tgt] += 1\n",
    "\n",
    "    avg_c_neighbors = c_neighbor_count.mean()\n",
    "    return avg_c_neighbors\n",
    "\n",
    "\n",
    "def compute_bde_bdfe(batch_data):\n",
    "    \"\"\"\n",
    "    Compute an aggregated BDE/BDFe value for the entire graph.\n",
    "    We'll just take the mean of bde_pred and bdfe_pred across edges as an example.\n",
    "    \"\"\"\n",
    "    edge_attrs = batch_data.edge_attr  # shape [E, 2] => columns: [bde_pred, bdfe_pred]\n",
    "    if edge_attrs.size(0) == 0:\n",
    "        return 0.0, 0.0\n",
    "    bde_mean = edge_attrs[:, 0].mean()\n",
    "    bdfe_mean = edge_attrs[:, 1].mean()\n",
    "    return bde_mean, bdfe_mean\n",
    "\n",
    "\n",
    "def compute_atom_energy(batch_data):\n",
    "    \"\"\"\n",
    "    If you store 'energy' in node features or in a separate attribute,\n",
    "    here's where you'd aggregate it. We only have 'symbol_id' for now.\n",
    "    So let's do a dummy approach: sum the symbol IDs as a stand-in \"energy\".\n",
    "    \"\"\"\n",
    "    energy = batch_data.x[:, 0].sum()  # Just a placeholder\n",
    "    return energy\n",
    "\n",
    "\n",
    "def compute_soft_constraints(batch_data, preds):\n",
    "    \"\"\"\n",
    "    Returns the three soft constraint losses:\n",
    "      - BDE_BDFe loss\n",
    "      - Energy loss\n",
    "      - Num C neighbors loss\n",
    "    Each is defined as mean(pred * aggregated_value).\n",
    "    \"\"\"\n",
    "    # 1) BDE and BDFe\n",
    "    bde_mean, bdfe_mean = compute_bde_bdfe(batch_data)\n",
    "    aggregated_bde = bde_mean + bdfe_mean\n",
    "    loss_bde = (preds * aggregated_bde).mean()\n",
    "\n",
    "    # 2) Atom energy\n",
    "    aggregated_energy = compute_atom_energy(batch_data)\n",
    "    loss_energy = (preds * aggregated_energy).mean()\n",
    "\n",
    "    # 3) Number of C neighbors\n",
    "    avg_c_neighbors = compute_c_neighbors(batch_data)\n",
    "    loss_c = (preds * avg_c_neighbors).mean()\n",
    "\n",
    "    return loss_bde, loss_energy, loss_c\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                   TRAIN/VALID/EVALUATION UTILS (WITH SOFT CONSTRAINTS)\n",
    "# =============================================================================\n",
    "\n",
    "def train_one_epoch_with_constraints(\n",
    "    model, loader, optimizer, device,\n",
    "    base_criterion, lambda_bde=0.001, lambda_energy=0.001, lambda_c=0.001\n",
    "):\n",
    "    \"\"\"\n",
    "    Train for one epoch, combining MSE (or other base_criterion) + additional soft constraints.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_data)\n",
    "        y = batch_data.y.to(device).view(-1)\n",
    "\n",
    "        # Base regression loss\n",
    "        base_loss = base_criterion(preds, y)\n",
    "\n",
    "        # Soft constraints\n",
    "        loss_bde, loss_energy, loss_c = compute_soft_constraints(batch_data, preds)\n",
    "        total_soft_loss = lambda_bde * loss_bde + lambda_energy * loss_energy + lambda_c * loss_c\n",
    "\n",
    "        # Final combined loss\n",
    "        loss = base_loss + total_soft_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "        count += batch_data.num_graphs\n",
    "\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def validate_with_constraints(model, loader, device, base_criterion,\n",
    "                              lambda_bde=0.001, lambda_energy=0.001, lambda_c=0.001):\n",
    "    \"\"\"\n",
    "    Validate for one epoch, combining MSE (or other base_criterion) + additional soft constraints.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            preds = model(batch_data)\n",
    "            y = batch_data.y.to(device).view(-1)\n",
    "\n",
    "            # Base regression loss\n",
    "            base_loss = base_criterion(preds, y)\n",
    "\n",
    "            # Soft constraints\n",
    "            loss_bde, loss_energy, loss_c = compute_soft_constraints(batch_data, preds)\n",
    "            total_soft_loss = lambda_bde * loss_bde + lambda_energy * loss_energy + lambda_c * loss_c\n",
    "\n",
    "            loss = base_loss + total_soft_loss\n",
    "\n",
    "            total_loss += loss.item() * batch_data.num_graphs\n",
    "            count += batch_data.num_graphs\n",
    "\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset loader and compute R² and RMSE.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            preds = model(batch)\n",
    "            y_true.append(batch.y.cpu())\n",
    "            y_pred.append(preds.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true).numpy().squeeze()\n",
    "    y_pred = torch.cat(y_pred).numpy().squeeze()\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return r2, rmse\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                             DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "env_file = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\dataset02.xlsx\"\n",
    "data = pd.read_excel(env_file, engine='openpyxl').dropna(subset=['degradation_rate'])\n",
    "data['seawater'] = data['seawater'].map({'art': 1, 'sea': 0})\n",
    "\n",
    "folder_path = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\"\n",
    "graph_pickles = [f for f in os.listdir(folder_path) if f.endswith(\".pkl\")]\n",
    "\n",
    "base_dir = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\"\n",
    "if os.path.exists(base_dir):\n",
    "    print(\"Directory exists:\", base_dir)\n",
    "    print(\"Files in directory:\", os.listdir(base_dir))\n",
    "else:\n",
    "    print(f\"Error: Directory {base_dir} does not exist!\")\n",
    "\n",
    "compounds = data.component.unique()\n",
    "graphs_dict = {}\n",
    "for compound, graph_pickle in zip(compounds, graph_pickles):\n",
    "    with open(os.path.join(base_dir, graph_pickle), 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "        graphs_dict[compound] = networkx_to_pyg(graph)\n",
    "\n",
    "dataset = MolDataset(\n",
    "    raw_dataframe=data,\n",
    "    nx_graph_dict=graphs_dict,\n",
    "    component_col=\"component\",\n",
    "    global_state_cols=[\"temperature\", \"concentration\", \"time\", \"seawater\"],\n",
    "    label_col=\"degradation_rate\",\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "#                      OPTUNA HYPERPARAM OPT + CROSS-VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "from torch_geometric.data import DataLoader as PyGDataLoader\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function:\n",
    "      - Sample hyperparams\n",
    "      - Do a 5-fold cross validation\n",
    "      - Return avg validation loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters to tune:\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 8, 64, step=8)  # Example range\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5, step=0.1)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "\n",
    "    # Soft constraint lambdas:\n",
    "    lambda_bde = trial.suggest_float(\"lambda_bde\", 1e-4, 1e-1, log=True)\n",
    "    lambda_energy = trial.suggest_float(\"lambda_energy\", 1e-4, 1e-1, log=True)\n",
    "    lambda_c = trial.suggest_float(\"lambda_c\", 1e-4, 1e-1, log=True)\n",
    "\n",
    "    # We'll do 5-fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    fold_val_losses = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "        val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "        model = GINE_Regression(\n",
    "            node_in_dim=1,\n",
    "            edge_in_dim=2,\n",
    "            external_in_dim=4,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        base_criterion = torch.nn.MSELoss()\n",
    "\n",
    "        # We'll do fewer epochs for faster hyperparam search\n",
    "        num_epochs = 100\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = train_one_epoch_with_constraints(\n",
    "                model,\n",
    "                train_loader,\n",
    "                optimizer,\n",
    "                device,\n",
    "                base_criterion,\n",
    "                lambda_bde=lambda_bde,\n",
    "                lambda_energy=lambda_energy,\n",
    "                lambda_c=lambda_c\n",
    "            )\n",
    "            val_loss = validate_with_constraints(\n",
    "                model,\n",
    "                val_loader,\n",
    "                device,\n",
    "                base_criterion,\n",
    "                lambda_bde=lambda_bde,\n",
    "                lambda_energy=lambda_energy,\n",
    "                lambda_c=lambda_c\n",
    "            )\n",
    "\n",
    "        fold_val_losses.append(val_loss)\n",
    "\n",
    "    # Return the average of the validation losses\n",
    "    avg_val_loss = float(np.mean(fold_val_losses))\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "# Create Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"\\n=== Optuna Study Results ===\")\n",
    "print(f\"Best trial number: {study.best_trial.number}\")\n",
    "print(f\"Best trial value (loss): {study.best_trial.value:.4f}\")\n",
    "print(\"Best hyperparameters:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# =============================================================================\n",
    "#          RETRAIN FINAL MODEL WITH BEST HYPERPARAMS + REPORT TEST METRICS\n",
    "# =============================================================================\n",
    "\n",
    "best_params = study.best_params\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "hidden_dim = best_params[\"hidden_dim\"]\n",
    "num_layers = best_params[\"num_layers\"]\n",
    "dropout = best_params[\"dropout\"]\n",
    "lr = best_params[\"lr\"]\n",
    "weight_decay = best_params[\"weight_decay\"]\n",
    "lambda_bde = best_params[\"lambda_bde\"]\n",
    "lambda_energy = best_params[\"lambda_energy\"]\n",
    "lambda_c = best_params[\"lambda_c\"]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"\\n--- Retrain Fold {fold + 1} with Best Hyperparams ---\")\n",
    "\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "    val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = GINE_Regression(\n",
    "        node_in_dim=1,\n",
    "        edge_in_dim=2,\n",
    "        external_in_dim=4,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    base_criterion = nn.MSELoss()\n",
    "\n",
    "    num_epochs = 300\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_one_epoch_with_constraints(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            device,\n",
    "            base_criterion,\n",
    "            lambda_bde=lambda_bde,\n",
    "            lambda_energy=lambda_energy,\n",
    "            lambda_c=lambda_c\n",
    "        )\n",
    "        val_loss = validate_with_constraints(\n",
    "            model,\n",
    "            val_loader,\n",
    "            device,\n",
    "            base_criterion,\n",
    "            lambda_bde=lambda_bde,\n",
    "            lambda_energy=lambda_energy,\n",
    "            lambda_c=lambda_c\n",
    "        )\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"[Fold {fold + 1} Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    print(f\"Evaluating fold {fold + 1} ...\")\n",
    "    r2, rmse = evaluate_model(model, val_loader, device)\n",
    "    fold_results.append({\"fold\": fold + 1, \"r2\": r2, \"rmse\": rmse})\n",
    "\n",
    "r2_scores = [res[\"r2\"] for res in fold_results]\n",
    "rmse_scores = [res[\"rmse\"] for res in fold_results]\n",
    "\n",
    "print(\"\\n--- Final Cross-Validation Summary with Best Hyperparams ---\")\n",
    "for res in fold_results:\n",
    "    print(f\"Fold {res['fold']}: R² = {res['r2']:.4f}, RMSE = {res['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                           DASHBOARD FOR RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "# Convert fold_results to a DataFrame for easy display\n",
    "fold_results_df = pd.DataFrame(fold_results)\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Optuna + GNN Training Results Dashboard\"),\n",
    "    html.H2(\"Best Hyperparameters Found by Optuna\"),\n",
    "    html.Ul([\n",
    "        html.Li(f\"{key}: {value}\") for key, value in best_params.items()\n",
    "    ]),\n",
    "    html.P(f\"Best trial value (loss): {study.best_trial.value:.4f}\"),\n",
    "    html.Br(),\n",
    "    html.H2(\"Cross-Validation Results with Best Hyperparams\"),\n",
    "    dash_table.DataTable(\n",
    "        id='fold-results-table',\n",
    "        columns=[{\"name\": i, \"id\": i} for i in fold_results_df.columns],\n",
    "        data=fold_results_df.to_dict('records'),\n",
    "        style_table={'overflowX': 'auto'},\n",
    "        style_cell={'textAlign': 'left'}\n",
    "    ),\n",
    "    html.Br(),\n",
    "    html.Div([\n",
    "        html.H3(\"Averaged Metrics Across Folds:\"),\n",
    "        html.P(f\"Average R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\"),\n",
    "        html.P(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n",
    "    ])\n",
    "])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run the Dash app\n",
    "    # Note: The rest of the model training has already been done above.\n",
    "    app.run_server(debug=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\n",
      "Files in directory: ['gpickle_graph_0.pkl', 'gpickle_graph_1.pkl', 'gpickle_graph_10.pkl', 'gpickle_graph_11.pkl', 'gpickle_graph_12.pkl', 'gpickle_graph_13.pkl', 'gpickle_graph_14.pkl', 'gpickle_graph_15.pkl', 'gpickle_graph_16.pkl', 'gpickle_graph_17.pkl', 'gpickle_graph_18.pkl', 'gpickle_graph_19.pkl', 'gpickle_graph_2.pkl', 'gpickle_graph_3.pkl', 'gpickle_graph_4.pkl', 'gpickle_graph_5.pkl', 'gpickle_graph_6.pkl', 'gpickle_graph_7.pkl', 'gpickle_graph_8.pkl', 'gpickle_graph_9.pkl']\n",
      "\n",
      "--- Fold 1 ---\n",
      "[Fold 1 Epoch 10] Train Loss: 0.1393 | Val Loss: 0.0680\n",
      "[Fold 1 Epoch 20] Train Loss: 0.0879 | Val Loss: 0.0614\n",
      "[Fold 1 Epoch 30] Train Loss: 0.0824 | Val Loss: 0.0537\n",
      "[Fold 1 Epoch 40] Train Loss: 0.0880 | Val Loss: 0.0546\n",
      "[Fold 1 Epoch 50] Train Loss: 0.0699 | Val Loss: 0.0505\n",
      "[Fold 1 Epoch 60] Train Loss: 0.0712 | Val Loss: 0.0493\n",
      "[Fold 1 Epoch 70] Train Loss: 0.0664 | Val Loss: 0.0496\n",
      "[Fold 1 Epoch 80] Train Loss: 0.0801 | Val Loss: 0.0516\n",
      "[Fold 1 Epoch 90] Train Loss: 0.0682 | Val Loss: 0.0450\n",
      "[Fold 1 Epoch 100] Train Loss: 0.0678 | Val Loss: 0.0452\n",
      "[Fold 1 Epoch 110] Train Loss: 0.0600 | Val Loss: 0.0443\n",
      "[Fold 1 Epoch 120] Train Loss: 0.0625 | Val Loss: 0.0443\n",
      "[Fold 1 Epoch 130] Train Loss: 0.0574 | Val Loss: 0.0455\n",
      "[Fold 1 Epoch 140] Train Loss: 0.0605 | Val Loss: 0.0445\n",
      "[Fold 1 Epoch 150] Train Loss: 0.0555 | Val Loss: 0.0439\n",
      "[Fold 1 Epoch 160] Train Loss: 0.0606 | Val Loss: 0.0432\n",
      "[Fold 1 Epoch 170] Train Loss: 0.0640 | Val Loss: 0.0441\n",
      "[Fold 1 Epoch 180] Train Loss: 0.0534 | Val Loss: 0.0434\n",
      "[Fold 1 Epoch 190] Train Loss: 0.0515 | Val Loss: 0.0462\n",
      "[Fold 1 Epoch 200] Train Loss: 0.0591 | Val Loss: 0.0411\n",
      "[Fold 1 Epoch 210] Train Loss: 0.0590 | Val Loss: 0.0417\n",
      "[Fold 1 Epoch 220] Train Loss: 0.0538 | Val Loss: 0.0400\n",
      "[Fold 1 Epoch 230] Train Loss: 0.0559 | Val Loss: 0.0408\n",
      "[Fold 1 Epoch 240] Train Loss: 0.0482 | Val Loss: 0.0376\n",
      "[Fold 1 Epoch 250] Train Loss: 0.0450 | Val Loss: 0.0372\n",
      "[Fold 1 Epoch 260] Train Loss: 0.0474 | Val Loss: 0.0375\n",
      "[Fold 1 Epoch 270] Train Loss: 0.0464 | Val Loss: 0.0360\n",
      "[Fold 1 Epoch 280] Train Loss: 0.0428 | Val Loss: 0.0397\n",
      "[Fold 1 Epoch 290] Train Loss: 0.0789 | Val Loss: 0.0559\n",
      "[Fold 1 Epoch 300] Train Loss: 0.0659 | Val Loss: 0.0499\n",
      "[Fold 1 Epoch 310] Train Loss: 0.0461 | Val Loss: 0.0354\n",
      "[Fold 1 Epoch 320] Train Loss: 0.0358 | Val Loss: 0.0347\n",
      "[Fold 1 Epoch 330] Train Loss: 0.0417 | Val Loss: 0.0332\n",
      "[Fold 1 Epoch 340] Train Loss: 0.0401 | Val Loss: 0.0378\n",
      "[Fold 1 Epoch 350] Train Loss: 0.0391 | Val Loss: 0.0327\n",
      "[Fold 1 Epoch 360] Train Loss: 0.0387 | Val Loss: 0.0338\n",
      "[Fold 1 Epoch 370] Train Loss: 0.0415 | Val Loss: 0.0354\n",
      "[Fold 1 Epoch 380] Train Loss: 0.0479 | Val Loss: 0.0365\n",
      "[Fold 1 Epoch 390] Train Loss: 0.0421 | Val Loss: 0.0354\n",
      "[Fold 1 Epoch 400] Train Loss: 0.0377 | Val Loss: 0.0358\n",
      "[Fold 1 Epoch 410] Train Loss: 0.0381 | Val Loss: 0.0328\n",
      "[Fold 1 Epoch 420] Train Loss: 0.0377 | Val Loss: 0.0335\n",
      "[Fold 1 Epoch 430] Train Loss: 0.0431 | Val Loss: 0.0339\n",
      "[Fold 1 Epoch 440] Train Loss: 0.0430 | Val Loss: 0.0374\n",
      "[Fold 1 Epoch 450] Train Loss: 0.0403 | Val Loss: 0.0328\n",
      "[Fold 1 Epoch 460] Train Loss: 0.0347 | Val Loss: 0.0333\n",
      "[Fold 1 Epoch 470] Train Loss: 0.0442 | Val Loss: 0.0358\n",
      "[Fold 1 Epoch 480] Train Loss: 0.0420 | Val Loss: 0.0352\n",
      "[Fold 1 Epoch 490] Train Loss: 0.0444 | Val Loss: 0.0324\n",
      "[Fold 1 Epoch 500] Train Loss: 0.0354 | Val Loss: 0.0320\n",
      "Evaluating fold 1 ...\n",
      "R²: 0.4298\n",
      "RMSE: 0.1790\n",
      "\n",
      "--- Fold 2 ---\n",
      "[Fold 2 Epoch 10] Train Loss: 0.1283 | Val Loss: 0.1175\n",
      "[Fold 2 Epoch 20] Train Loss: 0.1030 | Val Loss: 0.1224\n",
      "[Fold 2 Epoch 30] Train Loss: 0.0931 | Val Loss: 0.1070\n",
      "[Fold 2 Epoch 40] Train Loss: 0.0741 | Val Loss: 0.0957\n",
      "[Fold 2 Epoch 50] Train Loss: 0.0696 | Val Loss: 0.1045\n",
      "[Fold 2 Epoch 60] Train Loss: 0.0735 | Val Loss: 0.0971\n",
      "[Fold 2 Epoch 70] Train Loss: 0.0525 | Val Loss: 0.0928\n",
      "[Fold 2 Epoch 80] Train Loss: 0.0648 | Val Loss: 0.0866\n",
      "[Fold 2 Epoch 90] Train Loss: 0.0637 | Val Loss: 0.0928\n",
      "[Fold 2 Epoch 100] Train Loss: 0.0628 | Val Loss: 0.0866\n",
      "[Fold 2 Epoch 110] Train Loss: 0.0567 | Val Loss: 0.0822\n",
      "[Fold 2 Epoch 120] Train Loss: 0.0640 | Val Loss: 0.0975\n",
      "[Fold 2 Epoch 130] Train Loss: 0.0574 | Val Loss: 0.0800\n",
      "[Fold 2 Epoch 140] Train Loss: 0.0644 | Val Loss: 0.0763\n",
      "[Fold 2 Epoch 150] Train Loss: 0.0483 | Val Loss: 0.0880\n",
      "[Fold 2 Epoch 160] Train Loss: 0.0615 | Val Loss: 0.0733\n",
      "[Fold 2 Epoch 170] Train Loss: 0.0503 | Val Loss: 0.0818\n",
      "[Fold 2 Epoch 180] Train Loss: 0.0560 | Val Loss: 0.1014\n",
      "[Fold 2 Epoch 190] Train Loss: 0.0540 | Val Loss: 0.0783\n",
      "[Fold 2 Epoch 200] Train Loss: 0.0493 | Val Loss: 0.0844\n",
      "[Fold 2 Epoch 210] Train Loss: 0.0563 | Val Loss: 0.0825\n",
      "[Fold 2 Epoch 220] Train Loss: 0.0445 | Val Loss: 0.0755\n",
      "[Fold 2 Epoch 230] Train Loss: 0.0484 | Val Loss: 0.0709\n",
      "[Fold 2 Epoch 240] Train Loss: 0.0450 | Val Loss: 0.0580\n",
      "[Fold 2 Epoch 250] Train Loss: 0.0521 | Val Loss: 0.0602\n",
      "[Fold 2 Epoch 260] Train Loss: 0.0490 | Val Loss: 0.0706\n",
      "[Fold 2 Epoch 270] Train Loss: 0.0438 | Val Loss: 0.0660\n",
      "[Fold 2 Epoch 280] Train Loss: 0.0494 | Val Loss: 0.0699\n",
      "[Fold 2 Epoch 290] Train Loss: 0.0421 | Val Loss: 0.0673\n",
      "[Fold 2 Epoch 300] Train Loss: 0.0391 | Val Loss: 0.0597\n",
      "[Fold 2 Epoch 310] Train Loss: 0.0384 | Val Loss: 0.0840\n",
      "[Fold 2 Epoch 320] Train Loss: 0.0432 | Val Loss: 0.0582\n",
      "[Fold 2 Epoch 330] Train Loss: 0.0397 | Val Loss: 0.0532\n",
      "[Fold 2 Epoch 340] Train Loss: 0.0414 | Val Loss: 0.0552\n",
      "[Fold 2 Epoch 350] Train Loss: 0.0355 | Val Loss: 0.1044\n",
      "[Fold 2 Epoch 360] Train Loss: 0.0395 | Val Loss: 0.0855\n",
      "[Fold 2 Epoch 370] Train Loss: 0.0383 | Val Loss: 0.0570\n",
      "[Fold 2 Epoch 380] Train Loss: 0.0393 | Val Loss: 0.0550\n",
      "[Fold 2 Epoch 390] Train Loss: 0.0426 | Val Loss: 0.0602\n",
      "[Fold 2 Epoch 400] Train Loss: 0.0385 | Val Loss: 0.0553\n",
      "[Fold 2 Epoch 410] Train Loss: 0.0337 | Val Loss: 0.0536\n",
      "[Fold 2 Epoch 420] Train Loss: 0.0380 | Val Loss: 0.0558\n",
      "[Fold 2 Epoch 430] Train Loss: 0.0368 | Val Loss: 0.0609\n",
      "[Fold 2 Epoch 440] Train Loss: 0.0457 | Val Loss: 0.0577\n",
      "[Fold 2 Epoch 450] Train Loss: 0.0337 | Val Loss: 0.0597\n",
      "[Fold 2 Epoch 460] Train Loss: 0.0384 | Val Loss: 0.0558\n",
      "[Fold 2 Epoch 470] Train Loss: 0.0380 | Val Loss: 0.0547\n",
      "[Fold 2 Epoch 480] Train Loss: 0.0355 | Val Loss: 0.0631\n",
      "[Fold 2 Epoch 490] Train Loss: 0.0359 | Val Loss: 0.0665\n",
      "[Fold 2 Epoch 500] Train Loss: 0.0397 | Val Loss: 0.0840\n",
      "Evaluating fold 2 ...\n",
      "R²: 0.1952\n",
      "RMSE: 0.2899\n",
      "\n",
      "--- Fold 3 ---\n",
      "[Fold 3 Epoch 10] Train Loss: 0.2200 | Val Loss: 0.0845\n",
      "[Fold 3 Epoch 20] Train Loss: 0.0990 | Val Loss: 0.0796\n",
      "[Fold 3 Epoch 30] Train Loss: 0.0893 | Val Loss: 0.0671\n",
      "[Fold 3 Epoch 40] Train Loss: 0.0965 | Val Loss: 0.0719\n",
      "[Fold 3 Epoch 50] Train Loss: 0.0755 | Val Loss: 0.0760\n",
      "[Fold 3 Epoch 60] Train Loss: 0.0707 | Val Loss: 0.0621\n",
      "[Fold 3 Epoch 70] Train Loss: 0.0805 | Val Loss: 0.0738\n",
      "[Fold 3 Epoch 80] Train Loss: 0.0749 | Val Loss: 0.0600\n",
      "[Fold 3 Epoch 90] Train Loss: 0.0733 | Val Loss: 0.0636\n",
      "[Fold 3 Epoch 100] Train Loss: 0.0743 | Val Loss: 0.0689\n",
      "[Fold 3 Epoch 110] Train Loss: 0.0647 | Val Loss: 0.0615\n",
      "[Fold 3 Epoch 120] Train Loss: 0.0651 | Val Loss: 0.0662\n",
      "[Fold 3 Epoch 130] Train Loss: 0.0710 | Val Loss: 0.0647\n",
      "[Fold 3 Epoch 140] Train Loss: 0.0603 | Val Loss: 0.0575\n",
      "[Fold 3 Epoch 150] Train Loss: 0.0574 | Val Loss: 0.0618\n",
      "[Fold 3 Epoch 160] Train Loss: 0.0627 | Val Loss: 0.0595\n",
      "[Fold 3 Epoch 170] Train Loss: 0.0558 | Val Loss: 0.0671\n",
      "[Fold 3 Epoch 180] Train Loss: 0.0539 | Val Loss: 0.0562\n",
      "[Fold 3 Epoch 190] Train Loss: 0.0588 | Val Loss: 0.0517\n",
      "[Fold 3 Epoch 200] Train Loss: 0.0612 | Val Loss: 0.0547\n",
      "[Fold 3 Epoch 210] Train Loss: 0.0620 | Val Loss: 0.0472\n",
      "[Fold 3 Epoch 220] Train Loss: 0.0559 | Val Loss: 0.0476\n",
      "[Fold 3 Epoch 230] Train Loss: 0.0460 | Val Loss: 0.0484\n",
      "[Fold 3 Epoch 240] Train Loss: 0.0493 | Val Loss: 0.0447\n",
      "[Fold 3 Epoch 250] Train Loss: 0.0512 | Val Loss: 0.0486\n",
      "[Fold 3 Epoch 260] Train Loss: 0.0542 | Val Loss: 0.0402\n",
      "[Fold 3 Epoch 270] Train Loss: 0.0471 | Val Loss: 0.0422\n",
      "[Fold 3 Epoch 280] Train Loss: 0.0480 | Val Loss: 0.0408\n",
      "[Fold 3 Epoch 290] Train Loss: 0.0443 | Val Loss: 0.0395\n",
      "[Fold 3 Epoch 300] Train Loss: 0.0415 | Val Loss: 0.0335\n",
      "[Fold 3 Epoch 310] Train Loss: 0.0407 | Val Loss: 0.0363\n",
      "[Fold 3 Epoch 320] Train Loss: 0.0415 | Val Loss: 0.0358\n",
      "[Fold 3 Epoch 330] Train Loss: 0.0520 | Val Loss: 0.0363\n",
      "[Fold 3 Epoch 340] Train Loss: 0.0467 | Val Loss: 0.0421\n",
      "[Fold 3 Epoch 350] Train Loss: 0.0502 | Val Loss: 0.0361\n",
      "[Fold 3 Epoch 360] Train Loss: 0.0491 | Val Loss: 0.0356\n",
      "[Fold 3 Epoch 370] Train Loss: 0.0455 | Val Loss: 0.0335\n",
      "[Fold 3 Epoch 380] Train Loss: 0.0442 | Val Loss: 0.0365\n",
      "[Fold 3 Epoch 390] Train Loss: 0.0466 | Val Loss: 0.0354\n",
      "[Fold 3 Epoch 400] Train Loss: 0.0427 | Val Loss: 0.0354\n",
      "[Fold 3 Epoch 410] Train Loss: 0.0446 | Val Loss: 0.0363\n",
      "[Fold 3 Epoch 420] Train Loss: 0.0408 | Val Loss: 0.0362\n",
      "[Fold 3 Epoch 430] Train Loss: 0.0479 | Val Loss: 0.0356\n",
      "[Fold 3 Epoch 440] Train Loss: 0.0423 | Val Loss: 0.0353\n",
      "[Fold 3 Epoch 450] Train Loss: 0.0389 | Val Loss: 0.0334\n",
      "[Fold 3 Epoch 460] Train Loss: 0.0445 | Val Loss: 0.0344\n",
      "[Fold 3 Epoch 470] Train Loss: 0.0440 | Val Loss: 0.0423\n",
      "[Fold 3 Epoch 480] Train Loss: 0.0518 | Val Loss: 0.0348\n",
      "[Fold 3 Epoch 490] Train Loss: 0.0410 | Val Loss: 0.0453\n",
      "[Fold 3 Epoch 500] Train Loss: 0.0402 | Val Loss: 0.0329\n",
      "Evaluating fold 3 ...\n",
      "R²: 0.5290\n",
      "RMSE: 0.1813\n",
      "\n",
      "--- Fold 4 ---\n",
      "[Fold 4 Epoch 10] Train Loss: 0.0883 | Val Loss: 0.0893\n",
      "[Fold 4 Epoch 20] Train Loss: 0.0711 | Val Loss: 0.0844\n",
      "[Fold 4 Epoch 30] Train Loss: 0.0709 | Val Loss: 0.0814\n",
      "[Fold 4 Epoch 40] Train Loss: 0.0702 | Val Loss: 0.0780\n",
      "[Fold 4 Epoch 50] Train Loss: 0.0643 | Val Loss: 0.0789\n",
      "[Fold 4 Epoch 60] Train Loss: 0.0628 | Val Loss: 0.0762\n",
      "[Fold 4 Epoch 70] Train Loss: 0.0564 | Val Loss: 0.0763\n",
      "[Fold 4 Epoch 80] Train Loss: 0.0603 | Val Loss: 0.0782\n",
      "[Fold 4 Epoch 90] Train Loss: 0.0586 | Val Loss: 0.0785\n",
      "[Fold 4 Epoch 100] Train Loss: 0.0538 | Val Loss: 0.0704\n",
      "[Fold 4 Epoch 110] Train Loss: 0.0522 | Val Loss: 0.0683\n",
      "[Fold 4 Epoch 120] Train Loss: 0.0521 | Val Loss: 0.0670\n",
      "[Fold 4 Epoch 130] Train Loss: 0.0518 | Val Loss: 0.0671\n",
      "[Fold 4 Epoch 140] Train Loss: 0.0517 | Val Loss: 0.0666\n",
      "[Fold 4 Epoch 150] Train Loss: 0.0503 | Val Loss: 0.0667\n",
      "[Fold 4 Epoch 160] Train Loss: 0.0502 | Val Loss: 0.0662\n",
      "[Fold 4 Epoch 170] Train Loss: 0.0579 | Val Loss: 0.0645\n",
      "[Fold 4 Epoch 180] Train Loss: 0.0567 | Val Loss: 0.0707\n",
      "[Fold 4 Epoch 190] Train Loss: 0.0467 | Val Loss: 0.0672\n",
      "[Fold 4 Epoch 200] Train Loss: 0.0493 | Val Loss: 0.0691\n",
      "[Fold 4 Epoch 210] Train Loss: 0.0450 | Val Loss: 0.0676\n",
      "[Fold 4 Epoch 220] Train Loss: 0.0469 | Val Loss: 0.0637\n",
      "[Fold 4 Epoch 230] Train Loss: 0.0494 | Val Loss: 0.0627\n",
      "[Fold 4 Epoch 240] Train Loss: 0.0460 | Val Loss: 0.0596\n",
      "[Fold 4 Epoch 250] Train Loss: 0.0505 | Val Loss: 0.0635\n",
      "[Fold 4 Epoch 260] Train Loss: 0.0461 | Val Loss: 0.0597\n",
      "[Fold 4 Epoch 270] Train Loss: 0.0468 | Val Loss: 0.0570\n",
      "[Fold 4 Epoch 280] Train Loss: 0.0380 | Val Loss: 0.0569\n",
      "[Fold 4 Epoch 290] Train Loss: 0.0374 | Val Loss: 0.0474\n",
      "[Fold 4 Epoch 300] Train Loss: 0.0392 | Val Loss: 0.0456\n",
      "[Fold 4 Epoch 310] Train Loss: 0.0418 | Val Loss: 0.0439\n",
      "[Fold 4 Epoch 320] Train Loss: 0.0367 | Val Loss: 0.0450\n",
      "[Fold 4 Epoch 330] Train Loss: 0.0388 | Val Loss: 0.0509\n",
      "[Fold 4 Epoch 340] Train Loss: 0.0373 | Val Loss: 0.0407\n",
      "[Fold 4 Epoch 350] Train Loss: 0.0417 | Val Loss: 0.0461\n",
      "[Fold 4 Epoch 360] Train Loss: 0.0371 | Val Loss: 0.0464\n",
      "[Fold 4 Epoch 370] Train Loss: 0.0366 | Val Loss: 0.0450\n",
      "[Fold 4 Epoch 380] Train Loss: 0.0361 | Val Loss: 0.0397\n",
      "[Fold 4 Epoch 390] Train Loss: 0.0382 | Val Loss: 0.0528\n",
      "[Fold 4 Epoch 400] Train Loss: 0.0392 | Val Loss: 0.0569\n",
      "[Fold 4 Epoch 410] Train Loss: 0.0369 | Val Loss: 0.0405\n",
      "[Fold 4 Epoch 420] Train Loss: 0.0371 | Val Loss: 0.0583\n",
      "[Fold 4 Epoch 430] Train Loss: 0.0376 | Val Loss: 0.0488\n",
      "[Fold 4 Epoch 440] Train Loss: 0.0324 | Val Loss: 0.0418\n",
      "[Fold 4 Epoch 450] Train Loss: 0.0333 | Val Loss: 0.0440\n",
      "[Fold 4 Epoch 460] Train Loss: 0.0332 | Val Loss: 0.0473\n",
      "[Fold 4 Epoch 470] Train Loss: 0.0401 | Val Loss: 0.0512\n",
      "[Fold 4 Epoch 480] Train Loss: 0.0341 | Val Loss: 0.0452\n",
      "[Fold 4 Epoch 490] Train Loss: 0.0369 | Val Loss: 0.0388\n",
      "[Fold 4 Epoch 500] Train Loss: 0.0385 | Val Loss: 0.0390\n",
      "Evaluating fold 4 ...\n",
      "R²: 0.5651\n",
      "RMSE: 0.1976\n",
      "\n",
      "--- Fold 5 ---\n",
      "[Fold 5 Epoch 10] Train Loss: 0.1763 | Val Loss: 0.0728\n",
      "[Fold 5 Epoch 20] Train Loss: 0.1191 | Val Loss: 0.0583\n",
      "[Fold 5 Epoch 30] Train Loss: 0.0874 | Val Loss: 0.0586\n",
      "[Fold 5 Epoch 40] Train Loss: 0.0891 | Val Loss: 0.0580\n",
      "[Fold 5 Epoch 50] Train Loss: 0.0844 | Val Loss: 0.0587\n",
      "[Fold 5 Epoch 60] Train Loss: 0.0775 | Val Loss: 0.0611\n",
      "[Fold 5 Epoch 70] Train Loss: 0.0719 | Val Loss: 0.0604\n",
      "[Fold 5 Epoch 80] Train Loss: 0.0666 | Val Loss: 0.0575\n",
      "[Fold 5 Epoch 90] Train Loss: 0.0687 | Val Loss: 0.0595\n",
      "[Fold 5 Epoch 100] Train Loss: 0.0657 | Val Loss: 0.0574\n",
      "[Fold 5 Epoch 110] Train Loss: 0.0638 | Val Loss: 0.0555\n",
      "[Fold 5 Epoch 120] Train Loss: 0.0717 | Val Loss: 0.0556\n",
      "[Fold 5 Epoch 130] Train Loss: 0.0691 | Val Loss: 0.0557\n",
      "[Fold 5 Epoch 140] Train Loss: 0.0579 | Val Loss: 0.0562\n",
      "[Fold 5 Epoch 150] Train Loss: 0.0701 | Val Loss: 0.0552\n",
      "[Fold 5 Epoch 160] Train Loss: 0.0605 | Val Loss: 0.0517\n",
      "[Fold 5 Epoch 170] Train Loss: 0.0556 | Val Loss: 0.0546\n",
      "[Fold 5 Epoch 180] Train Loss: 0.0620 | Val Loss: 0.0507\n",
      "[Fold 5 Epoch 190] Train Loss: 0.0608 | Val Loss: 0.0510\n",
      "[Fold 5 Epoch 200] Train Loss: 0.0624 | Val Loss: 0.0472\n",
      "[Fold 5 Epoch 210] Train Loss: 0.0667 | Val Loss: 0.0492\n",
      "[Fold 5 Epoch 220] Train Loss: 0.0484 | Val Loss: 0.0412\n",
      "[Fold 5 Epoch 230] Train Loss: 0.0507 | Val Loss: 0.0387\n",
      "[Fold 5 Epoch 240] Train Loss: 0.0493 | Val Loss: 0.0362\n",
      "[Fold 5 Epoch 250] Train Loss: 0.0458 | Val Loss: 0.0332\n",
      "[Fold 5 Epoch 260] Train Loss: 0.0480 | Val Loss: 0.0331\n",
      "[Fold 5 Epoch 270] Train Loss: 0.0539 | Val Loss: 0.0318\n",
      "[Fold 5 Epoch 280] Train Loss: 0.0411 | Val Loss: 0.0318\n",
      "[Fold 5 Epoch 290] Train Loss: 0.0446 | Val Loss: 0.0325\n",
      "[Fold 5 Epoch 300] Train Loss: 0.0417 | Val Loss: 0.0283\n",
      "[Fold 5 Epoch 310] Train Loss: 0.0399 | Val Loss: 0.0306\n",
      "[Fold 5 Epoch 320] Train Loss: 0.0404 | Val Loss: 0.0351\n",
      "[Fold 5 Epoch 330] Train Loss: 0.0439 | Val Loss: 0.0429\n",
      "[Fold 5 Epoch 340] Train Loss: 0.0535 | Val Loss: 0.0421\n",
      "[Fold 5 Epoch 350] Train Loss: 0.0450 | Val Loss: 0.0394\n",
      "[Fold 5 Epoch 360] Train Loss: 0.0524 | Val Loss: 0.0481\n",
      "[Fold 5 Epoch 370] Train Loss: 0.0536 | Val Loss: 0.0378\n",
      "[Fold 5 Epoch 380] Train Loss: 0.0451 | Val Loss: 0.0305\n",
      "[Fold 5 Epoch 390] Train Loss: 0.0420 | Val Loss: 0.0297\n",
      "[Fold 5 Epoch 400] Train Loss: 0.0426 | Val Loss: 0.0269\n",
      "[Fold 5 Epoch 410] Train Loss: 0.0378 | Val Loss: 0.0272\n",
      "[Fold 5 Epoch 420] Train Loss: 0.0447 | Val Loss: 0.0380\n",
      "[Fold 5 Epoch 430] Train Loss: 0.0423 | Val Loss: 0.0321\n",
      "[Fold 5 Epoch 440] Train Loss: 0.0471 | Val Loss: 0.0298\n",
      "[Fold 5 Epoch 450] Train Loss: 0.0421 | Val Loss: 0.0285\n",
      "[Fold 5 Epoch 460] Train Loss: 0.0374 | Val Loss: 0.0285\n",
      "[Fold 5 Epoch 470] Train Loss: 0.0391 | Val Loss: 0.0291\n",
      "[Fold 5 Epoch 480] Train Loss: 0.0461 | Val Loss: 0.0362\n",
      "[Fold 5 Epoch 490] Train Loss: 0.0452 | Val Loss: 0.0378\n",
      "[Fold 5 Epoch 500] Train Loss: 0.0379 | Val Loss: 0.0364\n",
      "Evaluating fold 5 ...\n",
      "R²: 0.4746\n",
      "RMSE: 0.1908\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "Fold 1: R² = 0.4298, RMSE = 0.1790\n",
      "Fold 2: R² = 0.1952, RMSE = 0.2899\n",
      "Fold 3: R² = 0.5290, RMSE = 0.1813\n",
      "Fold 4: R² = 0.5651, RMSE = 0.1976\n",
      "Fold 5: R² = 0.4746, RMSE = 0.1908\n",
      "\n",
      "Average R²: 0.4388 ± 0.1302\n",
      "Average RMSE: 0.2077 ± 0.0416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 09:39:16,992] A new study created in RDB with name: GNN-mixed model01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e30c48c87cc407ca5a84915e3f17dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-07 09:52:28,448] Trial 0 finished with value: 0.3789858102798462 and parameters: {'learning_rate': 0.00024832838082315255, 'dropout': 0.5, 'num_layers': 5}. Best is trial 0 with value: 0.3789858102798462.\n",
      "[I 2025-03-07 10:06:22,990] Trial 1 finished with value: -0.039005601406097413 and parameters: {'learning_rate': 0.00019602220882739989, 'dropout': 0.1, 'num_layers': 6}. Best is trial 1 with value: -0.039005601406097413.\n",
      "[I 2025-03-07 10:16:29,003] Trial 2 finished with value: 0.5399153232574463 and parameters: {'learning_rate': 0.00016255138420160465, 'dropout': 0.5, 'num_layers': 2}. Best is trial 1 with value: -0.039005601406097413.\n",
      "[I 2025-03-07 10:28:41,602] Trial 3 finished with value: -0.06721740961074829 and parameters: {'learning_rate': 0.0004243064104699774, 'dropout': 0.1, 'num_layers': 3}. Best is trial 3 with value: -0.06721740961074829.\n",
      "[I 2025-03-07 10:41:29,270] Trial 4 finished with value: 0.03246970176696777 and parameters: {'learning_rate': 0.00037713955508620716, 'dropout': 0.5, 'num_layers': 4}. Best is trial 3 with value: -0.06721740961074829.\n",
      "[I 2025-03-07 10:51:38,496] Trial 5 finished with value: 0.3751766324043274 and parameters: {'learning_rate': 0.0003711962751226017, 'dropout': 0.1, 'num_layers': 2}. Best is trial 3 with value: -0.06721740961074829.\n",
      "[I 2025-03-07 11:01:50,809] Trial 6 finished with value: -0.05708576440811157 and parameters: {'learning_rate': 0.00028728554417165427, 'dropout': 0.1, 'num_layers': 2}. Best is trial 3 with value: -0.06721740961074829.\n",
      "[I 2025-03-07 11:04:04,142] Trial 7 pruned. \n",
      "[I 2025-03-07 11:04:17,008] Trial 8 pruned. \n",
      "[I 2025-03-07 11:05:16,076] Trial 9 pruned. \n",
      "[I 2025-03-07 11:18:53,537] Trial 10 finished with value: -0.0038034796714782717 and parameters: {'learning_rate': 0.0004498916389848395, 'dropout': 0.1, 'num_layers': 4}. Best is trial 3 with value: -0.06721740961074829.\n",
      "[I 2025-03-07 11:19:32,203] Trial 11 pruned. \n",
      "[I 2025-03-07 11:19:57,834] Trial 12 pruned. \n",
      "[I 2025-03-07 11:20:36,088] Trial 13 pruned. \n",
      "[I 2025-03-07 11:22:13,008] Trial 14 pruned. \n",
      "[I 2025-03-07 11:34:23,894] Trial 15 finished with value: -0.1190952181816101 and parameters: {'learning_rate': 0.0004118262376217361, 'dropout': 0.1, 'num_layers': 3}. Best is trial 15 with value: -0.1190952181816101.\n",
      "[I 2025-03-07 11:48:39,635] Trial 16 finished with value: -0.06768428087234497 and parameters: {'learning_rate': 0.000415653264166478, 'dropout': 0.1, 'num_layers': 5}. Best is trial 15 with value: -0.1190952181816101.\n",
      "[I 2025-03-07 11:48:54,364] Trial 17 pruned. \n",
      "[I 2025-03-07 11:49:09,179] Trial 18 pruned. \n",
      "[I 2025-03-07 12:04:09,014] Trial 19 finished with value: -0.1087916374206543 and parameters: {'learning_rate': 0.0004902815167382746, 'dropout': 0.1, 'num_layers': 6}. Best is trial 15 with value: -0.1190952181816101.\n",
      "[I 2025-03-07 12:04:24,662] Trial 20 pruned. \n",
      "[I 2025-03-07 12:04:40,456] Trial 21 pruned. \n",
      "[I 2025-03-07 12:05:11,819] Trial 22 pruned. \n",
      "[I 2025-03-07 12:05:27,535] Trial 23 pruned. \n",
      "[I 2025-03-07 12:05:42,361] Trial 24 pruned. \n",
      "[I 2025-03-07 12:05:58,083] Trial 25 pruned. \n",
      "[I 2025-03-07 12:06:12,845] Trial 26 pruned. \n",
      "[I 2025-03-07 12:06:40,645] Trial 27 pruned. \n",
      "[I 2025-03-07 12:06:54,442] Trial 28 pruned. \n",
      "[I 2025-03-07 12:07:09,184] Trial 29 pruned. \n",
      "[I 2025-03-07 12:07:56,306] Trial 30 pruned. \n",
      "[I 2025-03-07 12:08:09,241] Trial 31 pruned. \n",
      "[I 2025-03-07 12:20:37,814] Trial 32 finished with value: -0.0709218978881836 and parameters: {'learning_rate': 0.0004460372305059749, 'dropout': 0.1, 'num_layers': 3}. Best is trial 15 with value: -0.1190952181816101.\n",
      "[I 2025-03-07 12:20:50,806] Trial 33 pruned. \n",
      "[I 2025-03-07 12:21:04,695] Trial 34 pruned. \n",
      "[I 2025-03-07 12:21:18,515] Trial 35 pruned. \n",
      "[I 2025-03-07 12:21:33,357] Trial 36 pruned. \n",
      "[I 2025-03-07 12:21:46,236] Trial 37 pruned. \n",
      "[I 2025-03-07 12:22:01,952] Trial 38 pruned. \n",
      "[I 2025-03-07 12:22:13,738] Trial 39 pruned. \n",
      "[I 2025-03-07 12:22:28,621] Trial 40 pruned. \n",
      "[I 2025-03-07 12:22:54,434] Trial 41 pruned. \n",
      "[I 2025-03-07 12:23:07,408] Trial 42 pruned. \n",
      "[I 2025-03-07 12:23:19,369] Trial 43 pruned. \n",
      "[I 2025-03-07 12:23:33,313] Trial 44 pruned. \n",
      "[I 2025-03-07 12:23:46,191] Trial 45 pruned. \n",
      "[I 2025-03-07 12:23:57,951] Trial 46 pruned. \n",
      "[I 2025-03-07 12:24:23,597] Trial 47 pruned. \n",
      "[I 2025-03-07 12:24:36,516] Trial 48 pruned. \n",
      "[I 2025-03-07 12:24:50,392] Trial 49 pruned. \n",
      "[I 2025-03-07 12:25:02,303] Trial 50 pruned. \n",
      "[I 2025-03-07 12:25:14,112] Trial 51 pruned. \n",
      "[I 2025-03-07 12:25:25,889] Trial 52 pruned. \n",
      "[I 2025-03-07 12:37:54,781] Trial 53 finished with value: -0.09181479215621949 and parameters: {'learning_rate': 0.00029896100707594724, 'dropout': 0.1, 'num_layers': 3}. Best is trial 15 with value: -0.1190952181816101.\n",
      "[I 2025-03-07 12:38:07,757] Trial 54 pruned. \n",
      "[I 2025-03-07 12:38:20,755] Trial 55 pruned. \n",
      "[I 2025-03-07 12:38:33,727] Trial 56 pruned. \n",
      "[I 2025-03-07 12:38:47,596] Trial 57 pruned. \n",
      "[I 2025-03-07 12:39:03,261] Trial 58 pruned. \n",
      "[I 2025-03-07 12:39:16,204] Trial 59 pruned. \n",
      "[I 2025-03-07 12:39:30,910] Trial 60 pruned. \n",
      "[I 2025-03-07 12:39:42,695] Trial 61 pruned. \n",
      "[I 2025-03-07 12:39:55,555] Trial 62 pruned. \n",
      "[I 2025-03-07 12:40:09,313] Trial 63 pruned. \n",
      "[I 2025-03-07 12:40:21,060] Trial 64 pruned. \n",
      "[I 2025-03-07 12:40:59,726] Trial 65 pruned. \n",
      "[I 2025-03-07 12:41:13,718] Trial 66 pruned. \n",
      "[I 2025-03-07 12:41:29,672] Trial 67 pruned. \n",
      "[I 2025-03-07 12:41:42,666] Trial 68 pruned. \n",
      "[I 2025-03-07 12:41:57,443] Trial 69 pruned. \n",
      "[I 2025-03-07 12:42:09,305] Trial 70 pruned. \n",
      "[I 2025-03-07 12:42:25,157] Trial 71 pruned. \n",
      "[I 2025-03-07 12:42:40,858] Trial 72 pruned. \n",
      "[I 2025-03-07 12:42:56,659] Trial 73 pruned. \n",
      "[I 2025-03-07 12:43:12,539] Trial 74 pruned. \n",
      "[I 2025-03-07 12:43:59,389] Trial 75 pruned. \n",
      "[I 2025-03-07 12:44:12,255] Trial 76 pruned. \n",
      "[I 2025-03-07 12:44:26,933] Trial 77 pruned. \n",
      "[I 2025-03-07 12:45:08,589] Trial 78 pruned. \n",
      "[I 2025-03-07 12:46:17,218] Trial 79 pruned. \n",
      "[I 2025-03-07 12:46:32,051] Trial 80 pruned. \n",
      "[I 2025-03-07 12:46:45,938] Trial 81 pruned. \n",
      "[I 2025-03-07 12:46:59,729] Trial 82 pruned. \n",
      "[I 2025-03-07 12:47:12,428] Trial 83 pruned. \n",
      "[I 2025-03-07 12:47:28,077] Trial 84 pruned. \n",
      "[I 2025-03-07 12:47:40,843] Trial 85 pruned. \n",
      "[I 2025-03-07 12:47:55,510] Trial 86 pruned. \n",
      "[I 2025-03-07 12:48:07,235] Trial 87 pruned. \n",
      "[I 2025-03-07 12:48:20,051] Trial 88 pruned. \n",
      "[I 2025-03-07 12:48:34,845] Trial 89 pruned. \n",
      "[I 2025-03-07 12:49:06,392] Trial 90 pruned. \n",
      "[I 2025-03-07 12:49:20,278] Trial 91 pruned. \n",
      "[I 2025-03-07 12:49:34,085] Trial 92 pruned. \n",
      "[I 2025-03-07 12:49:47,995] Trial 93 pruned. \n",
      "[I 2025-03-07 12:50:02,068] Trial 94 pruned. \n",
      "[I 2025-03-07 12:50:14,968] Trial 95 pruned. \n",
      "[I 2025-03-07 12:50:27,884] Trial 96 pruned. \n",
      "[I 2025-03-07 12:50:41,802] Trial 97 pruned. \n",
      "[I 2025-03-07 12:50:53,602] Trial 98 pruned. \n",
      "[I 2025-03-07 12:51:08,428] Trial 99 pruned. \n",
      "\n",
      "================= Optuna Study Results =================\n",
      "Best Trial Value (Negative R²): -0.1190952181816101\n",
      "Best Hyperparameters:\n",
      "  learning_rate: 0.0004118262376217361\n",
      "  dropout: 0.1\n",
      "  num_layers: 3\n",
      "User Attrs (e.g., RMSE): {'avg_rmse': 0.26054930787141806}\n",
      "Could not generate optimization history plot: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\n",
      "Could not generate hyperparameter importance plot: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\n",
      "Could not generate intermediate values plot: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\n",
      "\n",
      "================= End of Optuna Tuning =================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch_geometric.data import Data, DataLoader as PyGDataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool, BatchNorm\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "#                             DATASET & PREP\n",
    "# =============================================================================\n",
    "\n",
    "class MolDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset that:\n",
    "      - Reads external factors from CSV\n",
    "      - Loads the corresponding pickle for the molecule's graph\n",
    "      - Converts it into a PyG Data object\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 raw_dataframe: pd.DataFrame,\n",
    "                 nx_graph_dict: dict,\n",
    "                 *,\n",
    "                 component_col: str,\n",
    "                 global_state_cols: list[str],\n",
    "                 label_col: str,\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_dataframe: The input dataframe containing molecule info.\n",
    "            nx_graph_dict: Dictionary mapping component names to networkx graphs.\n",
    "            component_col: Column name for the component.\n",
    "            global_state_cols: List of columns representing external factors.\n",
    "            label_col: Column name for the regression target.\n",
    "            transform: Any transform to apply to each PyG Data object.\n",
    "        \"\"\"\n",
    "        self.raw_dataframe = raw_dataframe\n",
    "        self.nx_graph_dict = nx_graph_dict\n",
    "        self.component_col = [component_col] if type(component_col) is str else component_col\n",
    "        self.global_state_cols = global_state_cols\n",
    "        self.label_col = [label_col] if type(label_col) is str else label_col\n",
    "        self.transform = transform\n",
    "        \n",
    "        required_cols = set(self.global_state_cols + self.label_col + self.component_col)\n",
    "        for col in required_cols:\n",
    "            if col not in self.raw_dataframe.columns:\n",
    "                raise ValueError(f\"Missing column in DataFrame: '{col}'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.raw_dataframe.iloc[idx]\n",
    "        \n",
    "        # 1. Load the molecule graph\n",
    "        component_name = row[self.component_col[0]]  # e.g. \"C23\"\n",
    "        pyg_data = self.nx_graph_dict[component_name]\n",
    "\n",
    "        # 2. Prepare the external factors\n",
    "        externals = torch.tensor(row[self.global_state_cols].values.astype(float), dtype=torch.float)\n",
    "        externals = externals.unsqueeze(0)\n",
    "\n",
    "        # 3. Prepare the label (regression target)\n",
    "        label = torch.tensor([row[self.label_col][0]], dtype=torch.float)\n",
    "\n",
    "        # 4. Attach externals & label to the Data object\n",
    "        pyg_data.externals = externals  # shape [1, external_in_dim]\n",
    "        pyg_data.y = label  # shape [1]\n",
    "\n",
    "        if self.transform:\n",
    "            pyg_data = self.transform(pyg_data)\n",
    "\n",
    "        return pyg_data\n",
    "\n",
    "\n",
    "def networkx_to_pyg(nx_graph):\n",
    "    \"\"\"\n",
    "    Convert a networkx graph to a torch_geometric.data.Data object.\n",
    "    This is a basic template; adjust for your actual node/edge features.\n",
    "    \"\"\"\n",
    "    # Sort nodes to ensure consistent ordering\n",
    "    node_mapping = {node: i for i, node in enumerate(nx_graph.nodes())}\n",
    "\n",
    "    x_list = []\n",
    "    edge_index_list = []\n",
    "    edge_attr_list = []\n",
    "\n",
    "    # Node features\n",
    "    for node in nx_graph.nodes(data=True):\n",
    "        original_id = node[0]\n",
    "        attrs = node[1]\n",
    "        symbol = attrs.get(\"symbol\", \"C\")\n",
    "        symbol_id = 0 if symbol == \"C\" else 1 if symbol == \"H\" else 2\n",
    "        x_list.append([symbol_id])\n",
    "\n",
    "    # Edge features\n",
    "    for u, v, edge_attrs in nx_graph.edges(data=True):\n",
    "        u_idx = node_mapping[u]\n",
    "        v_idx = node_mapping[v]\n",
    "        edge_index_list.append((u_idx, v_idx))\n",
    "        bde_pred = edge_attrs.get(\"bde_pred\", 0.0) or 0.0\n",
    "        bdfe_pred = edge_attrs.get(\"bdfe_pred\", 0.0) or 0.0\n",
    "        edge_attr_list.append([bde_pred, bdfe_pred])\n",
    "\n",
    "    x = torch.tensor(x_list, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                     BASE GNN MODEL (WITH DIM MATCH)\n",
    "# =============================================================================\n",
    "\n",
    "class GINE_Regression(nn.Module):\n",
    "    \"\"\"\n",
    "    A GNN for regression using GINEConv layers + edge attributes,\n",
    "    where all layers have the same hidden_dim (no dimension mismatch).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dim: int = 128,\n",
    "                 num_layers: int = 3,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encode edges from edge_in_dim to hidden_dim\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(edge_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Encode nodes from node_in_dim to hidden_dim\n",
    "        self.node_encoder = nn.Linear(node_in_dim, hidden_dim)\n",
    "        \n",
    "        # Multiple GINEConv layers & corresponding BatchNorm\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Process external factors\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Encode\n",
    "        x = self.node_encoder(x)\n",
    "        edge_emb = self.edge_encoder(edge_attr)\n",
    "        \n",
    "        # Pass through GINEConv layers\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Global pooling\n",
    "        graph_emb = global_mean_pool(x, batch)\n",
    "\n",
    "        # Process external factors\n",
    "        ext_emb = self.externals_mlp(data.externals)\n",
    "\n",
    "        # Combine & regress\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)\n",
    "        out = self.final_regressor(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                   TRAIN/VALID/EVALUATION UTILS\n",
    "# =============================================================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_data)\n",
    "        y = batch_data.y.to(device).view(-1)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "        count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            preds = model(batch_data)\n",
    "            y = batch_data.y.to(device).view(-1)\n",
    "            loss = criterion(preds, y)\n",
    "            total_loss += loss.item() * batch_data.num_graphs\n",
    "            count += batch_data.num_graphs\n",
    "    return total_loss / count if count > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset loader and compute R² and RMSE.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            preds = model(batch)\n",
    "            y_true.append(batch.y.cpu())\n",
    "            y_pred.append(preds.cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true).numpy().squeeze()\n",
    "    y_pred = torch.cat(y_pred).numpy().squeeze()\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return r2, rmse\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                             DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "env_file = r\"numberGNN molecules\\graph_pickles\\dataset02.xlsx\"\n",
    "data = pd.read_excel(env_file, engine='openpyxl').dropna(subset=['degradation_rate'])\n",
    "data['seawater'] = data['seawater'].map({'art': 1, 'sea': 0})\n",
    "\n",
    "folder_path = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\"\n",
    "graph_pickles = [f for f in os.listdir(folder_path) if f.endswith(\".pkl\")]\n",
    "\n",
    "base_dir = r\"F:\\2025 energing\\PYTHON\\GNN_chemicalENV-main\\GNN molecules\\graph_pickles\\molecules\"\n",
    "if os.path.exists(base_dir):\n",
    "    print(\"Directory exists:\", base_dir)\n",
    "    print(\"Files in directory:\", os.listdir(base_dir))\n",
    "else:\n",
    "    print(f\"Error: Directory {base_dir} does not exist!\")\n",
    "\n",
    "compounds = data.component.unique()\n",
    "graphs_dict = {}\n",
    "for compound, graph_pickle in zip(compounds, graph_pickles):\n",
    "    with open(os.path.join(base_dir, graph_pickle), 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "        graphs_dict[compound] = networkx_to_pyg(graph)\n",
    "\n",
    "dataset = MolDataset(\n",
    "    raw_dataframe=data,\n",
    "    nx_graph_dict=graphs_dict,\n",
    "    component_col=\"component\",\n",
    "    global_state_cols=[\"temperature\", \"concentration\", \"time\", \"seawater\"],\n",
    "    label_col=\"degradation_rate\",\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "#                      CROSS-VALIDATION (FIXED MODEL)\n",
    "# =============================================================================\n",
    "from torch_geometric.data import DataLoader as PyGDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 5  # number of folds\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"\\n--- Fold {fold + 1} ---\")\n",
    "\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "    val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = GINE_Regression(\n",
    "        node_in_dim=1,\n",
    "        edge_in_dim=2,\n",
    "        external_in_dim=4,\n",
    "        hidden_dim=16,  # Example hidden_dim\n",
    "        num_layers=5,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    num_epochs = 500\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[Fold {fold + 1} Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    print(f\"Evaluating fold {fold + 1} ...\")\n",
    "    r2, rmse = evaluate_model(model, val_loader, device)\n",
    "    fold_results.append({\"fold\": fold + 1, \"r2\": r2, \"rmse\": rmse})\n",
    "\n",
    "r2_scores = [res[\"r2\"] for res in fold_results]\n",
    "rmse_scores = [res[\"rmse\"] for res in fold_results]\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "for res in fold_results:\n",
    "    print(f\"Fold {res['fold']}: R² = {res['r2']:.4f}, RMSE = {res['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#             IMPROVED MODEL WITH TRAPEZOID DIMENSIONS & PROJECTIONS\n",
    "# =============================================================================\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "\n",
    "def build_trapezoid_dims(num_layers):\n",
    "    \"\"\"\n",
    "    Build a list of hidden dimensions in a trapezoid manner:\n",
    "    up to the first 4 layers: [128, 64, 32, 16]\n",
    "    if num_layers > 4, extend with 16 for extra layers.\n",
    "    \"\"\"\n",
    "    base_dims = [128, 64, 32, 16]\n",
    "    if num_layers > 4:\n",
    "        extra_layers = num_layers - 4\n",
    "        return base_dims + [16] * extra_layers\n",
    "    else:\n",
    "        return base_dims[:num_layers]\n",
    "\n",
    "\n",
    "class GINE_RegressionTrapezoid(nn.Module):\n",
    "    \"\"\"\n",
    "    A GINEConv-based regression model that uses a list of hidden dimensions\n",
    "    to build layers with decreasing size (trapezoid architecture),\n",
    "    ensuring dimension consistency with projection layers between convs.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 node_in_dim: int,\n",
    "                 edge_in_dim: int,\n",
    "                 external_in_dim: int,\n",
    "                 hidden_dims: list,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # For the first layer, encode edges to hidden_dims[0], and encode nodes as well\n",
    "        self.initial_edge_encoder = nn.Linear(edge_in_dim, hidden_dims[0])\n",
    "        self.initial_node_encoder = nn.Linear(node_in_dim, hidden_dims[0])\n",
    "\n",
    "        # We'll build each GINEConv to transform dimension: hidden_dims[i] -> hidden_dims[i].\n",
    "        # After each conv i, if i < len(hidden_dims)-1, we project to hidden_dims[i+1].\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.projections = nn.ModuleList()  # for node features\n",
    "        self.edge_projections = nn.ModuleList()  # for edge features\n",
    "\n",
    "        for i in range(len(hidden_dims)):\n",
    "            # GINEConv's internal MLP\n",
    "            net = nn.Sequential(\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i])\n",
    "            )\n",
    "            conv = GINEConv(nn=net)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(BatchNorm(hidden_dims[i]))\n",
    "\n",
    "            # If there's a next layer, we need a projection from hidden_dims[i] -> hidden_dims[i+1]\n",
    "            if i < len(hidden_dims) - 1:\n",
    "                self.projections.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "                self.edge_projections.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            else:\n",
    "                # No projection needed for the last layer\n",
    "                self.projections.append(None)\n",
    "                self.edge_projections.append(None)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "\n",
    "        # We'll process external factors to the final dimension\n",
    "        final_dim = hidden_dims[-1]\n",
    "        self.externals_mlp = nn.Sequential(\n",
    "            nn.Linear(external_in_dim, final_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(final_dim, final_dim)\n",
    "        )\n",
    "\n",
    "        # Final regression: combine final node features + final external features\n",
    "        self.final_regressor = nn.Sequential(\n",
    "            nn.Linear(final_dim + final_dim, final_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(final_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # 1) Encode node/edge to hidden_dims[0]\n",
    "        x = self.initial_node_encoder(x)\n",
    "        edge_emb = self.initial_edge_encoder(edge_attr)\n",
    "\n",
    "        # 2) Pass through each GINEConv\n",
    "        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):\n",
    "            # GINEConv forward\n",
    "            x = conv(x, edge_index, edge_emb)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout_layer(x)\n",
    "\n",
    "            # If there's a next layer, project node features and edge_emb to hidden_dims[i+1]\n",
    "            if i < len(self.projections) - 1 and self.projections[i] is not None:\n",
    "                x = self.projections[i](x)\n",
    "                edge_emb = self.edge_projections[i](edge_emb)\n",
    "\n",
    "        # 3) Global mean pooling\n",
    "        graph_emb = global_mean_pool(x, batch)\n",
    "\n",
    "        # 4) Process external factors into final dimension\n",
    "        ext_emb = self.externals_mlp(data.externals)\n",
    "\n",
    "        # 5) Concatenate and final regression\n",
    "        combined = torch.cat([graph_emb, ext_emb], dim=-1)\n",
    "        out = self.final_regressor(combined).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                          OPTUNA OBJECTIVE FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna.\n",
    "    Performs k-fold cross validation using GINE_RegressionTrapezoid,\n",
    "    returning negative average R². We log RMSE as a user attribute.\n",
    "    \"\"\"\n",
    "    # Hyperparameter search space\n",
    "    #lr = trial.suggest_categorical(\"learning_rate\", [1e-3, 3e-3, 1e-4, 3e-4])\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-4, 5e-4, log=True)\n",
    "\n",
    "    dropout = trial.suggest_categorical(\"dropout\", [0.1, 0.5])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "\n",
    "    hidden_dims = build_trapezoid_dims(num_layers)\n",
    "    num_epochs = 100  # Could also be a hyperparameter\n",
    "\n",
    "    # Local 5-fold for CV\n",
    "    kf_local = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    r2_scores = []\n",
    "    rmse_scores = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf_local.split(dataset)):\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "        val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "        model = GINE_RegressionTrapezoid(\n",
    "            node_in_dim=1,\n",
    "            edge_in_dim=2,\n",
    "            external_in_dim=4,\n",
    "            hidden_dims=hidden_dims,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "            val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "            # Report intermediate values (optional)\n",
    "            if epoch % 10 == 0:\n",
    "                trial.report(val_loss, step=epoch)\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "\n",
    "        # Evaluate R² and RMSE for this fold\n",
    "        model.eval()\n",
    "        y_true_fold, y_pred_fold = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch_data in val_loader:\n",
    "                batch_data = batch_data.to(device)\n",
    "                preds = model(batch_data)\n",
    "                y_true_fold.append(batch_data.y.cpu())\n",
    "                y_pred_fold.append(preds.cpu())\n",
    "\n",
    "        y_true_fold = torch.cat(y_true_fold).numpy().squeeze()\n",
    "        y_pred_fold = torch.cat(y_pred_fold).numpy().squeeze()\n",
    "\n",
    "        fold_r2 = r2_score(y_true_fold, y_pred_fold)\n",
    "        fold_rmse = np.sqrt(mean_squared_error(y_true_fold, y_pred_fold))\n",
    "\n",
    "        r2_scores.append(fold_r2)\n",
    "        rmse_scores.append(fold_rmse)\n",
    "\n",
    "    avg_r2 = np.mean(r2_scores)\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "    # Log RMSE as user attribute\n",
    "    trial.set_user_attr(\"avg_rmse\", float(avg_rmse))\n",
    "\n",
    "    # Return negative R² because Optuna minimizes the objective\n",
    "    return -avg_r2\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#                           OPTUNA STUDY & DASHBOARD\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(\n",
    "        storage=\"sqlite:///gnn_mix_op05.sqlite3\",\n",
    "        study_name=\"GNN-mixed model01\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "    print(\"\\n================= Optuna Study Results =================\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Best Trial Value (Negative R²): {best_trial.value}\")\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for key, val in best_trial.params.items():\n",
    "        print(f\"  {key}: {val}\")\n",
    "    print(f\"User Attrs (e.g., RMSE): {best_trial.user_attrs}\")\n",
    "\n",
    "    try:\n",
    "        fig1 = vis.plot_optimization_history(study)\n",
    "        fig1.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate optimization history plot: {e}\")\n",
    "\n",
    "    try:\n",
    "        fig2 = vis.plot_param_importances(study)\n",
    "        fig2.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate hyperparameter importance plot: {e}\")\n",
    "\n",
    "    try:\n",
    "        fig3 = vis.plot_intermediate_values(study)\n",
    "        fig3.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate intermediate values plot: {e}\")\n",
    "\n",
    "    print(\"\\n================= End of Optuna Tuning =================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1 ---\n",
      "[Fold 1 Epoch 10] Train Loss: 0.1210 | Val Loss: 0.0644\n",
      "[Fold 1 Epoch 20] Train Loss: 0.0977 | Val Loss: 0.0594\n",
      "[Fold 1 Epoch 30] Train Loss: 0.0972 | Val Loss: 0.0560\n",
      "[Fold 1 Epoch 40] Train Loss: 0.0762 | Val Loss: 0.0503\n",
      "[Fold 1 Epoch 50] Train Loss: 0.0821 | Val Loss: 0.0633\n",
      "[Fold 1 Epoch 60] Train Loss: 0.0713 | Val Loss: 0.0479\n",
      "[Fold 1 Epoch 70] Train Loss: 0.0706 | Val Loss: 0.0467\n",
      "[Fold 1 Epoch 80] Train Loss: 0.0635 | Val Loss: 0.0454\n",
      "[Fold 1 Epoch 90] Train Loss: 0.0611 | Val Loss: 0.0453\n",
      "[Fold 1 Epoch 100] Train Loss: 0.0603 | Val Loss: 0.0442\n",
      "[Fold 1 Epoch 110] Train Loss: 0.0595 | Val Loss: 0.0426\n",
      "[Fold 1 Epoch 120] Train Loss: 0.0574 | Val Loss: 0.0488\n",
      "[Fold 1 Epoch 130] Train Loss: 0.0613 | Val Loss: 0.0419\n",
      "[Fold 1 Epoch 140] Train Loss: 0.0658 | Val Loss: 0.0567\n",
      "[Fold 1 Epoch 150] Train Loss: 0.0585 | Val Loss: 0.0405\n",
      "[Fold 1 Epoch 160] Train Loss: 0.0558 | Val Loss: 0.0384\n",
      "[Fold 1 Epoch 170] Train Loss: 0.0468 | Val Loss: 0.0374\n",
      "[Fold 1 Epoch 180] Train Loss: 0.0512 | Val Loss: 0.0356\n",
      "[Fold 1 Epoch 190] Train Loss: 0.0474 | Val Loss: 0.0354\n",
      "[Fold 1 Epoch 200] Train Loss: 0.0444 | Val Loss: 0.0359\n",
      "[Fold 1 Epoch 210] Train Loss: 0.0394 | Val Loss: 0.0336\n",
      "[Fold 1 Epoch 220] Train Loss: 0.0434 | Val Loss: 0.0326\n",
      "[Fold 1 Epoch 230] Train Loss: 0.0401 | Val Loss: 0.0314\n",
      "[Fold 1 Epoch 240] Train Loss: 0.0383 | Val Loss: 0.0488\n",
      "[Fold 1 Epoch 250] Train Loss: 0.0397 | Val Loss: 0.0350\n",
      "[Fold 1 Epoch 260] Train Loss: 0.0430 | Val Loss: 0.0327\n",
      "[Fold 1 Epoch 270] Train Loss: 0.0459 | Val Loss: 0.0335\n",
      "[Fold 1 Epoch 280] Train Loss: 0.0491 | Val Loss: 0.0332\n",
      "[Fold 1 Epoch 290] Train Loss: 0.0391 | Val Loss: 0.0318\n",
      "[Fold 1 Epoch 300] Train Loss: 0.0494 | Val Loss: 0.0369\n",
      "[Fold 1 Epoch 310] Train Loss: 0.0403 | Val Loss: 0.0331\n",
      "[Fold 1 Epoch 320] Train Loss: 0.0364 | Val Loss: 0.0322\n",
      "[Fold 1 Epoch 330] Train Loss: 0.0393 | Val Loss: 0.0375\n",
      "[Fold 1 Epoch 340] Train Loss: 0.0407 | Val Loss: 0.0330\n",
      "[Fold 1 Epoch 350] Train Loss: 0.0349 | Val Loss: 0.0328\n",
      "[Fold 1 Epoch 360] Train Loss: 0.0377 | Val Loss: 0.0331\n",
      "[Fold 1 Epoch 370] Train Loss: 0.0391 | Val Loss: 0.0359\n",
      "[Fold 1 Epoch 380] Train Loss: 0.0383 | Val Loss: 0.0335\n",
      "[Fold 1 Epoch 390] Train Loss: 0.0405 | Val Loss: 0.0356\n",
      "[Fold 1 Epoch 400] Train Loss: 0.0415 | Val Loss: 0.0372\n",
      "[Fold 1 Epoch 410] Train Loss: 0.0354 | Val Loss: 0.0356\n",
      "[Fold 1 Epoch 420] Train Loss: 0.0399 | Val Loss: 0.0331\n",
      "[Fold 1 Epoch 430] Train Loss: 0.0331 | Val Loss: 0.0365\n",
      "[Fold 1 Epoch 440] Train Loss: 0.0362 | Val Loss: 0.0337\n",
      "[Fold 1 Epoch 450] Train Loss: 0.0394 | Val Loss: 0.0401\n",
      "[Fold 1 Epoch 460] Train Loss: 0.0390 | Val Loss: 0.0325\n",
      "[Fold 1 Epoch 470] Train Loss: 0.0358 | Val Loss: 0.0340\n",
      "[Fold 1 Epoch 480] Train Loss: 0.0417 | Val Loss: 0.0393\n",
      "[Fold 1 Epoch 490] Train Loss: 0.0353 | Val Loss: 0.0337\n",
      "[Fold 1 Epoch 500] Train Loss: 0.0379 | Val Loss: 0.0332\n",
      "Evaluating fold 1 ...\n",
      "R²: 0.4096\n",
      "RMSE: 0.1821\n",
      "\n",
      "--- Fold 2 ---\n",
      "[Fold 2 Epoch 10] Train Loss: 0.1002 | Val Loss: 0.1067\n",
      "[Fold 2 Epoch 20] Train Loss: 0.0770 | Val Loss: 0.1244\n",
      "[Fold 2 Epoch 30] Train Loss: 0.0866 | Val Loss: 0.1121\n",
      "[Fold 2 Epoch 40] Train Loss: 0.0736 | Val Loss: 0.1034\n",
      "[Fold 2 Epoch 50] Train Loss: 0.0703 | Val Loss: 0.1125\n",
      "[Fold 2 Epoch 60] Train Loss: 0.0685 | Val Loss: 0.1055\n",
      "[Fold 2 Epoch 70] Train Loss: 0.0655 | Val Loss: 0.0950\n",
      "[Fold 2 Epoch 80] Train Loss: 0.0740 | Val Loss: 0.1013\n",
      "[Fold 2 Epoch 90] Train Loss: 0.0673 | Val Loss: 0.1009\n",
      "[Fold 2 Epoch 100] Train Loss: 0.0666 | Val Loss: 0.0974\n",
      "[Fold 2 Epoch 110] Train Loss: 0.0635 | Val Loss: 0.0966\n",
      "[Fold 2 Epoch 120] Train Loss: 0.0598 | Val Loss: 0.0973\n",
      "[Fold 2 Epoch 130] Train Loss: 0.0609 | Val Loss: 0.1010\n",
      "[Fold 2 Epoch 140] Train Loss: 0.0643 | Val Loss: 0.1122\n",
      "[Fold 2 Epoch 150] Train Loss: 0.0560 | Val Loss: 0.0978\n",
      "[Fold 2 Epoch 160] Train Loss: 0.0555 | Val Loss: 0.0931\n",
      "[Fold 2 Epoch 170] Train Loss: 0.0563 | Val Loss: 0.0863\n",
      "[Fold 2 Epoch 180] Train Loss: 0.0527 | Val Loss: 0.0878\n",
      "[Fold 2 Epoch 190] Train Loss: 0.0539 | Val Loss: 0.0913\n",
      "[Fold 2 Epoch 200] Train Loss: 0.0494 | Val Loss: 0.0848\n",
      "[Fold 2 Epoch 210] Train Loss: 0.0509 | Val Loss: 0.0840\n",
      "[Fold 2 Epoch 220] Train Loss: 0.0516 | Val Loss: 0.0746\n",
      "[Fold 2 Epoch 230] Train Loss: 0.0485 | Val Loss: 0.0723\n",
      "[Fold 2 Epoch 240] Train Loss: 0.0476 | Val Loss: 0.0778\n",
      "[Fold 2 Epoch 250] Train Loss: 0.0505 | Val Loss: 0.0706\n",
      "[Fold 2 Epoch 260] Train Loss: 0.0480 | Val Loss: 0.0796\n",
      "[Fold 2 Epoch 270] Train Loss: 0.0482 | Val Loss: 0.0779\n",
      "[Fold 2 Epoch 280] Train Loss: 0.0518 | Val Loss: 0.0822\n",
      "[Fold 2 Epoch 290] Train Loss: 0.0518 | Val Loss: 0.0647\n",
      "[Fold 2 Epoch 300] Train Loss: 0.0460 | Val Loss: 0.0610\n",
      "[Fold 2 Epoch 310] Train Loss: 0.0472 | Val Loss: 0.0626\n",
      "[Fold 2 Epoch 320] Train Loss: 0.0425 | Val Loss: 0.0634\n",
      "[Fold 2 Epoch 330] Train Loss: 0.0423 | Val Loss: 0.0620\n",
      "[Fold 2 Epoch 340] Train Loss: 0.0548 | Val Loss: 0.0768\n",
      "[Fold 2 Epoch 350] Train Loss: 0.0472 | Val Loss: 0.0605\n",
      "[Fold 2 Epoch 360] Train Loss: 0.0477 | Val Loss: 0.0938\n",
      "[Fold 2 Epoch 370] Train Loss: 0.0428 | Val Loss: 0.0465\n",
      "[Fold 2 Epoch 380] Train Loss: 0.0377 | Val Loss: 0.0587\n",
      "[Fold 2 Epoch 390] Train Loss: 0.0392 | Val Loss: 0.0487\n",
      "[Fold 2 Epoch 400] Train Loss: 0.0434 | Val Loss: 0.0542\n",
      "[Fold 2 Epoch 410] Train Loss: 0.0399 | Val Loss: 0.0574\n",
      "[Fold 2 Epoch 420] Train Loss: 0.0386 | Val Loss: 0.0506\n",
      "[Fold 2 Epoch 430] Train Loss: 0.0414 | Val Loss: 0.0580\n",
      "[Fold 2 Epoch 440] Train Loss: 0.0431 | Val Loss: 0.0675\n",
      "[Fold 2 Epoch 450] Train Loss: 0.0399 | Val Loss: 0.0634\n",
      "[Fold 2 Epoch 460] Train Loss: 0.0405 | Val Loss: 0.0719\n",
      "[Fold 2 Epoch 470] Train Loss: 0.0411 | Val Loss: 0.0584\n",
      "[Fold 2 Epoch 480] Train Loss: 0.0405 | Val Loss: 0.0614\n",
      "[Fold 2 Epoch 490] Train Loss: 0.0419 | Val Loss: 0.0615\n",
      "[Fold 2 Epoch 500] Train Loss: 0.0390 | Val Loss: 0.0603\n",
      "Evaluating fold 2 ...\n",
      "R²: 0.4225\n",
      "RMSE: 0.2456\n",
      "\n",
      "--- Fold 3 ---\n",
      "[Fold 3 Epoch 10] Train Loss: 0.1249 | Val Loss: 0.0701\n",
      "[Fold 3 Epoch 20] Train Loss: 0.0956 | Val Loss: 0.0708\n",
      "[Fold 3 Epoch 30] Train Loss: 0.0850 | Val Loss: 0.0544\n",
      "[Fold 3 Epoch 40] Train Loss: 0.0645 | Val Loss: 0.0568\n",
      "[Fold 3 Epoch 50] Train Loss: 0.0613 | Val Loss: 0.0536\n",
      "[Fold 3 Epoch 60] Train Loss: 0.0654 | Val Loss: 0.0507\n",
      "[Fold 3 Epoch 70] Train Loss: 0.0661 | Val Loss: 0.0506\n",
      "[Fold 3 Epoch 80] Train Loss: 0.0620 | Val Loss: 0.0551\n",
      "[Fold 3 Epoch 90] Train Loss: 0.0582 | Val Loss: 0.0506\n",
      "[Fold 3 Epoch 100] Train Loss: 0.0618 | Val Loss: 0.0524\n",
      "[Fold 3 Epoch 110] Train Loss: 0.0715 | Val Loss: 0.0512\n",
      "[Fold 3 Epoch 120] Train Loss: 0.0580 | Val Loss: 0.0548\n",
      "[Fold 3 Epoch 130] Train Loss: 0.0629 | Val Loss: 0.0501\n",
      "[Fold 3 Epoch 140] Train Loss: 0.0539 | Val Loss: 0.0523\n",
      "[Fold 3 Epoch 150] Train Loss: 0.0517 | Val Loss: 0.0514\n",
      "[Fold 3 Epoch 160] Train Loss: 0.0552 | Val Loss: 0.0495\n",
      "[Fold 3 Epoch 170] Train Loss: 0.0552 | Val Loss: 0.0563\n",
      "[Fold 3 Epoch 180] Train Loss: 0.0528 | Val Loss: 0.0494\n",
      "[Fold 3 Epoch 190] Train Loss: 0.0599 | Val Loss: 0.0460\n",
      "[Fold 3 Epoch 200] Train Loss: 0.0485 | Val Loss: 0.0488\n",
      "[Fold 3 Epoch 210] Train Loss: 0.0451 | Val Loss: 0.0523\n",
      "[Fold 3 Epoch 220] Train Loss: 0.0486 | Val Loss: 0.0487\n",
      "[Fold 3 Epoch 230] Train Loss: 0.0496 | Val Loss: 0.0428\n",
      "[Fold 3 Epoch 240] Train Loss: 0.0425 | Val Loss: 0.0414\n",
      "[Fold 3 Epoch 250] Train Loss: 0.0403 | Val Loss: 0.0449\n",
      "[Fold 3 Epoch 260] Train Loss: 0.0495 | Val Loss: 0.0453\n",
      "[Fold 3 Epoch 270] Train Loss: 0.0413 | Val Loss: 0.0429\n",
      "[Fold 3 Epoch 280] Train Loss: 0.0435 | Val Loss: 0.0379\n",
      "[Fold 3 Epoch 290] Train Loss: 0.0427 | Val Loss: 0.0393\n",
      "[Fold 3 Epoch 300] Train Loss: 0.0440 | Val Loss: 0.0377\n",
      "[Fold 3 Epoch 310] Train Loss: 0.0384 | Val Loss: 0.0385\n",
      "[Fold 3 Epoch 320] Train Loss: 0.0427 | Val Loss: 0.0403\n",
      "[Fold 3 Epoch 330] Train Loss: 0.0387 | Val Loss: 0.0448\n",
      "[Fold 3 Epoch 340] Train Loss: 0.0430 | Val Loss: 0.0439\n",
      "[Fold 3 Epoch 350] Train Loss: 0.0402 | Val Loss: 0.0357\n",
      "[Fold 3 Epoch 360] Train Loss: 0.0368 | Val Loss: 0.0353\n",
      "[Fold 3 Epoch 370] Train Loss: 0.0411 | Val Loss: 0.0431\n",
      "[Fold 3 Epoch 380] Train Loss: 0.0402 | Val Loss: 0.0364\n",
      "[Fold 3 Epoch 390] Train Loss: 0.0381 | Val Loss: 0.0357\n",
      "[Fold 3 Epoch 400] Train Loss: 0.0405 | Val Loss: 0.0375\n",
      "[Fold 3 Epoch 410] Train Loss: 0.0413 | Val Loss: 0.0340\n",
      "[Fold 3 Epoch 420] Train Loss: 0.0420 | Val Loss: 0.0343\n",
      "[Fold 3 Epoch 430] Train Loss: 0.0405 | Val Loss: 0.0343\n",
      "[Fold 3 Epoch 440] Train Loss: 0.0397 | Val Loss: 0.0340\n",
      "[Fold 3 Epoch 450] Train Loss: 0.0402 | Val Loss: 0.0360\n",
      "[Fold 3 Epoch 460] Train Loss: 0.0382 | Val Loss: 0.0396\n",
      "[Fold 3 Epoch 470] Train Loss: 0.0411 | Val Loss: 0.0350\n",
      "[Fold 3 Epoch 480] Train Loss: 0.0426 | Val Loss: 0.0347\n",
      "[Fold 3 Epoch 490] Train Loss: 0.0367 | Val Loss: 0.0351\n",
      "[Fold 3 Epoch 500] Train Loss: 0.0398 | Val Loss: 0.0314\n",
      "Evaluating fold 3 ...\n",
      "R²: 0.5500\n",
      "RMSE: 0.1772\n",
      "\n",
      "--- Fold 4 ---\n",
      "[Fold 4 Epoch 10] Train Loss: 0.1662 | Val Loss: 0.0956\n",
      "[Fold 4 Epoch 20] Train Loss: 0.1288 | Val Loss: 0.1279\n",
      "[Fold 4 Epoch 30] Train Loss: 0.0853 | Val Loss: 0.0974\n",
      "[Fold 4 Epoch 40] Train Loss: 0.0789 | Val Loss: 0.0863\n",
      "[Fold 4 Epoch 50] Train Loss: 0.0773 | Val Loss: 0.0990\n",
      "[Fold 4 Epoch 60] Train Loss: 0.0892 | Val Loss: 0.0881\n",
      "[Fold 4 Epoch 70] Train Loss: 0.0686 | Val Loss: 0.0940\n",
      "[Fold 4 Epoch 80] Train Loss: 0.0671 | Val Loss: 0.0921\n",
      "[Fold 4 Epoch 90] Train Loss: 0.0596 | Val Loss: 0.0935\n",
      "[Fold 4 Epoch 100] Train Loss: 0.0616 | Val Loss: 0.0897\n",
      "[Fold 4 Epoch 110] Train Loss: 0.0542 | Val Loss: 0.0850\n",
      "[Fold 4 Epoch 120] Train Loss: 0.0581 | Val Loss: 0.0852\n",
      "[Fold 4 Epoch 130] Train Loss: 0.0592 | Val Loss: 0.0866\n",
      "[Fold 4 Epoch 140] Train Loss: 0.0605 | Val Loss: 0.0879\n",
      "[Fold 4 Epoch 150] Train Loss: 0.0609 | Val Loss: 0.0823\n",
      "[Fold 4 Epoch 160] Train Loss: 0.0766 | Val Loss: 0.0854\n",
      "[Fold 4 Epoch 170] Train Loss: 0.0611 | Val Loss: 0.0833\n",
      "[Fold 4 Epoch 180] Train Loss: 0.0574 | Val Loss: 0.0807\n",
      "[Fold 4 Epoch 190] Train Loss: 0.0601 | Val Loss: 0.0803\n",
      "[Fold 4 Epoch 200] Train Loss: 0.0517 | Val Loss: 0.0768\n",
      "[Fold 4 Epoch 210] Train Loss: 0.0575 | Val Loss: 0.0729\n",
      "[Fold 4 Epoch 220] Train Loss: 0.0551 | Val Loss: 0.0723\n",
      "[Fold 4 Epoch 230] Train Loss: 0.0619 | Val Loss: 0.0737\n",
      "[Fold 4 Epoch 240] Train Loss: 0.0561 | Val Loss: 0.0690\n",
      "[Fold 4 Epoch 250] Train Loss: 0.0572 | Val Loss: 0.0686\n",
      "[Fold 4 Epoch 260] Train Loss: 0.0484 | Val Loss: 0.0686\n",
      "[Fold 4 Epoch 270] Train Loss: 0.0550 | Val Loss: 0.0718\n",
      "[Fold 4 Epoch 280] Train Loss: 0.0535 | Val Loss: 0.0788\n",
      "[Fold 4 Epoch 290] Train Loss: 0.0469 | Val Loss: 0.0696\n",
      "[Fold 4 Epoch 300] Train Loss: 0.0506 | Val Loss: 0.0700\n",
      "[Fold 4 Epoch 310] Train Loss: 0.0474 | Val Loss: 0.0665\n",
      "[Fold 4 Epoch 320] Train Loss: 0.0504 | Val Loss: 0.0686\n",
      "[Fold 4 Epoch 330] Train Loss: 0.0474 | Val Loss: 0.0685\n",
      "[Fold 4 Epoch 340] Train Loss: 0.0420 | Val Loss: 0.0556\n",
      "[Fold 4 Epoch 350] Train Loss: 0.0508 | Val Loss: 0.0637\n",
      "[Fold 4 Epoch 360] Train Loss: 0.0383 | Val Loss: 0.0653\n",
      "[Fold 4 Epoch 370] Train Loss: 0.0454 | Val Loss: 0.0551\n",
      "[Fold 4 Epoch 380] Train Loss: 0.0440 | Val Loss: 0.0576\n",
      "[Fold 4 Epoch 390] Train Loss: 0.0419 | Val Loss: 0.0491\n",
      "[Fold 4 Epoch 400] Train Loss: 0.0424 | Val Loss: 0.0499\n",
      "[Fold 4 Epoch 410] Train Loss: 0.0403 | Val Loss: 0.0552\n",
      "[Fold 4 Epoch 420] Train Loss: 0.0440 | Val Loss: 0.0538\n",
      "[Fold 4 Epoch 430] Train Loss: 0.0442 | Val Loss: 0.0508\n",
      "[Fold 4 Epoch 440] Train Loss: 0.0354 | Val Loss: 0.0539\n",
      "[Fold 4 Epoch 450] Train Loss: 0.0408 | Val Loss: 0.0457\n",
      "[Fold 4 Epoch 460] Train Loss: 0.0444 | Val Loss: 0.0500\n",
      "[Fold 4 Epoch 470] Train Loss: 0.0396 | Val Loss: 0.0494\n",
      "[Fold 4 Epoch 480] Train Loss: 0.0464 | Val Loss: 0.0494\n",
      "[Fold 4 Epoch 490] Train Loss: 0.0351 | Val Loss: 0.0448\n",
      "[Fold 4 Epoch 500] Train Loss: 0.0347 | Val Loss: 0.0472\n",
      "Evaluating fold 4 ...\n",
      "R²: 0.4746\n",
      "RMSE: 0.2172\n",
      "\n",
      "--- Fold 5 ---\n",
      "[Fold 5 Epoch 10] Train Loss: 0.2208 | Val Loss: 0.0688\n",
      "[Fold 5 Epoch 20] Train Loss: 0.1073 | Val Loss: 0.0548\n",
      "[Fold 5 Epoch 30] Train Loss: 0.1024 | Val Loss: 0.0553\n",
      "[Fold 5 Epoch 40] Train Loss: 0.0846 | Val Loss: 0.0596\n",
      "[Fold 5 Epoch 50] Train Loss: 0.0784 | Val Loss: 0.0583\n",
      "[Fold 5 Epoch 60] Train Loss: 0.0628 | Val Loss: 0.0872\n",
      "[Fold 5 Epoch 70] Train Loss: 0.0664 | Val Loss: 0.0566\n",
      "[Fold 5 Epoch 80] Train Loss: 0.0647 | Val Loss: 0.0594\n",
      "[Fold 5 Epoch 90] Train Loss: 0.0664 | Val Loss: 0.0573\n",
      "[Fold 5 Epoch 100] Train Loss: 0.0610 | Val Loss: 0.0547\n",
      "[Fold 5 Epoch 110] Train Loss: 0.0674 | Val Loss: 0.0570\n",
      "[Fold 5 Epoch 120] Train Loss: 0.0668 | Val Loss: 0.0541\n",
      "[Fold 5 Epoch 130] Train Loss: 0.0618 | Val Loss: 0.0540\n",
      "[Fold 5 Epoch 140] Train Loss: 0.0601 | Val Loss: 0.0535\n",
      "[Fold 5 Epoch 150] Train Loss: 0.0619 | Val Loss: 0.0549\n",
      "[Fold 5 Epoch 160] Train Loss: 0.0626 | Val Loss: 0.0563\n",
      "[Fold 5 Epoch 170] Train Loss: 0.0622 | Val Loss: 0.0554\n",
      "[Fold 5 Epoch 180] Train Loss: 0.0613 | Val Loss: 0.0542\n",
      "[Fold 5 Epoch 190] Train Loss: 0.0624 | Val Loss: 0.0529\n",
      "[Fold 5 Epoch 200] Train Loss: 0.0588 | Val Loss: 0.0523\n",
      "[Fold 5 Epoch 210] Train Loss: 0.0550 | Val Loss: 0.0527\n",
      "[Fold 5 Epoch 220] Train Loss: 0.0662 | Val Loss: 0.0518\n",
      "[Fold 5 Epoch 230] Train Loss: 0.0609 | Val Loss: 0.0505\n",
      "[Fold 5 Epoch 240] Train Loss: 0.0564 | Val Loss: 0.0507\n",
      "[Fold 5 Epoch 250] Train Loss: 0.0541 | Val Loss: 0.0484\n",
      "[Fold 5 Epoch 260] Train Loss: 0.0614 | Val Loss: 0.0423\n",
      "[Fold 5 Epoch 270] Train Loss: 0.0528 | Val Loss: 0.0388\n",
      "[Fold 5 Epoch 280] Train Loss: 0.0484 | Val Loss: 0.0369\n",
      "[Fold 5 Epoch 290] Train Loss: 0.0473 | Val Loss: 0.0471\n",
      "[Fold 5 Epoch 300] Train Loss: 0.0524 | Val Loss: 0.0341\n",
      "[Fold 5 Epoch 310] Train Loss: 0.0391 | Val Loss: 0.0348\n",
      "[Fold 5 Epoch 320] Train Loss: 0.0482 | Val Loss: 0.0361\n",
      "[Fold 5 Epoch 330] Train Loss: 0.0448 | Val Loss: 0.0362\n",
      "[Fold 5 Epoch 340] Train Loss: 0.0474 | Val Loss: 0.0350\n",
      "[Fold 5 Epoch 350] Train Loss: 0.0559 | Val Loss: 0.0320\n",
      "[Fold 5 Epoch 360] Train Loss: 0.0428 | Val Loss: 0.0342\n",
      "[Fold 5 Epoch 370] Train Loss: 0.0523 | Val Loss: 0.0376\n",
      "[Fold 5 Epoch 380] Train Loss: 0.0493 | Val Loss: 0.0307\n",
      "[Fold 5 Epoch 390] Train Loss: 0.0461 | Val Loss: 0.0366\n",
      "[Fold 5 Epoch 400] Train Loss: 0.0491 | Val Loss: 0.0328\n",
      "[Fold 5 Epoch 410] Train Loss: 0.0426 | Val Loss: 0.0337\n",
      "[Fold 5 Epoch 420] Train Loss: 0.0490 | Val Loss: 0.0325\n",
      "[Fold 5 Epoch 430] Train Loss: 0.0480 | Val Loss: 0.0325\n",
      "[Fold 5 Epoch 440] Train Loss: 0.0449 | Val Loss: 0.0347\n",
      "[Fold 5 Epoch 450] Train Loss: 0.0480 | Val Loss: 0.0317\n",
      "[Fold 5 Epoch 460] Train Loss: 0.0551 | Val Loss: 0.0334\n",
      "[Fold 5 Epoch 470] Train Loss: 0.0458 | Val Loss: 0.0361\n",
      "[Fold 5 Epoch 480] Train Loss: 0.0446 | Val Loss: 0.0313\n",
      "[Fold 5 Epoch 490] Train Loss: 0.0425 | Val Loss: 0.0332\n",
      "[Fold 5 Epoch 500] Train Loss: 0.0527 | Val Loss: 0.0336\n",
      "Evaluating fold 5 ...\n",
      "R²: 0.5154\n",
      "RMSE: 0.1833\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "Fold 1: R² = 0.4096, RMSE = 0.1821\n",
      "Fold 2: R² = 0.4225, RMSE = 0.2456\n",
      "Fold 3: R² = 0.5500, RMSE = 0.1772\n",
      "Fold 4: R² = 0.4746, RMSE = 0.2172\n",
      "Fold 5: R² = 0.5154, RMSE = 0.1833\n",
      "\n",
      "Average R²: 0.4744 ± 0.0535\n",
      "Average RMSE: 0.2011 ± 0.0264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold  #  k 折交叉验证\n",
    "\n",
    "# ---------------------\n",
    "# k-Fold Cross Validation\n",
    "# ---------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 5  # number of folds\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42) # shuffle = True 参数 随机打乱\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)): # 循环 kf.split的折叠 enumerate函数添加计数\n",
    "                                                                # fold 计数器 迭代次数\n",
    "    print(f\"\\n--- Fold {fold + 1} ---\")\n",
    "    \n",
    "    # Create train and validation subsets\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx) # 子集：训练集\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "    \n",
    "    train_loader = PyGDataLoader(train_subset, batch_size=16, shuffle=True) # PyG 数据加载\n",
    "    val_loader = PyGDataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Initialize a new instance of the model, optimizer, and loss function for each fold\n",
    "    model = GINE_Regression(\n",
    "        node_in_dim=1,\n",
    "        edge_in_dim=2,\n",
    "        external_in_dim=4,\n",
    "        hidden_dim=16,\n",
    "        num_layers=5,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    num_epochs = 500\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"[Fold {fold + 1} Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    print(f\"Evaluating fold {fold + 1} ...\")\n",
    "    r2, rmse = evaluate_model(model, val_loader, device)\n",
    "    fold_results.append({\"fold\": fold + 1, \"r2\": r2, \"rmse\": rmse})\n",
    "\n",
    "# ---------------------\n",
    "# Summary of Cross-Validation Results\n",
    "# ---------------------\n",
    "r2_scores = [res[\"r2\"] for res in fold_results]\n",
    "rmse_scores = [res[\"rmse\"] for res in fold_results]\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "for res in fold_results:\n",
    "    print(f\"Fold {res['fold']}: R² = {res['r2']:.4f}, RMSE = {res['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfabet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
